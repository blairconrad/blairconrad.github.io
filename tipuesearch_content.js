var tipuesearch = {"pages":[{"title":"Almond Logs","text":"1 c butter mix roll bake 30 minutes at 300°F shake to coat 4 heaping T sugar 2 c flour 1 generous t almond extract 1 c ground almonds (100 g) 1 c sugar 1 paper bag Preheat oven to 300°F Mix everything but the last cup of sugar Roll into logs (16 g each) Bake 30 minutes Put sugar in a paper bag and shake the baked logs to coat","tags":"Recipes","url":"https://blairconrad.com/pages/almond-logs/","loc":"https://blairconrad.com/pages/almond-logs/"},{"title":"Arnold Palmer Cookies","text":"10 T (140 g) unsalted butter, at room temperature beat beat beat beat ball with #50 disher (23 g) join and ball roll in sugar smoosh bake 13 minutes at 350°F cool on pan 5 minutes cool on rack 1 c (200 g) granulated sugar (maybe try less next time—150g?) zest of 2 large lemons 1 T lemon extract ¾ t baking powder mix ½ t baking soda ½ t kosher salt 1 large egg 1¾ c (245 g) all-purpose flour 7 t tea from regular bags like Red Rose or something. Adjust depending on strength and coarseness. grind mix beat beat ball with #50 disher (23 g) 1 large egg beat 10 T (140 g) unsalted butter, at room temperature beat beat ½ c (110 g) packed dark brown sugar (maybe try less next time—85g?) ½ c (100 g) granulated sugar (maybe try less next time—75g?) ¾ t baking powder mix ½ t baking soda ½ t kosher salt 1 c (140 g) all-purpose flour ½ c (75 g) whole-wheat flour Make the lemonade cookie dough Beat the butter, sugar, lemon zest, and lemon extract together on medium speed until the mixture looks lighter in color than when you started, about 1 minute. Add the baking powder, baking soda, and salt, beating until the batter is pale yellow, about 30 seconds. Add the egg and beat until incorporated. Add the flour and beat until a dough forms. Scoop out the dough and transfer to a clean bowl. Make the iced tea cookie dough 1. Grind the tea (after measuring it). You can do this in a spice grinder or by placing it in a zip-top plastic bag and using a rolling pin to crush the leaves. 1. Add the egg to the tea and beat to blend. 1. Place the butter and both sugars in the bowl of the stand mixer (no need to wash it after making the lemon dough) and beat together on medium speed until the mixture looks lighter in color than when you started, about 1 minute. 1. Add the baking powder, baking soda, and salt and beat for about 30 seconds to incorporate. 1. Scrape the egg and tea into the bowl and beat for about 30 seconds more to incorporate. 1. Add both flours and beat until a dough forms. Finish the cookies 1. Make a ball of lemon dough. Use a #50 disher for about 23g of dough and roll into a ball. 1. Make a same-sized ball of tea dough. 1. Combine the two balls and roll them together into one big ball. 1. Roll the ball in sugar. Smoosh it on a cookie sheet. For fun, smoosh each ball at a different angle. 1. Repeat until you fill up a cookie sheet. The cookies spread like crazy; I get 9 on a full-sized pan. 1. Bake at 350°F for about 13 minutes. 1. Cool on baking sheet about 5 minutes. 1. Cool completely on a rack. Adapted from SprinkleBake's adaptation of Irvin Lin's recipe. I reduced the sugar and omitted the glaze.","tags":"Recipes","url":"https://blairconrad.com/pages/arnold-palmer-cookies/","loc":"https://blairconrad.com/pages/arnold-palmer-cookies/"},{"title":"Banana Nut Cookies","text":"2⅔ c all-purpose flour mix well blend drop sprinkle 6-8 bake 20-24 minutes at 300°F 1 t baking soda ¼ t salt 1 c light brown sugar, packed blend mix beat ½ c white sugar 1 c salted butter, softened 1 large egg 1 t crême de banana liqueur or pure banana extract ¾ c (2 medium) mashed ripe banana 1 c (12 oz) semisweet chocolate chips 1 c (4 oz) chopped walnuts ~1 c semisweet chocolate chips Preheat oven to 300°F In a medium bowl combine flour, soda, and salt. Mix well. In large bowl with an electric mixer blend sugars at medium speed. Add butter and mix to form a grainy paste. Add egg, liqueur and banana, and beat at medium speed until smooth. Add the flour mixture, 1 cup of the chocolate chips and the walnuts, and blend at low speed until just combined. Drop by rounded tablespons onto ungreased cookie sheets, 2 inches apart. Sprinkle cookies with chocolate chips, 6 to 8 per coookie. Bake 20-24 minutes or until cookie edges begin to brown. Shamelessly stolen from Mrs. Fields Cookie Book , by Debbi Fields","tags":"Recipes","url":"https://blairconrad.com/pages/banana-nut-cookies/","loc":"https://blairconrad.com/pages/banana-nut-cookies/"},{"title":"Bird's Nest Cookies","text":"¾ c margarine, room temperature make into dough roll into balls and push hole in top fill bake at 325°F 13-15 minutes ½ c brown sugar ½ c fine coconut 2 egg yolks 2 c flour 1 t vanilla pinch of salt raspberry jam Turn everything but jam into dough Roll into ball, push hole in middle, and put in jam Bake at 325°F until golden brown on bottom, about 13-15 minutes","tags":"Recipes","url":"https://blairconrad.com/pages/birds-nest-cookies/","loc":"https://blairconrad.com/pages/birds-nest-cookies/"},{"title":"Biscuits","text":"4 c all-purpose flour mix cut together gradually add and mix cut out bake at 350°F 5 T baking powder 1 t salt 4 T sugar ½ c shortening enough milk to make firm, yet pliable Preheat oven to 350 °F Combine flour, sugar, baking powder, and salt. Mix well. Cut in shortening. Gradually add milk until the whole thing becomes firm, yet pliable. Roll out and cut into biscuit shapes Bake until done.","tags":"Recipes","url":"https://blairconrad.com/pages/biscuits/","loc":"https://blairconrad.com/pages/biscuits/"},{"title":"Black-Bottom Cupcakes","text":"8 oz (225 g) cream cheese, full- or reduced-fat, at room temperature beat until smooth stir divide into cups bake for 25 minutes at 350°F (175°C) ⅓ c (65 g) granulated sugar 1 large egg, at room temperature 2 oz (60 g) bittersweet chocolate, coarsely chopped 1½ c (210 g) all-purpose flour sift together stir just until smooth 1 c (240 g) firmly packed light brown sugar 5 T (30 g) natural unsweetened cocoa powder (not Dutch-process) 1 t baking soda ¼ t salt 1 c (250 ml) water mix ⅓ c (85 ml) unflavoured vegetable oil 1 T white or cider vinegar 1 t vanilla extract Butter a 12-cup muffin tin, or line with paper muffin cups. For the filling: Beat together the cream cheese, granulated sugar, and egg until smooth. Stir in the chopped chocolate pieces. Set aside. For the batter: In a medium bowl, sift together the flour, brown sugar, cocoa powder, baking soda, and salt. In a separate bowl, mix together the water, oil, and vanilla. Make a well in the center of the dry ingredients and stir in the wet ingredients, just until smooth. Do not overmix. Divide the batter among the muffin cups, and spoon a few tablespoons of filling into the center of each cupcake, dividing the filling evenly. The cups will be almost completely full. Bake for 25 minutes at 350°F (175°C), or until the tops are slightly golden brown and the cupcakes feel springy when gently pressed. Makes about 12 cupcakes. Shamelessly stolen from David Leibowitz's The Great Book of Chocolate","tags":"Recipes","url":"https://blairconrad.com/pages/black-bottom-cupcakes/","loc":"https://blairconrad.com/pages/black-bottom-cupcakes/"},{"title":"Black Forest Pot Roast","text":"1 3–3½ lb boneless beef chuck or round bone roast trim place in slow cooker pour cook on low for about 8 hours remove meat keep warm eat 1 onion, chopped mix turn slow cooker to high add cook on high 15–20 minutes ¼ c water 4 dried shiitake mushrooms, stems removed, crumbled and rinsed ¼ c ketchup or barbecue sauce ¼ dry red wine 2 T Dijon-style mustard 1 T Worcestershire sauce ½ t salt &frac18 t pepper 1 clove garlic, crushed 2 T cornstarch mix 3 T water Trim visible fat from roast. Place roast in slow cooker. Combine onion, water, mushrooms, ketchup, wine, mustard, Worcestershire sauce, salt, pepper, and garlic. Pour sauce over meat. Cover and cook on low about 8 hours. Remove meat and keep warm. Turn slow cooker to high. Dissolve cornstarch in water. Stir cornstarch slurry into slow cooker. Cover and cook on high 15– 20 minutes or until thickened. Serve sauce with meat. Stolen from Mabel Hoffmans' Crockery Cookery, ISBN 1-55788-217-7","tags":"Recipes","url":"https://blairconrad.com/pages/black-forest-pot-roast/","loc":"https://blairconrad.com/pages/black-forest-pot-roast/"},{"title":"Blair's Elephant Ice Cream","text":"6 large egg yolks whisk temper heat until thick, stirring constantly mix cool fridge it freeze 1 c whole milk warm 135 g sugar 55 g black sesame powder pinch of salt 2 c heavy cream Whisk the egg yolks together. Set aside. Put the cream in a large bowl with a strainer on top. Whisk remaining ingredients together in a medium saucepan. Heat on medium low until quite warm. Slowly pour the warm mixture into the egg yolks, whisking constantly. Return the egg mixture to the saucepan and heat on medium low heat, stirring constantly with a heatproof spatula and scraping the bottom of the pan as you stir. When the mixture thickens and coats the spatula (just before you get greyish sweet scrambled eggs), pour it through the strainer into the cream. Stir the cream and egg mixture together. Place the bowl in a cold water bath until cool. Chill the mixture in the fridge until cold. Freeze the mixture in your ice cream maker according to the manufacturer's instructions. Yield: a generous litre Barely modified from David Leibovitz's Kinako Ice Cream recipe, in The Perfect Scoop","tags":"Recipes","url":"https://blairconrad.com/pages/blairs-elephant-ice-cream/","loc":"https://blairconrad.com/pages/blairs-elephant-ice-cream/"},{"title":"Blind-Baking a Pie Crust","text":"Prick crust all over with a fork Freeze pan until solid, about 20 minutes Lightly coat aluminum foil with butter or nonstick spray and press tightly to crust Bake 20 minutes at 400°F Carefully remove foil. Patch with dough scraps, if necessary Bake 10–12 minutes until light to deep gold Stolen from Smitten Kitchen 's method in her interpretation of the Triple Coconut Cream Pie.","tags":"Recipes","url":"https://blairconrad.com/pages/blind-baking-a-pie-crust/","loc":"https://blairconrad.com/pages/blind-baking-a-pie-crust/"},{"title":"Blueberry Pie","text":"6 c frozen blueberries, about 900g combine encase vent protect bake 50 minutes at 400°F slip a cookie sheet under bake 40 minutes at 350°F ¾ c sugar 2 T quick-cooking tapioca 2 T cornstarch 1 T lemon juice 1 t grated lemon zest &frac18 t salt 2 pie dough rounds Combine berries, sugar, tapioca, cornstarch, lemon juice, lemon zest, and salt Put the fruit mixture between the pie crust rounds, in a pie plate Cut vents in the roof of the pie, to let steam out Use aluminum foil or a pie shield to protect the outer edge of the crust Bake at 400°F, in the bottom third of the oven for 50 minutes Place a cookie sheet under the pie and reduce temperature to 350°F for about another 40 minutes, until thick juices bubble up through the vents Stolen from The Joy of Cooking .","tags":"Recipes","url":"https://blairconrad.com/pages/blueberry-pie/","loc":"https://blairconrad.com/pages/blueberry-pie/"},{"title":"Boston Baked Beans","text":"½ lb salt pork or bacon, cut into ½″–1″ pieces layer ½ layer ½ layer all layer rest of pork pour slow cook 6–7 hours 1 lb dry white beans soak overnight layer rest of beans 1 medium onion, chopped (1½ c) ⅓ c molasses mix ⅛ c brown sugar 4 T Dijon mustard ⅛ t ground cloves 3 c hot water Soak beans in water overnight. Mix molasses, brown sugar, mustard, and cloves with the hot water. Add ½ salt pork, ½ beans, all the onions, remaining beans, and remaining pork to the slow cooker. Pour molasses mixture over the beans. Slow cook for 6–7 hours. Shamelessly stolen from Simple Recipes","tags":"Recipes","url":"https://blairconrad.com/pages/boston-baked-beans/","loc":"https://blairconrad.com/pages/boston-baked-beans/"},{"title":"Brown Bread","text":"3 T margarine add to bucket make a well run dough cycle put in greased pan(s) and shape cover and let rise 1 hour bake at 325°F for 45 minutes 1¼ c water 1/3 c (110 g) molasses 3½ c flour stir together in separate bowl ¾ c rolled oats 1 t salt 2 t yeast add to bucket and cover ½ t sugar add to bucket Put margarine, water, and molasses in the bread machine bucket Stir flour, rolled oats, and salt together in a bowl Put dry ingredients in bread machine bucket Make a well in the top of the dry ingredients Put the yeast in the well and cover with the flour Sprinkle the sugar on top Run the bread machine dough cycle Put the dough in greased bread pan (or pans) Cover and let rise for one hour Bake at 325°F for 45 minutes","tags":"Recipes","url":"https://blairconrad.com/pages/brown-bread/","loc":"https://blairconrad.com/pages/brown-bread/"},{"title":"Brown Sugar Poppyseed Cake","text":"1 c milk combine and soak 20 minutes combine fold pour bake at 350°F 45 - 50 minutes 1 T vinegar ½ c poppy seeds 2½ c flour combine 2 t baking powder 1 t baking soda ½ t salt 1 c margarine cream beat 1½ c brown sugar 4 egg yolks 1 t vanilla 4 egg whites beat until stiff 4 t brown sugar combine 1 T cinammon Soak poppyseeds in milk and vinegar for 20 minutes Mix flour, baking powder, baking soda, and salt Cream margarine and 1½ c brown sugar Add egg yolks and vanilla to margarine and sugar Add flour to creamed mixture in two batches, alternating with milky poppyseeds Beat egg whites until stiff peaks form Combine cinnamon and 4 t brown sugar Pour half the batter into a tub pan Sprinkle cinnamon mixture on batter Pour remaining batter into pan Bake at 350°F for 45–50 minutes","tags":"Recipes","url":"https://blairconrad.com/pages/brown-sugar-poppyseed-cake/","loc":"https://blairconrad.com/pages/brown-sugar-poppyseed-cake/"},{"title":"Buckwheat Banana Bread","text":"2 bananas cut into chunks stir add dry to wet, in 2 additions stir top top sprinkle bake at 350°F for 60–90 minutes, rotating halfway 2 T flaxseed meal stir, or just use 2 eggs stir &frac13 c hot water ¼ c olive oil &frac13 c maple syrup 1 t vanilla extract 1 t baking soda stir ½ t salt top 1 t cinnamon 140–173 g almond flour 38–42 g tapioca flour 54 g buckwheat flour 3 T millet &frac23 cup toasted walnuts 1 banana slice lengthwise ¼ c turbinado sugar Spray the bottom and sides of an 8″ loaf pan with non-stick spray and preheat the oven to 350°F. Cut the bananas into large chunks. In a small bowl, stir together the flaxseed meal and hot water. Set aside. In a medium bowl, stir together olive oil, maple syrup, vanilla, and flaxseed meal mixture until smooth and even. Add this mixture to the mashed bananas using a spatula, making sure the mixture comes together well. In a separate bowl, stir together baking soda, salt, cinnamon, and almond, tapioca, and buckwheat flours. Add the dry ingredients to the wet ingredients, half at a time, using a spatula. Add the millet and toasted walnuts and stir until evenly combined and distributed. Pour half of the batter into the prepared pan and lay one half of the banana on top. Pour the rest of the batter into the pan and lay the remaining slice of banana on top with the cut edge facing up. Sprinkle the top generously with turbinado sugar. Bake for 60–90 minutes, rotating halfway, until a cake tester comes out clean. Let cool and slice to serve. Adapted from Abby Reisner's recipe on Tasting Table","tags":"Recipes","url":"https://blairconrad.com/pages/buckwheat-banana-bread/","loc":"https://blairconrad.com/pages/buckwheat-banana-bread/"},{"title":"Cherry, Chocolate, and Buttermilk Scones","text":"3 T sugar mix press bake 15-20 minutes at 375°F (190°C) ¼ t cinnamon 1 c (140 g) all-purpose flour whisk cut stir stir form and cut 1 c (140 g) cake flour ¼ c (50 g) sugar 2 t baking powder ½ t baking soda ¼ t salt 1 t orange zest 8 T (1 stick, 115 g) unsalted butter, cold cube ¾ c (90 g) dried cherries, sweet or sour ⅓ c (40 g) miniature chocolate chips ⅔ c (170 ml) buttermilk Stir together 3 T sugar and the cinnamon. Set aside. In a large bowl, whisk together the flours, remaining sugar, baking powder, baking soda, salt, and orange zest. Cut the butter into ¼-inch cubes and add to the flour mixture. Cut in, using a pastry cutter, your fingers, or the paddle attachment of a stand mixer, until the mixture resembles cornmeal with evenly dispersed chunks of butter. Stir in the cherries and chocolate chips. Stir in the buttermilk, just until a wet sticky dough is formed. Turn out the dough onto a lightly-floured work surface. Pat into a round, 1″ thick. Cut into 8 wedges. Press the top of each wedge into the reserved cinnamon-sugar mixture. Bake 15-20 minutes at 375°F (190°C), until firm to the touch and lightly golden brown. Makes 8 scones Variation: substitute an equal amount of any dried fruit for the cherries. Dried cranberries work very well. Shamelessly stolen from David Leibowitz's The Great Book of Chocolate","tags":"Recipes","url":"https://blairconrad.com/pages/cherry-chocolate-and-buttermilk-scones/","loc":"https://blairconrad.com/pages/cherry-chocolate-and-buttermilk-scones/"},{"title":"Chewy Black Licorice Chocolate Brownies","text":"1 c all-purpose flour sift stir stir pour press bake at 350°F about 40 minutes ½ c + 2 T (60 g) unsweetened cocoa powder 2 t ground anise seeds ½ t kosher salt ¾ c (6 oz) unsalted butter, melted whisk whisk ½ c granulated sugar ½ c packed brown sugar 3 large eggs 1 t pure vanilla extract 2 oz bittersweet chocolate, chopped mix ½ c (3 oz) chopped soft black licorice chews Line a 9″ square baking pan with paper or foil, leaving 2″ of overhang on 2 sides. Brush the paper with butter. In a medium bowl, sift the flour with the cocoa powder, anise seeds, and salt. In a large bowl, whisk the melted butter with both sugars. Whisk the eggs and vanilla into the sugary butter. Stir dry ingredients into wet mixture. Stir three-fourths of the chocolate and licorice chews into the batter. Put batter in pan, smoothing the top. Gently press the remaining chocolate and licorice chews onto the batter. Bake at 350°F for about 40 minutes, until a toothpick inserted in the center comes out clean, with a few moist crumbs attached. Cool completely, then lift out of the pan using the paper. Shamelessly stolen from Food & Wine .","tags":"Recipes","url":"https://blairconrad.com/pages/chewy-black-licorice-chocolate-brownies/","loc":"https://blairconrad.com/pages/chewy-black-licorice-chocolate-brownies/"},{"title":"Chicken and Peaches","text":"½ c peach jam combine pour bake at 350°F 40–45 minutes, basting occasionally add bake 10 minutes more, basting once or twice &frac13 c water ¼ c barbecue sauce ¼ c onions, diced ¼ c green pepper, diced 1 T soya sauce 2 T flour combine dredge ½ t salt ½ t paprika 4–6 chicken breasts, trimmed and skin removed &frac13 c water chestnuts, sliced 1 c peaches, sliced Combine jam, water, sauce, onions, pepper, and soya sauce. Set aside. Combine flour, salt, and paprika. Dredge chicken in flour mixture and place in a large, shallow casserole. Pour jam mixture over chicken. Bake, uncovered, at 350°F for 40–45 minutes, basting occasionally. Add peach slices and water chestnuts and bake for about 10 minutes more, basting once or twice to make sure peaches are glazed with the sauce. Shamelessly stolen from an earlier edition of Maritime Flavours Guidebook & Cookbook","tags":"Recipes","url":"https://blairconrad.com/pages/chicken-and-peaches/","loc":"https://blairconrad.com/pages/chicken-and-peaches/"},{"title":"Chocolate Coconut Macaroons","text":"4 oz (120 g) semi sweet or bittersweet chocolate, chopped melt, then cool stir fridge it for 1 hour bake at 325°F for 13–15 minutes cool 10 minutes on pan cool on wire rack 3 large (90 g) egg whites lightly whisk whisk ¼ c (25 g) unsweetened cocoa powder, sifted ½ c (100 g) sugar ¼ t salt 1 t pure vanilla extract 2½ c (220 g) unsweetened dried coconut (shredded or flaked) Melt the chocolate, then set aside to cool to room temperature Lightly whisk the egg whites in a large bowl Whisk in cocoa, sugar, salt, and vanilla Stir in coconut and chocolate Cover and refrigerate about 1 hour until firm Bake heaping tablespoons of batter at 325°F for 13–15 minutes or until shiny and just set Let cool on the baking sheet for about 10 minutes Let cool on a wire rack Stolen from the Joy of Baking","tags":"Recipes","url":"https://blairconrad.com/pages/chocolate-coconut-macaroons/","loc":"https://blairconrad.com/pages/chocolate-coconut-macaroons/"},{"title":"Chocolate Gingerbread","text":"5 oz butter melt whisk whisk beat beat pour bake at 350°F about 45 minutes 1½ c molasses 1 t ground cinnamon 2 t ground ginger ½ t ground cloves ¼ t allspice 1¼ t baking soda dissolve 2 t water 1 c milk 2 eggs 2¼ c flour whisk ¾ c cocoa 1 c chocolate chips (optional) 1 c chopped nuts (optional) Line a 9″×13″ pan with parchment paper or grease it well. Melt the butter, molasses, and spices together. Whisk well and remove from heat. Whisk eggs, milk, and baking soda-water mixture into molasses mixture. Whisk flour and cocoa together. Beat flour and cocoa into liquid. Beat chocolate chips and nuts (if using) into batter. Pour batter into prepared pan. Bake at 350°F until set, maybe 45 minutes. Watch it. Adapted from the recipe on the Crosby's Molasses box .","tags":"Recipes","url":"https://blairconrad.com/pages/chocolate-gingerbread/","loc":"https://blairconrad.com/pages/chocolate-gingerbread/"},{"title":"Chocolate Raspberry Curl Cake","text":"Cake 3 c all-purpose flour sift sift again stir together — 3 additions of dry, 2 of sour cream divide among three pans bake at 350°F 30–35 minutes cool in pans 20 minutes, remove and cool completely ¾ c cocoa powder 1½ t baking powder 1½ t baking soda ½ t salt 1½ c butter, softened beat until fluffy beat in, one at a time beat 2¼ c granulated sugar 3 eggs ¼ c thawed raspberry cocktail concentrate 2 t vanilla 2 c sour cream Sift together flour, cocoa, baking power, baking soda, and salt. Sift again. Grease the sides of three 9″ round metal cake pans; line bottoms with parchment paper Beat sugar and butter until fluffy. Beat in eggs, one at a time. Beat in raspberry concentrate and vanilla. Stir together, adding the dry ingredients to the butter in 3 batches, and the sour cream in 2 batches. Divide the batter among the three pans and smooth the tops. Bake at 350°F for 30–35 minutes. Cool the cakes in their pans for 20 minutes. Remove the cakes from their pans, cool completely, and remove the parchment. Icing 1 lb bittersweet or semisweet chocolate, chopped melt whisk 2⅓ c sour cream whisk ¼ c granulated sugar 1 t vanilla Melt the chocolate in the top of a double boiler. Whisk the sugar and vanilla into the sour cream. Whisk the sour cream and chocolate together. Assembly top cake layer spread on bottom stack spread over top and sides ⅓ c seedless raspberry jam spread on bottom stack spread ⅓ c on top middle cake layer bottom cake layer spread ⅓ c on top icing Place one cake layer on a plate. Slide strips of waxed paper under the cake. Spread ⅓ c icing on the cake layer. Spread bottoms of other two layers with jam. Place second layer, jam-side down, on first. Spread ⅓ c icing on the second cake layer. Place third layer, jam-side down, on second. Spread remaining icing over top and sides of cake. Remove paper strips. Refrigerate 30 minutes. shamelessly stolen from the February 2005 issue of Canadian Living , but I omitted the curls, as they seem like a tonne of work.","tags":"Recipes","url":"https://blairconrad.com/pages/chocolate-raspberry-curl-cake/","loc":"https://blairconrad.com/pages/chocolate-raspberry-curl-cake/"},{"title":"Coq au Vin","text":"2 c pearl onions, peeled (1 bag/284 g) 2. brown 6. simmer 20 minutes 7. add 2 c button mushrooms (8 oz/250 g) 1 T butter 1. brown 5. simmer 25 minutes 3 lb boneless skinless chicken thighs and/or breasts 1 T butter 3. fry 4. boil 1 c chopped onions 2 t chopped fresh thyme (or 1 t dried) pinch salt pinch pepper 1 bay leaf 1½ c dry red wine 1½ c chicken stock 2 T tomato paste 1 T Cognac or brandy (optional) 2 T chopped fresh parsley If using chicken breasts, cut crosswise in half. In shallow Dutch oven, heat 1 T butter over medium-high heat. Brown chicken, in batches, about 10 minutes. Remove to plate. <li> Drain fat from pan; reduce heat to medium. Add pearl onions and mushrooms; fry until browned, about 5 minutes. Transfer to separate plate. </li> <li> Add 1 T butter to pan; fry chopped onions, thyme, salt, pepper, and bay leaf until onions are softened, about 8 minutes. </li> <li> Add wine, stock, tomato paste, and Cognac (if using); bring to boil over high heat, stirring and scraping up any brown bits. </li> <li> Return chicken and any accumulated juices to pan. Reduce heat to medium; cover and simmer, stirring occasionally, for 20 minutes. </li> <li> Return mushroom mixture to pan; simmer, covered and stirring occasionally, until reduced to consistency of maple syrup and juices run clear when chicken is pierced, about 25 minutes. Discard bay leaf. </li> <li> Stir in parsley. </li>","tags":"Recipes","url":"https://blairconrad.com/pages/coq-au-vin/","loc":"https://blairconrad.com/pages/coq-au-vin/"},{"title":"A-maize-ing Cornbread","text":"1¼ c all-purpose flour mix stir until dry ingredients are moistened bake 30 minutes at 375° 1 c cornmeal 2 t baking powder 1 t baking soda ¼ t salt ¾ c buttermilk whisk until well blended 1 egg, lightly beaten ½ c low-fat (1%) cottage cheese 2 T honey 2 T butter or margarine, melted 1 can (4 oz) diced green chilies, drained 1 c whole kernel corn (thaw first if using frozen) Preheat oven to 375° In a large bowl, combine flour, cornmeal, baking powder, baking soda, and salt. Mix well. In a medium bowl, combine buttermilk, egg, cottage cheese, honey, butter, chilies, and corn. Beat with a whisk until well blended. Add corn mixture to flour mixture and stir until dry ingredients are moistened. Poor batter into an 8×8″ greased baking pan. Bake for 30 minutes or until a wooden pick inserted in the center comes out clean Shamelessly stolen from Looneyspoons: Low-fat food made fun! , by Janet & Greta Podleski","tags":"Recipes","url":"https://blairconrad.com/pages/a-maize-ing-cornbread/","loc":"https://blairconrad.com/pages/a-maize-ing-cornbread/"},{"title":"Cranberry Oatmeal Cookies","text":"1½ c old-fashioned rolled oats combine mix stir bake 10 minutes at 350°F 1½ c all purpose flour 1 t baking soda ½ t salt ⅔ c packed brown sugar beat beat ⅔ c butter or margarine, softened 2 eggs 1 pkg (170 g) sweetened dried cranberries ⅔ c white or semi-sweet chocolate chips (optional) Combine oats, flour, baking soda, and salt. Set aside. Beat sugar and butter until light and fluffy. Add eggs to butter mixture and mix well. Add flour mixture to butter in several additions, mixing well after each addition. Stir in cranberries and chocolate chips. Drop by rounded tablespoons onto ungreased cookie sheets. Bake for 10 minutes or until golden brown. Makes about 24 large cookies","tags":"Recipes","url":"https://blairconrad.com/pages/cranberry-oatmeal-cookies/","loc":"https://blairconrad.com/pages/cranberry-oatmeal-cookies/"},{"title":"Creamy Vegan Lemon Bars","text":"1 c (120 g) raw cashews soak 1 hour drain blend until very creamy and smooth blend until smooth pour bake 20–23 minutes at 350°F rest 10 minutes cool 1 c (240 g) coconut cream (the hardened portion at the top of full-fat coconut milk—about the amount from a 398 ml can) 2 T (14 g) arrowroot or cornstarch ⅔ c (120 ml) lemon juice (~2 large lemons) ~2 T (4 g) lemon zest (~2 large lemons) pinch sea salt ¼ c (60 ml) maple syrup, plus more to taste 1 c (90 g) gluten-free oats blend to a fine meal stir pan it bake 15 minutes at 350°F bake 5–8 minutes more at 375°F 1 c (112 g) almonds ¼ t sea salt 2 T (24 g) coconut sugar 1 T (15 ml) maple syrup 4 T (60 g) coconut oil, melted Add raw cashews to a mixing bowl and cover with boiling hot water. Let rest uncovered for 1 hour. Then drain thoroughly. Preheat oven to 350°F (176°C) and line an 8×8″ baking dish with parchment paper. Add oats, almonds, sea salt, and coconut sugar to a high speed blender and mix on high until a fine meal is achieved. Transfer to a medium mixing bowl and add maple syrup and melted coconut oil. Stir with a spoon to combine until a loose dough is formed. You should be able to squeeze the mixture between two fingers and form a dough instead of it crumbling. If it's too dry, add more oil. Transfer mixture to the baking pan and spread evenly. Press down firmly until it's evenly distributed and well packed. Bake for 15 minutes, then increase heat to 375°F (190°C) and bake for 5–8 minutes more, or until the edges are golden brown and there is some browning on the surface. Remove from oven to cool slightly, then reduce oven heat to 350°F (176°C). Once cashews are soaked and drained, add to a high speed blender with coconut cream, arrowroot starch, lemon juice, lemon zest, sea salt, and maple syrup. Mix on high until very creamy and smooth. Taste and adjust flavor as needed. You may need more lemon zest and maple syrup. It should be very lemony, and not overly sweet. Pour filling over the pre-baked crust and spread into an even layer. Tap on counter to remove any air bubbles. Bake for 20–23 minutes or until the edges look very slightly dry and the center appears \"jiggly\" but not liquidy. Let rest for 10 minutes, then transfer to refrigerator to let cool completely (uncovered) at least 4 hours, preferably overnight. Shamelessly stolen from The Minamalist Baker .","tags":"Recipes","url":"https://blairconrad.com/pages/creamy-vegan-lemon-bars/","loc":"https://blairconrad.com/pages/creamy-vegan-lemon-bars/"},{"title":"Decadent Double Chocolate Chip Cookies","text":"1&frac23 c flour (205 g) combine beat stir bake 9–11 minutes at 375°F cool first on sheets, then on racks &frac13 c cocoa (40 g) 1 t baking soda ½ t salt 1 c butter (227 g) cream ¾ c lightly packed brown sugar (150 g) ½ c granulated sugar (94 g) 1 egg 1 t vanilla 297 g dark chocolate chips 1¼ c (~130 g) coarsely chopped pecans (optional) Whisk together flour, cocoa, baking soda, and salt. Set aside. Cream butter and sugars with egg and vanilla. Add flour mixture to butter mixture and beat until combined. Stir in chocolate chips and pecans. Drop onto cookie sheets. I use a #40 disher . Bake at 375°F for 9–11 minutes or until set. Cool for 5 minutes on sheets, then transfer to racks and cool completely. Yield: 3 dozen","tags":"Recipes","url":"https://blairconrad.com/pages/decadent-double-chocolate-chip-cookies/","loc":"https://blairconrad.com/pages/decadent-double-chocolate-chip-cookies/"},{"title":"Dressing","text":"4-5 potatoes boil mash mix bake at 325-350°F for a while 1 onion fry up margarine summer savoury up to 1 c bread crumbs Boil 4-5 potatoes Fry an onion in some margarine Mash potatoes onion, bread crumbs, and summer savour together Bake for about an hour at 325-350°F","tags":"Recipes","url":"https://blairconrad.com/pages/dressing/","loc":"https://blairconrad.com/pages/dressing/"},{"title":"Earl Grey Tea Cookies","text":"2 c flour whisk mix until just combined shape into two logs, 1¼″ in diameter freeze ~1 hour slice into ¼″ disks bake 13–15 minutes at 350°F cool on sheets 3 T finely ground Earl Grey tea leaves ½ t coarse salt 1 c unsalted butter, room temperature mix until pale and fluffy ½ c icing sugar 1 T finely grated orange zest (optional) Whisk together flour, tea, and salt. Mix butter, sugar, and zest at medium speed about 3 minutes, until pale and fluffy. Reduce mixer speed and gradually mix in flour mixture until just combined. Divide dough in half, and shape into logs around 1¼″ in diameter. Freeze until firm, about 1 hour. Cut logs into ¼″ slices. Bake, one inch apart, 13–15 minutes, until edges are golden. Cool on sheets. Variation : you could probably just add some tea to your favourite shortbread recipe… Stolen from Martha Stewart's Cookies","tags":"Recipes","url":"https://blairconrad.com/pages/earl-grey-tea-cookies/","loc":"https://blairconrad.com/pages/earl-grey-tea-cookies/"},{"title":"Ed's Famous Seafood Chowder","text":"butter for frying 3 big potatoes 1 12 oz can frozen lobster 1 can clams 2 onions 1 can mushrooms ½ lb haddock 175 g sea scallops ½ lb baby shrimp black pepper ¼ oyster sauce 1½ c 10% cream Cube potatoes. Add clam and lobster juice to a Dutch oven, squeezing lobster can well. Boil potatoes in clam and lobster juice until almost cooked. Do not drain. Drain mushrooms, discarding juice. Dice onions and fry with butter. Cook lobster meat in butter. Cut larger scallops into manageable pieces. Add onions, lobster, clams, mushrooms, haddock, scallops, and shrimp to Dutch oven. Add pepper and oyster sauce to Dutch oven. Cook on low until seafood is done. Cool chowder. Optional - refigerated and eat the next day. Add creeam to chowder and reheat. Makes about 8 bowls. Stolen from Ed.","tags":"Recipes","url":"https://blairconrad.com/pages/eds-famous-seafood-chowder/","loc":"https://blairconrad.com/pages/eds-famous-seafood-chowder/"},{"title":"Elephant Bread","text":"1 egg beat add milk to make 300 mL add to bread-baking pan in order bake until done milk 1½ t salt 2 T milk powder 1½ T butter 4 T sugar 420g bread flour 40g sesame powder 2 t yeast Makes 1 loaf","tags":"Recipes","url":"https://blairconrad.com/pages/elephant-bread/","loc":"https://blairconrad.com/pages/elephant-bread/"},{"title":"Flourless Chocolate Orange Cake","text":"2 small (or 1 large) thin-skinned oranges, about 375 g total weight boil 2 hours remove large pips puree in food proessor puree in food proessor bake in 9″ cake pan for 45 minutes at 350°F cool completely in pan on rack remove from pan optionally decorate 6 eggs 1 t baking powder ½ t baking soda 200 g ground almonds 125 g sugar 50 g cocoa orange peel, for decoration Put the whole orange or oranges in a pan with some cold water, bring to a boil and cook for 2 hours or until soft. Drain oranges, and when cool, cut the oranges in half and remove any big pips. Puree everything—pith, peel and all—in a food processor. Preheat the oven to 350°F. Butter and line a 9″ cake pan. Add the eggs, baking powder, baking soda, almonds, sugar, and cocoa to the orange in the food processor. Run the motor until you have a cohesive cake mixture, but slightly knobbly with the flecks of puréed orange. Pour and scrape into the cake pan and bake for about 45 minutes, by which time a cake tester should come out pretty well clean. Leave the cake to cool in the pan, on a cooling rack. When the cake is cold you can take it out of the pan. Decorate with strips of orange peel or coarsely grated zest if desired. Stolen from Genius Kitchen (who stole it from Nigela Lawson)","tags":"Recipes","url":"https://blairconrad.com/pages/flourless-chocolate-orange-cake/","loc":"https://blairconrad.com/pages/flourless-chocolate-orange-cake/"},{"title":"Gluten Free Pear Bread","text":"1½ c superfine brown rice flour (plus more for dusting) whisk stir add bake 60–70 minutes at 350°F cool 5 minutes in pan cool until cool ½ c tapioca (or corn or potato) starch 1 t baking powder 1 t kosher or fine sea salt 2 t ground cinnamon 2 large eggs whisk ¾ c sugar ½ cup grapeseed (or other neutral flavoured) oil 2 t pure vanilla extract 2 ripe pears, peeled, cored, grated, and drained 1 c walnuts, chopped Grease and dust a 9″×5″ loaf pan. Whisk the 1½ c brown rice flour, the tapioca starch, the baking powder, the salt, and the cinnamon. In another bowl, whisk the eggs, sugar, oil, and vanilla. Add the wet mixture to the dry and stir to combine. Add the pears and walnuts and pour the batter into the prepared pan. Bake 60–70 minutes at 350°F or until golden brown and a toothpick inserted in the center comes out clean. Cool in the pan for 5 minutes. Remove from pan and cool on a wire rack. Stolen from simply gluten-free","tags":"Recipes","url":"https://blairconrad.com/pages/gluten-free-pear-bread/","loc":"https://blairconrad.com/pages/gluten-free-pear-bread/"},{"title":"Grape-Nuts Bread","text":"1&frac23 c water Add to bread pan in order Bake in bread maker at medium crust setting 4 T (60 g) vegetable oil 2 T (24 g) sugar 1 t (7.2 g) salt 1 c (116 g) Grape-Nuts cereal 4 c (516 g) bread flour 2 t (9.3 g) active dry yeast 1 T (10 g) gluten (optional) Makes 1 loaf Adapted from Mikekey's recipe at Food.com","tags":"Recipes","url":"https://blairconrad.com/pages/grape-nuts-bread/","loc":"https://blairconrad.com/pages/grape-nuts-bread/"},{"title":"Hoisin-Glazed Salmon","text":"2 T hoisin sauce mix coat bake 10–12 minutes at 425°F 1 T soy sauce 1 t sesame oil ¼ t pepper 4 4-oz salmon fillets, about 1″ thick Mix all the stuff up except the fish. Coat fish with sauce Bake 10–12 minutes at 425°F Stolen from Simply HeartSmart Cooking, ISBN 0-394-22401-9","tags":"Recipes","url":"https://blairconrad.com/pages/hoisin-glazed-salmon/","loc":"https://blairconrad.com/pages/hoisin-glazed-salmon/"},{"title":"Jiggly Jello","text":"2 packages Jello mix mix wait ½ c sugar 2 c boiling water 3 envelopes gelatin mix 1 c cold water Combine Jello powder, sugar, and boiling water Combine gelatin powder and cold water Mix it all up Chill until set","tags":"Recipes","url":"https://blairconrad.com/pages/jiggly-jello/","loc":"https://blairconrad.com/pages/jiggly-jello/"},{"title":"Krispies","text":"2 c all-purpose flour mix well blend drop bake 18-20 minutes at 300°F cool ¼ t salt 1 t baking soda ½ c dark brown sugar, packed blend mix beat ½ c white sugar ¾ c salted butter, softened 1 large egg 2 t vanilla extract 1 c crispy rice cereal 1½ c (6 oz) coarsely chopped crispy rice chocolate bars Preheat oven to 300°F In a medium bowl combine flour, salt, and soda. Mix well. In a large bowl with an electric mixer blend sugars at medium speed. Add butter and mix to form a grainy paste. Add egg and vanilla, and beat at medium speed until fully combined. Add the flour mixture, rice cereal, and chocolate chunks. Blend at low speed until just combined. Do not overmix. Drop by rounded tablespons onto ungreased cookie sheets, 2 inches apart. Bake 18-20 minutes. Immediately transfer cookies with a spatula to a cool, flat surface. Yield: 3 dozen Shamelessly stolen from Mrs. Fields Cookie Book , by Debbi Fields","tags":"Recipes","url":"https://blairconrad.com/pages/krispies/","loc":"https://blairconrad.com/pages/krispies/"},{"title":"Macadamia and Coconut Clusters","text":"10 oz (315 g) milk chocolate, preferably European, chopped melt cool over lukewarm water stir thoroughly drop refrigerate 3 oz (90 g) bittersweet or semisweet chocolate, chopped 2 c (10 oz/315 g) coarsely chopped lightly salted roasted macadamia nuts 1½ c (6 oz/185 g) sweetened shredded coconut, lightly toasted Line a baking sheet with parchment paper or waxed paper Place both chocolates in the top pan of a double boiler or in a heatproof bowl. Set over (but not touching) hot (not simmering) water. Heat, stirring constantly, until melted and smooth. Pour out the hot water from the bottom pan and replace it with lukewarm water. Replace the top pan or heatproof bowl. Let the chocolate stand uncovered, stirring frequently, until it cools slightly and begins to thicken, about 10 minutes. Stir the macadamia nuts and coconut into the chocolate, mixing thoroughly. Using a small spoon, scoop out slightly rounded teaspoonfuls of the mixture and drop onto the prepared sheet, spacing evenly. Refrigerate uncovered until set, about 2 hours. Store in an airtight container in the refrigerator for up to 1 month or in the freezer for up to 2 months. Makes about 4 dozen Shamelessly stolen from the Williams-Sonoma Gifts from the Kitchen , recipes by Kristine Kidd","tags":"Recipes","url":"https://blairconrad.com/pages/macadamia-and-coconut-clusters/","loc":"https://blairconrad.com/pages/macadamia-and-coconut-clusters/"},{"title":"Magic Cookie Bars","text":"¼ c butter, melted combine pour layer press bake 25–30 minutes at 350° loosen cool in pan 1½ c graham cracker crumbs 1 (14 oz) can sweetened condensed milk 1&frac13 c (8 oz) semi-sweet chocolate chips, or mixture of chocolate and butterscotch 1 c chopped nuts 1&frac13 c flaked coconut Combine graham cracker crumbs and butter in a small bowl. Press crumb mixture into greased 9″×13″ baking pan. Pour sweetened condensed milk over crumbs. Layer chocolate chips, coconut, and nuts over milk. Press chunks down with a fork. Bake 25–30 minutes at 350° or until lightly browned. Loosen from sides of pan while still warm. Cool on wire rack. Stolen from Wanna","tags":"Recipes","url":"https://blairconrad.com/pages/magic-cookie-bars/","loc":"https://blairconrad.com/pages/magic-cookie-bars/"},{"title":"Magically Moist Almond Cake","text":"1½ c almond flour combine stir beat spread in pan bake at 350°F for 30 minutes ½ c coconut flour ¼ t salt 2 t baking powder ¾ c (or try ½ c) unsalted butter cream beat mix ⅔ c sugar 4 eggs ½ c milk 1 t vanilla Grease a 9″×13″ cake pan Combine flours, salt, and baking powder In a separate bowl, cream together butter and sugar until smooth Add in eggs, one at a time, and beat until fully blended Add milk and vanilla; mix until combined Stir dry ingredients into wet ingredients Beat until creamy Spread batter into prepared pan Bake at 350°F for 30 minutes Stolen from the back of Bob's Red Mill Almond Meal bag.","tags":"Recipes","url":"https://blairconrad.com/pages/magically-moist-almond-cake/","loc":"https://blairconrad.com/pages/magically-moist-almond-cake/"},{"title":"Marinated Peanut Tempeh","text":"8 oz tempeh boil 10–12 minutes, flipping halfway rinse, pat dry, and cut into bite-sized triangles marinate 2–24 hours bake 22–30 minutes at 375°F brush with reserved marinade hot sauce to taste whiz 1½ T sesame 2 T salted creamy peanut (or almond, cashew, or sunflower butter) 2 T tamari or soy sauce zest of 1 lime 2 T lime juice (about 1 lime) 3 T maple syrup Bring tempeh to a low boil over medium heat in a saucepan with 1″ of water for 10–12 minutes, flipping halfway. Rinse, pat dry, and cut tempeh into thin, bite-sized pieces. Mix all other ingredients together, maybe in a blender or tiny food processor. Adjust to taste. Add tempeh to marinade and toss to coat. Marinate, covered and fridged, for 2–24 hours. Reserve marinade and bake tempeh for 22–30 minutes at 375°F on a parchment-lined baking sheet. Should be a deep golden brown. Brush with reserved marinade. Shamelessly stolen from Minimalist Baker","tags":"Recipes","url":"https://blairconrad.com/pages/marinated-peanut-tempeh/","loc":"https://blairconrad.com/pages/marinated-peanut-tempeh/"},{"title":"Blueberry Buttermilk Coffeecake","text":"½ c pecans, chopped stir top Bake at 350°F for about 1 hour 5 minutes ½ c sugar 2 T butter, melted 1½ t cinnamon 2 c all-purpose flour stir rub stir just until blended fold ¾ c sugar 1 T baking powder ½ c butter, chilled and cut into pea-sized pieces 2 large eggs whisk 1 c buttermilk 2 t vanilla extract 1½ c frozen blueberries For topping: stir together pecans, ½ c sugar, 2 T melted butter, and the cinnamon. Set aside Grease and flour a 9″×9″×2″ baking pan In a large bowl, stir together flour, ¾ c sugar, and baking powder Using fingertips, rub butter into dry ingredients until mixture resembles coarse meal In another bowl, whisk together eggs, buttermilk, and vanilla Add buttermilk mixture to dry ingredients and stir just until blended Fold berries into batter Transfer batter to prepared pan and sprinkle the topping mixture on top Bake at 350°F until a tester inserted into the middle comes out clean, about 1 hour 5 minutes Cool completely in pan on rack Makes about 1 coffeecake","tags":"Recipes","url":"https://blairconrad.com/pages/blueberry-buttermilk-coffeecake/","loc":"https://blairconrad.com/pages/blueberry-buttermilk-coffeecake/"},{"title":"Matcha Custard Pie","text":"½ c granulated sugar sift stir beat in one at a time beat until thin and light stir strain into pie shell bake on middle rack 40–50 minutes at 325°F, rotating when edges start to set, about 35 minutes in cool 3–4 hours 1½ T flour (or sweet rice flour) ¼ t salt 5 t matcha powder ½ c melted butter 3 whole eggs 1 egg yolk 2 c heavy cream ½ t vanilla extract Sift together sugar, sweet rice flour, salt, and matcha powder Stir melted butter into dry ingredients Beat eggs into mixture, one at a time Beat until thing and light coloured Stir in heavy cream and vanilla extract Strain into pie shell Bake on middle rack 40–50 minutes at 325°F, rotating 180° when edges start to set, about 35 minutes in. Pie is done when edges are set and center wobbles slightly. Cool 3 to 4 hours. Adapted from Four & Twenty Blackbirds Matcha Custard Pie on Food52","tags":"Recipes","url":"https://blairconrad.com/pages/matcha-custard-pie/","loc":"https://blairconrad.com/pages/matcha-custard-pie/"},{"title":"Mayan Chocolate Sparklers","text":"⅓ c sugar combine roll bake 8–10 minutes at 350°F cool on baking sheet 3–5 minutes cool on wire rack 1 t cinnamon 1¾ c all purpose flour combine mix stir roll 1¼ c cocoa powder 2 t baking soda ¼ t pepper ⅛ t cayenne 1 T cinnamon ¾ c (142 g) shortening beat beat ½ c butter, softened ¾ c granulated sugar ¾ c brown sugar 2 eggs 1 c chocolate chips line cookie sheets with parchment paper combine ½ cup sugar and 1 teaspoon of cinnamon for topping. Set aside combine flour, cocoa, baking soda, black pepper, cayenne, and remaining tablespoon of cinnamon. Set aside beat shortening, butter, brown sugar, and remaining ¾ cup granulated sugar until creamy beat in eggs, one at a time add flour mixture to wet ingredients. Mix until incorporated stir chocolate chips into dough roll dough into balls, 1″ in diameter (about 22 g). Do not flatten roll balls into cinnamon and sugar topping. Place on cookie sheets, about 2″ apart bake 8–10 minutes at 350°F cool 3–5 minutes on cookie sheet remove to wire rack and cool Makes about 54 cookies Note: Cookies can be frozen raw and stored in an airtight container. If baking from frozen, add 5 minutes to baking time. Stolen from Baking is Back 2007 , which I only realized was online as I typed this very paragraph.","tags":"Recipes","url":"https://blairconrad.com/pages/mayan-chocolate-sparklers/","loc":"https://blairconrad.com/pages/mayan-chocolate-sparklers/"},{"title":"Molasses Crumbles","text":"2 t baking soda dissolve mix roll into balls roll bake at 350°F about 10 minutes just a little hot water ¾ c shortening 1 c brown sugar 1 egg 4 T molasses 2¼ c flour ¼ t salt ½ t cloves 1 t cinnamon 1 t ginger (generous) sugar Dissolve baking soda in water. Mix all other ingredients except rolling sugar together. Roll dough into balls Roll balls in sugar Bake at 350°F for about 10 minutes","tags":"Recipes","url":"https://blairconrad.com/pages/molasses-crumbles/","loc":"https://blairconrad.com/pages/molasses-crumbles/"},{"title":"Moussaka Made with Cauliflower Bechamel","text":"3 russet potatoes slice ¼″ thick place on oiled baking sheet; brush with olive oil; sprinkle with salt and pepper bake 15 minutes at 450° layer: all potatoes, ½ eggplant, all the meat, ½ eggplant, all the bechamel Bake at 350°F, uncovered, for about 45 minutes 2 medium-large eggplants slice ¼–½″ thick sprinkle with salt sweat 30 minutes, rinse, dry place on oiled baking sheet; brush with olive oil; sprinkle with salt and pepper bake 15 minutes at 450° 1 head of cauliflower (about 6 c chopped) chop steam 15–20 minutes blend, adjust spices to taste blend &frac23 c chicken broth ¼ c full fat coconut milk 1 T ghee (or butter) ½ t ground nutmeg &frac58 t salt ½ t black pepper 1 egg 1–2 T olive oil fry 1–2 minutes fry 10 minutes, until meat is no longer pink adjust seasoning 1 c chopped yellow onion 1 T minced garlic 1 T chopped fresh parsley 1 lb ground beef 2 c canned diced tomatoes (including juice) 1 T dried oregano ½ t ground cinnamon ½ t allspice 1 t salt ½ t black pepper Slice eggplant ¼–½″ thick, as evenly as possible Lay eggplant slices in a colander and sprinkle generously with salt. Let sit for about 30 minutes Slice potatoes about ¼″ thick Lay the potatoes on an oiled baking sheet in a single layer Brush lightly with olive oil, then sprinkle with salt and pepper Bake for about 15 minutes, until cooked. Set aside. Rinse the eggplant under cool water. Pat dry with a towel. Layer the eggplant on an oiled baking sheet, then brush them with olive oil and sprinkle with salt and pepper Bake for about 15 minutes, until soft and lightly golden Chop the cauliflower and steam in the chicken broth until cooked through, 15–20 minutes Blend cauliflower (and any remaining broth from the pot), coconut milk, nutmeg, ghee, salt, and pepper until smooth. Taste and adjust seasonings Blend in egg. Set aside. Fry onion, parsley, and garlic over medium-high heat for 1–2 minutes Add all meat, tomatoes, and remaining seasonings. Cook for about 10 minutes, stirring every so often, until meat is no longer pink Taste and adjust flavors as desired Grease a large (at least 9″×13″) baking dish Layer all the potatoes along the bottom, spreading out as much as possible, but don't worry if you have to overlap Top with half the eggplant slices Top with the meat mixture Top with the remaining eggplant slices Pour the bechamel over the top Bake at 350°F, uncovered, for about 45 minutes, until the top is golden Lightly adapted from Honey, Ghee, & Me","tags":"Recipes","url":"https://blairconrad.com/pages/moussaka-made-with-cauliflower-bechamel/","loc":"https://blairconrad.com/pages/moussaka-made-with-cauliflower-bechamel/"},{"title":"Natalie's Godmother's Not-So-Secret and Very Yummy Brownies","text":"a scant 1¾ c white sugar cream beat well beat until just combined stir in by hand stir in by hand bake 25 minutes at 350°F 1 c butter 4 eggs 1 t vanilla 1 c cocoa a pinch of salt 1 c flour 1 c walnuts (optional) Cream butter and sugar together. Add eggs and beat until soft and fluffy. Beat in the vanilla, cocoa, and salt, just until combined. Stir in the flour by hand, just until combined. If using, stir in nuts, just until dispersed. Pour into a greased 9″×13″ baking pan. Bake about 25 minutes at 350°F, or until a toothpick inserted in the centre comes out clean. Variation: Blair's Ginger Brownies Instead of walnuts, use 1 c of chopped candied ginger.","tags":"Recipes","url":"https://blairconrad.com/pages/natalies-godmothers-not-so-secret-and-very-yummy-brownies/","loc":"https://blairconrad.com/pages/natalies-godmothers-not-so-secret-and-very-yummy-brownies/"},{"title":"Nutty White Chunk Cookies","text":"2¼ c all-purpose flour whisk mix just until combined drop by rounded tablespoons bake 20-22 minutes at 300°F 1 t baking soda ¼ t salt 1 c light brown sugar beat beat beat well ½ c white sugar 1 c salted butter, softened 2 large eggs 2 t vanilla extract 1 c (4 oz) pecans, chopped 1½ c (8 oz) white chocolate bar, coarsely chopped Preheat oven to 300°F In a medium bowl combine flour, soda, and salt. Mix well In large bowl with an electric mixer blend sugars at medium speed Add butter and mix to form a grainy paste Add eggs and vanilla extract, and beat at medium speed until well combined Add the flour mixture, pecans, and chocolate, and blend at low speed until just combined Drop by rounded tablespons onto ungreased cookie sheets, 2 inches apart Bake 20-22 minutes or until cookie edges begin to brown Yield: 3 dozen Shamelessly stolen from Mrs. Fields Cookie Book , by Debbi Fields","tags":"Recipes","url":"https://blairconrad.com/pages/nutty-white-chunk-cookies/","loc":"https://blairconrad.com/pages/nutty-white-chunk-cookies/"},{"title":"Oatcakes","text":"3 c flour mix mix mix again roll out cut bake at 325°F for 25-30 minutes 3 c rolled oats 1 c sugar 4 t baking powder ½ t salt 1 c soft margarine melt ½ c cold water additional rolled oats mix dry ingredients in a large bowl mix in melted margarine mix in cold water Roll out on a surface that has rolled oats scattered on it Cut into rounds Bake at 325°F for 25-30 minutes","tags":"Recipes","url":"https://blairconrad.com/pages/oatcakes/","loc":"https://blairconrad.com/pages/oatcakes/"},{"title":"Oatmeal Scotchies","text":"1¼ cups all-purpose flour combine beat stir bake 11–12 minutes at 375°F cool 2 minutes on sheet 1 t baking soda ½ t salt 1 t ground cinnamon 1 c (2 sticks) butter or margarine, softened beat ¾ c granulated sugar ¾ c packed brown sugar 2 large eggs 1 t vanilla extract or grated peel of 1 orange 3 c quick or old-fashioned oats 1⅔ c (311 g) butterscotch chips (or chocolate…) Combined flour, baking soda, salt, and cinnamon in a small bowl. Set aside. 2.Beat together butter, sugars, eggs, and vanilla. Beat flour mixture into butter mixture. Stir in oats and chips. Drop by rounded tablespoons onto ungreased baking sheets. Bake 11–12 minutes at 375°F. Cool 2 minutes on sheet before transferring to racks. Makes about 48 cookies Variation: Pan Cookies Grease a 15″×10″ pan. Spread dough into pan and bake 18–22 minutes. Cool completely in pan. Shamelessly stolen from Nestlé's Very Best Baking","tags":"Recipes","url":"https://blairconrad.com/pages/oatmeal-scotchies/","loc":"https://blairconrad.com/pages/oatmeal-scotchies/"},{"title":"Old-Fashioned Beef Stew","text":"¼ c flour combine toss, in 3 batches brown add to slow cooker slow cook 8–10 hours on low or 4–6 hours on high slow cook 15 minutes remove the bay leaf! 1 t salt ½ t freshly ground black pepper 2 lb stewing beef 1 T vegetable oil 2 c beef stock deglaze add to slow cooker 4 carrots, peeled and sliced add to slow cooker 4 potatoes, peeled and cut into bite-sized pieces 2 stalks celery, chopped 1 large onion, diced, or 15–20 white pearl onions, peeled 19 oz can diced tomatoes, with juice 1 bay leaf 1 T Worcestershire sauce ¼ c chopped fresh parsley or 2 T dried 1 c frozen peas add to slow cooker combine flour, salt, and pepper in a plastic bag or other container toss beef, ⅓ at a time, in the flour brown beef on all sides on medium-high heat add beef to slow cooker add beef stock to the pan and boil, stirring up the brown bits pour all the stock into the slow cooker add everything else, except for the peas, to the slow cooker cook on low heat for 8–10 hours, or on high heat for 4–6 hours put the peas in the slow cooker and stir continue cooking for 15 minutes remove the bay leaf shamelessly stolen from 300 Slow Cooker Favorites by Donna-Marie Pye, as retold in 30-Minute Miracle: The Record, 3 October 2007 by Luisa D'Amato","tags":"Recipes","url":"https://blairconrad.com/pages/old-fashioned-beef-stew/","loc":"https://blairconrad.com/pages/old-fashioned-beef-stew/"},{"title":"Orange Glaze","text":"2 T orange juice whisk drizzle on something 2 t orange zest 1¼ c icing sugar Whisk together orange juice, orange zest, and icing sugar. Drizzle it on something.","tags":"Recipes","url":"https://blairconrad.com/pages/orange-glaze/","loc":"https://blairconrad.com/pages/orange-glaze/"},{"title":"Parker House Rolls","text":"2½ t dry yeast dissolve mix knead 10 minutes rise 60–90 minutes punch rest 10 minutes roll divide in 16 brush fold proof 30 minutes brush with butter bake 15–20 minutes at 350°F 1 c milk warm cool beat 4 T unsalted butter, melted 2 T sugar 2 eggs, beaten 4¼ c bread flour mix 2 t salt 2 T melted butter Sprinkle the yeast into ½ c of the milk in a bowl. Let stand 5 minutes, then stir to dissolve. Warm the other ½ c of milk in a saucepan with butter and sugar. Stir until butter melts. Cool until lukewarm. Beat eggs into milk/butter/sugar mixture. Mix the flour and salt in a large bowl. Make a well in the middle of the flour and pour in the two milk mixtures. Mix to forma soft, sticky dough. Turn the dough out onto a floured work surface. Knead until smooth, shiny, and elastic, about 10 minutes. If the dough is too sticky, knead in extra flour, 1 T at a time, but be careful—the dough should not be dry. Put the dough in a buttered bowl and cover with a dish towel. Let rise until doubled in side, 60–90 minutes. Punch down the dough. Let rest 10 minutes. Roll out the dough to form a 16″×8″ rectangle. Cut dough into four strips, and cut each strip into four pieces. Brush half of each rectangle with melted butter, then fold in half, leaving a 1″ flap. Place rolls on a buttered baking sheet so each roll overlaps slightly with the one next to it. Cover rolls with a dish towel and proof until doubled in size, about 30 minutes. Brush the tops of the rolls with melted butter. Bake 15–20 minutes at 350°F, until golden and hollow sounding when tapped underneath. Cool on a wire rack. Makes 16 rolls Shamelessly stolen from bread by Eric Treuille and Ursula Ferrigno - 978-1-55363-062-3","tags":"Recipes","url":"https://blairconrad.com/pages/parker-house-rolls/","loc":"https://blairconrad.com/pages/parker-house-rolls/"},{"title":"Peanut Butter Balls","text":"1 c peanut butter (260 g) mix mix just to incorporate roll into balls chill until firm dip chill 1 c icing sugar (115 g) ½ c coconut flakes (45 g) ¼ c margarine (58 g) 1 c crisp rice cereal (28 g) approximately ¼ lb of semisweet chocolate (225 g) melt Mix peanut butter, icing sugar, coconut flakes, and margarine Mix in crisp rice cereal, trying not to break it up too much Roll mixture into balls about 1 inch across. You may need to flour your hands Chill the balls until firm Melt the chocolate in a double boiler One at a time, dip the balls in the melted chocolate, rolling to coat. Place in a single layer on a cookie sheet lined with waxed paper Chill the balls until chocolate is well set. You may do this in the freezer","tags":"Recipes","url":"https://blairconrad.com/pages/peanut-butter-balls/","loc":"https://blairconrad.com/pages/peanut-butter-balls/"},{"title":"Peanut Butter and Jelly Bars","text":"1½ c jam or jelly spread sprinkle sprinkle bake 45 minutes at 350°F 3 c all-purpose flour sift incorporate spread ⅔ in pan 1 t baking powder ¾ t salt ½ lb (1 c) butter, at room temp. beat mix mix 1 c granulated sugar 2 c (19 oz, by weight) creamy peanut butter 2 large eggs, at room temp. 1 t vanilla extract ⅔ c peanuts, coarsely chopped (unsalted or salted) Sift together flour, baking powder, and salt. Set aside. Cream butter and sugar. Mix in peanut butter. Mix in eggs and vanilla. Mix flour mixture into peanut butter mixture until just incorporated. Spread ⅔ of the dough into a greased and floured 9″&times13″ pan. Spread jam over the dough. Sprinkle small pieces of dough over jam, leaving some jam exposed. Sprinkle with peanuts. Bake for 45 minutes at 350°F, until golden brown. Makes a pan full of peanut butter and jelly bars. Adapted from Ina Garten . I use less sugar.","tags":"Recipes","url":"https://blairconrad.com/pages/peanut-butter-and-jelly-bars/","loc":"https://blairconrad.com/pages/peanut-butter-and-jelly-bars/"},{"title":"Perfect Cheesecake","text":"Due to the number of ingredients and steps, the recipes has been split into several stages. Note that there are opportunities to perform steps in parallel. For example, the sour cream topping and raspberry sauces could be made in advance, or while the cheesecake is baking. Be sure to read each stage to ensure you have all ingredients. Prepare Crust 1¾ c (230g) Graham cracker crumbs stir stir press into pan bake on cookie sheet at 350°F for 10 minutes in lower third of oven cool wrap 2 T sugar unless crumbs are sweetened, like Kinnikinnick's are Pinch of salt 4 T &plus 1 t (60 g) unsalted butter (if using salted butter, omit the pinch of salt), melted 3&plus 18″ squares of heavy duty aluminum foil Stir graham crumbs, sugar, and salt (if using) together Stir in melted butter Gently press crumbs into bottom of 9″×2¾″ round springform pan, with maybe a slight rise along the edge Place pan on a baking sheet and bake at 350°F for 10 minutes in the lower third of the oven Wait for pan to cool Triple wrap pan in 18″ squares of heavy duty aluminum foil. Crimp around top edge of pan Bake Cheesecake 1½ lb cream cheese (750 g), room temperature beat 4 minutes beat 4 minutes beat beat beat 1 minute each beat beat pour pour about halfway up bake at 325°F about 1 hour turn off oven, crack door 1″, and let cool 1 hour cool outside of oven cover and chill 4&plus hours ½ c granulated sugar (100 g) Pinch of salt 1½ t vanilla extract 3 large eggs ½ c sour cream (120 mL) ½ c heavy whipping cream (35% milk fat) (120 mL) 1 crust in pan , from above ~2 L water, to be boiled boil Beat cream cheese for 4 minutes on medium, until smooth, soft, and creamy Beat sugar into cream cheese for 4 minutes more Beat in salt Beat in vanilla Beat in each egg for 1 minute Beat in sour cream until incorporated Beat in heavy cream until incorporated Pour filling on top of crust and smooth top Boil 2 L water Put pan in a larger roasting pan, and pour water around it, up to about halfway up the inner pan Bake at 325°F for about an hour. When finished baking, the outer ring of the cheesecake should look slightly puffed and set, but the inner circle should still jiggle just a little bit, like Jell-O after it has set Turn off oven and crack door 1″ (maybe put a folded-over oven mitt in the door to prop it open), letting cake cool for 1 hour Take out of oven and cool to room temperature Cover top of cheesecake with foil, but do not touch cheesecake, and chill for 4 hours or overnight Top Cheesecake 1½ c sour cream (360 mL) stir chill spread 2 T powdered sugar (13 g) ¾ t vanilla extract 1 baked cheesecake, from above Stir together sour cream, powdered sugar, and vanilla Chill until ready to serve the cake Spread on cheesecake Serve with Raspberry Sauce 12 oz (340 g) fresh raspberries place in saucepan mash cook on medium about 5 minutes chill drizzle &frac38 c granulated sugar (100 g) &frac13 c water (80 mL) 1 topped cheesecake, from above cut Place raspberries, sugar, and water in a small saucepan Mash raspberries Heat on medium, whisking, about 5 minutes, until sauce begins to thicken Cool Drizzle on cheesecake pieces just before serving Makes 1 cheesecake. Lightly adapted from Simply Recipes , by reducing filling by ¼","tags":"Recipes","url":"https://blairconrad.com/pages/perfect-cheesecake/","loc":"https://blairconrad.com/pages/perfect-cheesecake/"},{"title":"Pie Crust (Crisco No Fail Pastry)","text":"2 c (272g) all-purpose flour combine cut stir ball flatten wrap chill ¾ t salt 190 g shortening 1 egg beat 1–2 T cold water 1 T white vinegar Combine flour and salt in a mixing bowl. Cut shortening into flour until mixture is uniform and shortening resembles large peas. Beat egg, water, and vinegar together. Pour liquid evenly over flour mixture. Stir with a fork until all of the mixture is moistened. Divide dough in half, roll into a ball, and flatten each into a ball about 10 cm in diameter. Wrap and chill dough 15 minutes for easier rolling. Stolen from an old Crisco package insert.","tags":"Recipes","url":"https://blairconrad.com/pages/pie-crust-crisco-no-fail-pastry/","loc":"https://blairconrad.com/pages/pie-crust-crisco-no-fail-pastry/"},{"title":"Pork Calvados","text":"1½ pounds pork tenderloin, in ¾″ slices sprinkle saute bake at 350°F 20 minutes stir bake 5-10 minutes sprinkle salt and pepper to taste 2 T butter 1 T butter saute 2-3 minutes boil 2 minutes 6 oz mushrooms, sliced 3 T Calvados ½ c chicken stock salt and pepper to taste ½ c 35% cream combine 2 t cornstarch 1-3 T toasted coarsely chopped hazelnuts, optional combine 1-2 t fresh thyme 1-2 t fresh parsley Sprinkle pork slices with salt and pepper. Saute pork in 2 T butter over medium heat until lightly browned. Transfer to a 2L casserole. Saute mushrooms gently in 1 T butter for 2-3 minutes. Add Calvados, chicken stock and salt and pepper to taste. Boil 2 minutes. Pour sauce onto pork. Cover casserole and bake for 20 minutes or until pork is cooked and tender. In a small bowl, combine cream and cornstarch. Stir into hot casserole. Bake 5 or 10 minutes until sauce is thickened. Combine hazelnuts with herbs and sprinkle over top of casserole. Suggestion: serve with sauteed apples for more appley goodness. Stolen from Dana Shortt, as seen in the Waterloo Chronicle , 24 June 2009.","tags":"Recipes","url":"https://blairconrad.com/pages/pork-calvados/","loc":"https://blairconrad.com/pages/pork-calvados/"},{"title":"Pumpkin Cheesecake Brownies","text":"2 (8 oz) packages cheese, at room temperature beat beat beat at low speed spread drop remaining ¼ of batter on cheesecake swirl sprinkle bake 35 minutes at 325° until cheesecake is set cool completely on wire rack ½ c pumpkin puree ¾ c sugar 2 t vanilla 1 t cinnamon ½ t ginger powder ¼ t freshly grated nutmeg ¼ t allspice ¼ c (regular, not light) sour cream 2 eggs ¾ c unsalted butter melt over low heat stir until hot and shiny, not bubbly remove from heat stir whisk stir spread ¾ of batter in pan 1&frac23 c sugar 1 c Dutch-process cocoa ½ t salt ½ t baking powder 1 T vanilla 3 eggs 1 c unbleached, all-purpose flour ¼ c dark chocolate chips Cheesecake mixture Beat the cream cheese until no lumps remain. Add in the pumpkin, sugar, vanilla, and spices, and beat until smooth. On low speed, add in the sour cream and eggs until well combined. Set aside. Brownies In a medium saucepan, melt the butter over low heat. Add the sugar and stir over low heat until combined and hot, not bubbly. You'll only need to heat it for a couple of minutes, until the mixture looks shiny. Remove the pan from the heat. Stir in the cocoa powder, salt, baking powder and vanilla. Whisk in the eggs until smooth. Stir in the flour. Assemble Pour about ¾ of the brownie mixture into a greased 9″×13″ pan. Spread with an offset spatula. Pour the cheesecake batter over the brownie layer and spread evenly with an offset spatula. Drop the remaining brownie batter on top of the cheesecake layer. Swirl the brownie dollops into the cheesecake making a pretty, abstract pattern. Sprinkle with the chocolate chips. Bake at 325° for 35 minutes or so, until the cheesecake layer is set. Let cool completely on a wire rack, then chill and store in the refrigerator. Stolen from Bake at 350","tags":"Recipes","url":"https://blairconrad.com/pages/pumpkin-cheesecake-brownies/","loc":"https://blairconrad.com/pages/pumpkin-cheesecake-brownies/"},{"title":"Pumpkin Chocolate Chip Bars","text":"2 c all-purpose flour whisk add, just barely stir bake at 350°F for 35–40 minutes cool on rack ⅛ t allspice ⅛ t cloves ½ t nutmeg ¾ t ginger &frac38 t salt 1 t baking soda 1½ t cinnamon 1 c unsalted butter, room temperature cream beat beat ⅔ c sugar 1 egg 2 t vanilla 1 c pumpkin puree 8 oz chocolate chips Line or grease 13″×9″ baking pan. Whisk together flour, spices, salt, and baking soda. Cream together the butter and sugar on medium-high until smooth. Beat in the egg and vanilla. Beat in the pumpkin puree (it will look curdled… do not worry). Add the dry ingredients and mix on low just until combined. Stir in the chocolate chips. Using an offset spatula (or the back of a spoon), spread the batter into the prepared pan. Bake at 350°F for about 35–40 minutes or until a toothpick comes out clean (or with a few crumbs, not batter). Cool completely in the pan. Makes about 24 pieces, if you don't cut them too big. Stolen from Bake at 350","tags":"Recipes","url":"https://blairconrad.com/pages/pumpkin-chocolate-chip-bars/","loc":"https://blairconrad.com/pages/pumpkin-chocolate-chip-bars/"},{"title":"Pumpkin Cranberry Spice Cake","text":"1¼ c all-purpose flour combine stir just until combined stir bake 40–50 minutes at 350°F cool on rack top 1 c whole-wheat flour 1 T cinnamon 2 t ground ginger 1 t allspice 1 t nutmeg 2 t baking powder 2 t baking soda 1 t salt 1½ c sugar beat well beat until smooth ⅓ c canola oil 3 large eggs 2 t vanilla extract 15 oz pumpkin 2 t grated orange zest 2 T orange juice 2 T water 1 c dried cranberries powdered sugar or orange glaze Brush a large Bundt TM pan with oil. Add a small amount of granulated sugar and turn to coat inside. Discard excess sugar. Stir together flours, cinnamon, ground ginger, allspice, nutmeg, baking powder, baking soda, and salt. Beat sugar, oil, eggs, and vanilla on high until thick and pale, about 3 minutes. Add pumpkin, zest, juice, and water to sugar mixture. Beat on low until smooth. Sift dry ingredients on top of pumpkin mixture and stir unti just combined. Stir cranberries into batter. Scape batter into pan, smooth top, and bake 40–50 minutes or a skewer inserted into the centre comes out clean. Turn cake out onto a wire rack to cool completely. Before serving, dust with icing sugar or coat with orange glaze.","tags":"Recipes","url":"https://blairconrad.com/pages/pumpkin-cranberry-spice-cake/","loc":"https://blairconrad.com/pages/pumpkin-cranberry-spice-cake/"},{"title":"Raisin Bread","text":"1 c water Add to bread machine pan in listed order Start it a-baking Add to pan when machine tells you to 2 T margarine 3 T sugar 1 t salt 2 t cinnamon 3 c flour 2½ t yeast ¾ c raisins Put water, margarine, sugar, salt, cinnamon, flour, and yeast in the bread machine pan. Start bread machine on \"regular\" bread setting. Add raisins when machine's \"add ingredients\" claxon sounds.","tags":"Recipes","url":"https://blairconrad.com/pages/raisin-bread/","loc":"https://blairconrad.com/pages/raisin-bread/"},{"title":"Raspberry Cheese Coffee Cake","text":"½ c pecans, chopped stir sprinkle Bake at 350°F for about 75 minutes Cool in pan for about 30 minutes ½ c sugar 1½ t cinnamon 2 T butter, melted 8 oz cream cheese, softened beat beat smooth top with rest of berries ¼ c granulated sugar 1 egg 1 t lemon zest 1½ c flour whisk stir, making 3 additions of flour and 2 of sour cream to butter mixture put in pan, mounding slightly top with 1 c of berries 1 t baking powder ½ t baking soda ¼ t salt &frac13 c butter, softened beat beat beat &frac23 c granulated sugar 2 eggs 2 t vanilla ½ c sour cream 1½ c fresh or frozen raspberries Stir pecans, the ½ c sugar, metled butter, and cinnamon together in a bowl. Set aside. Beat cream cheese and ¼ c sugar until fluffy. Beat in eggs and lemon zest until just smooth. Set aside. In a separate bowl, beat softened butter and &frac23 c sugar until well combined. Beat in eggs, one at a time. Beat in vanilla. In a separate bowl, whisk together flour, baking powder, baking soda, and salt. Stir flour mixture into butter mixture, alternating with sour cream: 3 additions of dry, 2 of sour cream. Spread batter in greased 8″×8″ baking pan, mounding slightly. Sprinkle 1 c of the berries on top. Gently spread cream cheese filling over blueberries. Sprinkle with remaining blueberries. Sprinkle evenly with crumb topping. Bake at 350°F for about 75 minutes, or until edge is set. Cool in pan on rack for 30 minutes. Makes 1 cake. Stolen from The Complete Canadian Living Cookbook","tags":"Recipes","url":"https://blairconrad.com/pages/raspberry-cheese-coffee-cake/","loc":"https://blairconrad.com/pages/raspberry-cheese-coffee-cake/"},{"title":"Real-Deal Chocolate Chip Cookies","text":"¼ c palm shortening, ghee, or grass-fed butter cream in food processor mix in food processor stir place golf-ball sized balls on cookie sheet flatten to final desired shape bake 9–12 minutes at 350°F ¼ c coconut palm sugar 2 T honey 1 large egg at room temperature 2 t vanilla 1½ c blanched almond flour 2 T coconut flour ½ t baking soda ¼ t salt ½ c chocolate chips In a food processor, cream the palm shortening, coconut sugar, honey, egg, and vanilla for about 15 seconds until smooth and fluffy. Add the almond flour, coconut flour, baking soda and salt and mix again until combined, about 30 seconds. Scrape down the sides of the bowl if needed in order to incorporate all of the flour. Pulse once or twice more. Stir in the chocolate chips by hand. Place golf-ball sized balls of dough on a cookie sheet. Flatten the cookies slightly with a flat-bottomed glass, the palm, or your hand or a spatula. The cookies don't spread much so create the size and thickness you want before baking them. Bake for 9–12 minutes at 350°F, until slightly golden around the edges. Stolen from Against All Grain","tags":"Recipes","url":"https://blairconrad.com/pages/real-deal-chocolate-chip-cookies/","loc":"https://blairconrad.com/pages/real-deal-chocolate-chip-cookies/"},{"title":"Rhapsody in Blueberry","text":"⅓ c granulated sugar mix mix put in a greased 9″ by 13″ pan bake 30 minutes at 375°F cool 10 minutes 2 T cornstarch 2 T freshly squeezed lemon juice 2 t grated lemon zest 6 c fresh blueberries 1 c quick-cooking rolled oats (not instant) mix mix ½ c all-purpose flour ⅓ c lightly packed brown sugar ½ t ground cinnamon ¼ c butter, melted 2 T apple or orange juice Sitr together white sugar, cornstarch, lemon juice and zest. Stir sugar mixture and blueberries together. Combine oats, flour, brown sugar, and cinnamon. Add melted butter to oat mixture, and stir until it resembles coarse crumbs. Put blueberries in a greased 9″ by 13″ baking dish. Top with crumb mixture. Bake a 375°F for 30 minutes, until blueberries are bubbling and topping is golden brown. Cool for 10 minutes before serving. Shamelessly stolen from [Eat, Shrink & Be Merry](http://janetandgreta.com/cookbook/eat-shrink-be-merry/)","tags":"Recipes","url":"https://blairconrad.com/pages/rhapsody-in-blueberry/","loc":"https://blairconrad.com/pages/rhapsody-in-blueberry/"},{"title":"Roast Turkey with Pan Gravy","text":"The interesting aspect of chef Andrew Taylor's turkey recipe is the sautéed onion, celery, carrot, sage, and thyme he adds to the cavity to flavour the turkey, then incorporates into the pan gravy. Roast Turkey 1 Spanish onion or 2 onions dice sauté cool mix stuff brush sprinkle foil and roast at 375°F for 1 hour remove foil roast, basting every 20 minutes, until thigh is 185°F, 1–1½ hours remove vegetable mixture to bowl place turkey on platter, tent, and let rest 2 stalks celery 1 large carrot 1 T vegetable oil 1 T chopped fresh sage 4 sprigs fresh thyme 1 bay leaf 5 peppercorns, crushed 1 turkey (12–14 lb/5.5–6.3 kg) pat dry 2 T butter, melted ¾ t salt ¼ t pepper Dice onion, celery, and carrot into small pieces. In skillet, heat oil over medium-high heat; sauté vegetables until lightly browned, 7–8 minutes. Let cool. Mix in sage, thyme, bay leaf and peppercorns. ( Make ahead: Refrigerate in airtight container for up to 8 hours. ) Pat turkey dry inside and out. Stuff body cavity with vegetable mixture; skewer cavity shut. Tie legs together and tuck wings under back. Place turkey, breast side up, on greased rack in roasting pan. Brush with butter; sprinkle with salt and pepper. Loosely cover with foil. Roast in 375° F oven for 1 hour. Remove foil; roast, basting with drippings every 20 minutes, until meat thermometer inserted into thickest part of thigh registers 185°F, 1–1½ hours. ( We need to flip the turkey so it cooks evenly. Breast-side up first, then back side, then back to breast-side. Flip when it looks nicely browned on that side—no hard-and-fast rules, just evaluate when you baste. ) Remove skewers. Spoon vegetable mixture and juices into a bowl; set aside for use in pan gravy. Lift turkey onto large cutting board; tent with foil and let stand for 15–30 minutes before carving . Makes about 1 turkey Pan Gravy 1 turkey roasting pan, full of drippings skim off all but 2 T fat whisk cook 2 minutes, scraping whisk add stir boil 15 minutes strain ¼ c all-purpose flour 6 cups Turkey Stock vegetables from inside turkey ¼ t salt ¼ t pepper 1 t lemon juice After roasting the turkey, skim all but 2 T of the fat from pan juices. Whisk flour into pan. Place pan over medium heat; cook for 2 minutes, stirring and scraping up any brown bits from bottom of pan. Gradually whisk in stock. Add scooped-out vegetable mixture to pan juices, along with salt and pepper. Bring to boil; boil, stirring constantly, until reduced by half, about 15 minutes. Stir in lemon juice, adding a little more to taste, if desired. Strain into warmed gravy boat. Substitution: You can use canned condensed chicken stock instead of turkey stock, but dilute it with twice the amount of water and omit the salt. Turkey Stock 1 turkey neck, chopped in 5 or 6 pieces boil skim cover and simmer 2 hours strain 1 turkey gizzard, halved 1 turkey heart, halved 8 cups water 1 onion, quartered 1 stalk celery with leaves, chopped 1 clove garlic 3 sprigs parsley 2 whole cloves 1 bay leaf ½ t salt ½ t peppercorns Place turkey parts in large saucepan with 8 cups of cold water. Bring to boil. Skim off foam. Add onion, celery, garlic, parsley, cloves, bay leaf, salt and peppercorns; cover and simmer over low heat for 2 hours. Strain. ( Make ahead: Let cool; refrigerate in airtight container for up to 2 days. ) Makes about 6 cups. Easy Turkey Carving Remove one leg: wiggle the leg to see where it joins the turkey. Insert the tip of a sharp knife into the hip joint to remove the leg. Cutting through the knee joint as above, separate the thigh from the drumstick. Cut the thigh meat off the bone, cutting parallel to the bone. If desired, repeat with drumstick. Remove one breast: cut down the center of the turkey and follow the ribcage down one side to the bottom. The breast should come right off. Cut the breast into thick slices across the grain. This yields more tender pieces than cutting along the length of the breast. Repeat all this on the other side. Now that the heavy meat is off the carcass, you don't need the wings to stabilise it. Remove the wings as you did the legs, piercing the shoulder joint with a sharp knife. Cut each wing into three pieces at the joints - a drumette, forearm, and crunchy delicious hand. Arrange pieces on a warmed serving platter and take to the table. Stolen from Canadian Living 's October 2003 issue — \"Thanksgiving at Langdon Hall\"","tags":"Recipes","url":"https://blairconrad.com/pages/roast-turkey-with-pan-gravy/","loc":"https://blairconrad.com/pages/roast-turkey-with-pan-gravy/"},{"title":"Roasted Cauliflower with Bacon and Garlic","text":"1 head cauliflower cut into small florets toss roast 20 minutes at 375°F season 6 (or more) cloves garlic halve 3 strips bacon cut into bite sized pieces 2 t olive oil salt pepper Preheat oven to 375°F. Cut the cauliflower into small florets. Halve the garlic cloves. Or make them as small as you want. On a baking sheet, toss together the cauliflower, garlic slices, bacon and olive oil. Roast for 20 minutes until bacon is crisp and cauliflower is cooked through. Season with salt and pepper to taste. Makes 4 servings (pretty generous ones) Shamelessly stolen from Jaden's Steamy Kitchen","tags":"Recipes","url":"https://blairconrad.com/pages/roasted-cauliflower-with-bacon-and-garlic/","loc":"https://blairconrad.com/pages/roasted-cauliflower-with-bacon-and-garlic/"},{"title":"Roasted Eggplant","text":"2½ medium eggplants cut into 1″ cubes toss rest for 30 minutes spread on baking sheet bake about 45 minutes at 375°, tossing occasionally 2 cloves of garlic finely crush ½ t salt 2–3 T vegetable oil Cut the eggplant into 1″ cubes and place in a bowl. Sprinkle salt, garlic and oil over eggplant. Toss well and allow to sit for 30 minutes for flavours to blend. Spread cubes in a single layer on a baking sheet. They'll roast better with a little space between them. Roast at 375° for about 45 minutes, tossing a few times during roasting. Roast until the cubes are brown and the insides creamy. Serve hot or at room temperature. Makes one pan of roasted eggplant cubes. Stolen from Fresh Bites Daily Variation: freezing Place 1 pound of cooled eggpant into a medium freezer bag. Freeze. You can get 9–10 bags out of a bushel of eggplant .","tags":"Recipes","url":"https://blairconrad.com/pages/roasted-eggplant/","loc":"https://blairconrad.com/pages/roasted-eggplant/"},{"title":"Rosemary Parmesan Shortbread","text":"1 c all purpose flour combine mix make two logs refrigerate slice bake 15–17 minutes at 350°F cool on baking sheet 5 minutes cool on wire rack ¾ c corn starch 1 t salt 1 c grated Parmesan cheese 1 T chopped fresh rosemary, or 1 t dried 1 c butter, softened beat until creamy 2 T sugar combine flour, corn starch, salt, cheese, and rosemary beat butter and sugar until light and creamy add dry ingredients to butter and mix until blended on a lightly floured surface, form dough into two 10″ logs wrap each log in plastic wrap and refrigerate 30 minutes or up to 24 hours grease or line baking sheets with parchment paper slice logs ¼″ thick and place 2″ apart on baking sheets bake 15–17 minutes at 350°F cool 5 minutes on cookie sheet remove to wire rack and cool Makes about 40 Stolen from Baking is Back 2007","tags":"Recipes","url":"https://blairconrad.com/pages/rosemary-parmesan-shortbread/","loc":"https://blairconrad.com/pages/rosemary-parmesan-shortbread/"},{"title":"Sablés au Chocolat et à la Fleur de Sel","text":"150 g butter, at room temperature cream until soft beat a minute or two mix until just combined mix just to incorporate roll into two logs chill 3+ hours cut into coins bake 12 minutes at 170°C/338°F 120 g light brown sugar 50 g sugar ½ t fleur de sel 1 t vanilla extract 175 g all-purpose flour sift together 30 g unsweetened cocoa powder ½ t baking soda 150 g dark chocolate, chopped into chips Sift the flour, cocoa and baking soda together. Cream the butter on medium speed until soft. Add both sugars, the salt and vanilla extract and continue beating for another minute or two. Pour in the flour and mix just until combined - work the dough as little as possible once the flour is added, and don't be concerned if the dough looks a little crumbly. Toss in the chocolate pieces and mix only to incorporate. Turn the dough out onto a work surface, gather it together and divide it in half. Working with one half at a time, shape the dough into logs that are 4 cm in diameter. Wrap the logs in plastic wrap and refrigerate them for at least 3 hours or overnight. Preheat the oven to 170°C/338°F. Line two baking sheets with parchment or silicone mats. Working with a sharp knife, slice the logs into rounds that are 1 cm thick. (The rounds are likely to crack as you're cutting them - don't be concerned, just squeeze the bits back onto each cookie.) Arrange the rounds on the baking sheets, leaving about 2 cm between them. Bake the cookies one sheet at a time for 12 minutes - they won't look done, nor will they be firm, but that's just the way they should be. Transfer the baking sheet to a cooling rack and let the cookies rest until they are only just warm, at which point you can serve them or let them reach room temperature. Makes about 35 cookies Shamelessly stolen from foodbeam","tags":"Recipes","url":"https://blairconrad.com/pages/sables-au-chocolat-et-a-la-fleur-de-sel/","loc":"https://blairconrad.com/pages/sables-au-chocolat-et-a-la-fleur-de-sel/"},{"title":"Salmon \"Bulgogi\"","text":"1 large garlic clove, peeled blend marinate 30 minutes roast at 500°F until 140°F in the center &frac13 c chopped green onions ¼ c reduced-sodium soy sauce or coconut aminos (or 2 T regular soy sauce) 1 T Chinese rice wine or dry Sherry 1″ cube peeled fresh ginger 2 t sugar 1 t Asian sesame oil ¾ t chili-garlic sauce 4 6-ounce center-cut skinless salmon fillets, or one big fillet Blend all ingredients except salmon in mini processor. Arrange salmon in a glass baking dish. Spoon marinade over (excess marinade may be heated and used as additional glaze when serving). Let marinate 30 minutes, if time allows, but at least 5 minutes. Preheat oven to 500°F. Roast fish until it reaches 140°F in the center, when checked with an instant-read thermometer. Variation: cook salmon from frozen Adds about 5 minutes to the cooking time. Stolen from epicurious .","tags":"Recipes","url":"https://blairconrad.com/pages/salmon-bulgogi/","loc":"https://blairconrad.com/pages/salmon-bulgogi/"},{"title":"Salted Banoffee Tart","text":"Crust 1¼ c all-purpose flour combine cut combine chill 2+ hours place in pan chill 30 minutes blind bake 10 minutes at 375°F blind bake 20 minutes at 350°F remove foil and continue baking 10 minutes 3 T sugar ¼ t fine sea salt 6 T cold unsalted butter, cut into pieces 1 large egg yolk mix ½ t cider vinegar 2 T cold water Filling 2 tins, 10 oz/300 mL each, sweetened condensed milk stir foil and place in water bath bake 2 hours at 425°F cool 20 minutes whisk chill ¼ t fleur de sel Assembly & Topping 2 large bananas, sliced toss spread pour sprinkle spread garnish 2 T rum 1 t pure vanilla extract prepared crust , from above chilled filling , from above 1 t fleur de sel 1½ cups whipping cream whip it stir 1½ t sugar ½ t pure vanilla extract Toasted sliced almonds or chocolate shavings, for garnish Make the Crust Combine flour, sugar, and salt in a large bowl. Cut in butter until mixture has a rough, crumbly texture Whisk egg yolk, vinegar and 2 T water together Pour liquid into flour and combine. If dough does not come together into a ball, add additional 1 T water. Shape dough into a disc, wrap and chill for at least 2 hours. Make the Filling Preheat oven to 425°F. Scrape condensed milk into a 9″ pie plate and stir in ¼ t fleur de sel. Cover pie plate with foil and place in a larger pan with sides at least 2″ high. Fill larger pan with 1″ water and place in oven. Cook condensed milk for 2 hours without stirring, topping water in pan halfway through cooking. Uncover pie plate to cool for 20 minutes, then scrape toffee into a bowl. Whisk until smooth and chill until ready to use. Bake the Crust While condensed milk is cooking, roll out chilled dough on a lightly floured surface to just under ¼″ thick. Line a 9″ removable-bottom fluted tart pan. Prick pastry with a fork and chill for 30 minutes. Reduce oven temperature to 375°F Line tart shell with foil and weight with dried beans or raw rice (about 2 c). Bake tart shell on a baking tray for 10 minutes. Reduce oven temperature to 350°F and bake 20 minutes more. Remove foil and weights and bake tart shell 10 minutes before removing to cool. Put it together Toss sliced bananas gently with rum and 1 t vanilla. Spread bananas in a single layer over cooled tart shell. Spread toffee filling over bananas Sprinkle toffee with 1 t fleur de sel. Whip cream to medium peaks Stir in sugar and remaining ½ t vanilla. Spread cream overtop toffee filling Garnish with toasted sliced almonds or chocolate shavings. Chill until ready to serve. Tart can be prepared up to a day in advance, and keeps for 2 days, refrigerated. Although it kind of slumps once cut. Makes about 1 tart. Shamelessly stolen from LCBO's Food & Drink","tags":"Recipes","url":"https://blairconrad.com/pages/salted-banoffee-tart/","loc":"https://blairconrad.com/pages/salted-banoffee-tart/"},{"title":"Scalloped Potatoes","text":"3 T butter melt cook add add cook until thick and bubbly simmer 8 minutes bake 45 minutes at 375°F 1 onion 3 T flour 2 t salt ¼ pepper 3 c milk 6 c thinly-sliced potatoes In a large saucepan, melt butter over medium-low heat. Cook onions until tender. Stir in flour until smooth and bubbly. Stir in salt and pepper. Gradually add milk, stirring until thick and bubbly. Add sliced potatoes to the sauce. Cover and simmer for 8 minutes. Transfer mixture to a greased 12x8 inch baking dish. Bake, uncovered, at 375°F for 45 minutes, or until potatoes are tender.","tags":"Recipes","url":"https://blairconrad.com/pages/scalloped-potatoes/","loc":"https://blairconrad.com/pages/scalloped-potatoes/"},{"title":"Steamed Salmon in Black Bean Sauce","text":"2 T black bean sauce combine steam sprinkle 1 T dark sesame oil 1 T chopped fresh ginger root ½ t pepper 2 T frozen orange juice concentrate 2 T water 4 6-oz (180 g) salmon or sea bass fillets, about 1 inch thick 2 green onions, finely chopped 2 T finely chopped fresh cilantro or parsley Set up a steaming unit by placing crisscrossed sets of chopsticks in the bottom of a wok or by placing a small rack in the bottom of a large, deep skillet. Fill with boiling water up to the rack or chopsticks. Combine black bean sauce, sesame oil, ginger, pepper, orange juice concentrate, and water. Pat fish dry and place in a single layer in an 8″/1.5 L baking dish. Pour sauce over fish. Put fish dish on steaming apparatus. Cover tightly (use foil if wok or skillet doesn't have a tight-fitting lid). Steam fish over high heat for 10 minutes, or until just cookied. Sprinkle fish with green onions and cliantro. Variation: if steaming isn't desired or practical, the fish can be covered with foil and baked at 425°F/220°C for 10 minutes, or just until it is cooked through. Shamelessly stolen from In the Kitchen with Bonnie Stern .","tags":"Recipes","url":"https://blairconrad.com/pages/steamed-salmon-in-black-bean-sauce/","loc":"https://blairconrad.com/pages/steamed-salmon-in-black-bean-sauce/"},{"title":"Sticky Rice","text":"1 c white sweet rice soak drain ½ c regular jasmine rice 1 c Chinese dried shiitake mushrooms soak drain, reserve ¼ c ½ c dried Chinese shrimp soak drain, reserve ¼ c drained mushrooms, from above chop fry 5–10 minutes cook, stirring occasionally cool a bit mix drained shrimp, from above 3 Chinese sausages fry all the fat out fry a bit 1 T Chinese cooking wine (optional) 1–2 t sugar (optional) 1 T soy sauce 1¼ c chicken broth bring to a boil bring to a boil reserved shrimp and mushroom liquids, from above drained rices, from above chopped cilantro (optional) chopped green onions (optional) Soak the sweet rice in water, same with the jasmine rice. Soak the mushrooms, shrimp in water as well. For at least 2–3 hours (I do it overnight sometimes or during the work day). Soak each the mushrooms and shrimp separately from the rice and each other. Drain the water from each of the ingredients, but from the mushroom and shrimp reserve ¼ cup of liquid to be used later. Using a food processor (or a knife and elbow grease), chop the mushrooms, shrimp and sausage (separately) into small pieces. In a frying pan, stir fry the sausage until all the fat comes out. Add the cooking wine and stir fry a bit. Then add the drained mushrooms and shrimp. sugar and soya sauce. Stir fry for about 5–10 minutes until it's all cooked. Add more soy sauce or sugar to taste. Set aside. In a large (preferably non stick) pot, add the chicken broth and ¼ cup of the reserved soaking liquid from the mushroom and shrimp to make 1¾ cups of liquid total. When it starts to boil, add the drained sweet and jasmine rice. Make sure the rice is immersed in the broth and wait to boil. Once it starts to begin to boil add the sausage, shrimp, mushroom stir fry. Leave to cook but mix periodically to make sure it doesn't stick to the bottom of the pot. After a while the broth will be absorbed and the rice will be cooked (around 10 minutes?). Then it is finished. When the rice has cooled a bit, mix in the chopped cilantro and green onions. Shamelessly stolen from Shirley McWhirley","tags":"Recipes","url":"https://blairconrad.com/pages/sticky-rice/","loc":"https://blairconrad.com/pages/sticky-rice/"},{"title":"Strawberry Rhubarb Pie","text":"1 c (250 g) sugar stir mix encase bake at 350°F for 50–60 minutes 2 T cornstarch 2 T quick-cooking tapioca pinch of salt 3 c hulled and quartered strawberries (375 g) 3 c rhubarb, sliced ½″ thick (375 g) 2 pie dough rounds Stir together the dry ingredients Mix fruit and dry ingredients Put the fruit mixture between the pie crust rounds, in a pie plate Bake at 350°F for 50–60 minutes Stolen from Williams-Sonoma Pie & Tart, ISBN 0-7432-4316-1 .","tags":"Recipes","url":"https://blairconrad.com/pages/strawberry-rhubarb-pie/","loc":"https://blairconrad.com/pages/strawberry-rhubarb-pie/"},{"title":"Stripey Chocolate Peanut Butter Loaf","text":"I like to make lots of stuff at once and freeze it. Also, I don't care for dividing batter. So this will make two loaves. That's why most of the ingredients are listed twice. Pay attention. You'll need 4 eggs for this, not 2, and 3 cups of milk, not 1½. 2× Peanut Butter Batter 2 c flour whisk stir stir 2 t baking powder ½ t baking soda ¼ t salt ⅔ c unsalted butter, softened beat beat beat ¾ c sugar 2 eggs 1 t vanilla ⅔ c natural peanut butter 1½ c milk We're going to make two loaves here. I find the easiest thing is to just make two batches of batter. So do all this twice. If you have a huge mixer, you can just put twice the ingredients in the bowl and then divide the batter later. Whatever works for you. Whisk together flour, baking powder, baking soda, and salt. Set aside. Beat butter and sugar until fluffly. Beat eggs and vanilla into the butter. Beat the peanut butter in to the butter mixture. Stir the flour mixture into the butter mixture. Stir in the milk until smooth. Now that you have two bowls of peanut butter batter, you can make one chocolatey and then stripe them up. Loaves 6 T cocoa powder whisk fold Spoon ¼ c of each batter into each of two greased loaf pans, jiggling between additions. Alternate colours until all batter is used. bake at 350°F for 60 minutes cool in pan for 10 minutes transfer to rack 6 T water one unit of batter the other unit of batter Whisk the cocoa powder with the water until it's combined, mostly to make it easier to mix into the batter. Fold the cocoa sludge into one of the bowls of peanut butter batter. Take two greased pans. In each one: Spoon a generous ¼ c of chocolate batter into the bottom. Jiggle the pan or gently spread the batter with a spoon. Spoon a generous ¼ c of peanut butter batter on top of that. Jiggle the pan or gently spread the batter with a spoon. Keep going until you're out of batter. Bake at 350°F for 60 minutes. Let cool in the pans for 10 minutes. Transfer from pans to rack. (If glazing, apply glaze now. Although if you're glazing, you probably wanted a sweeter bread and should've used more sugar. Allow to cool completely. Makes 2 loaves. Variation: Gluten-free Reduce liquid by ½ c or maybe ¾ c per loaf. Adapted from Canadian Living's Stripey Chocolate Peanut Butter Loaf . The original recipe was for one loaf and had the same (total) amount of sugar. Also a sugary topping. (It's just 1 T water plus ½ c icing sugar brushed on the top of each of the warm loaves.)","tags":"Recipes","url":"https://blairconrad.com/pages/stripey-chocolate-peanut-butter-loaf/","loc":"https://blairconrad.com/pages/stripey-chocolate-peanut-butter-loaf/"},{"title":"Taro Coconut Milk with Tapioca Pearls","text":"6 c water (for tapioca pearls) boil cook, covered, for 5 minutes rest, covered, for 30 minutes rinse with cold water stir 1 c tapioca pearls 6 c water (for taro) boil simmer on medium-low 15 minutes dissolve boil 2½ c taro, skinned and diced 1 can of coconut milk 1 c rock sugar Bring first 6 cups of water to a boil. Stir in sago pearls, cover and cook for 5 minutes. Remove from heat and leave covered for 30 minutes. Rinse pearls with cold water and set aside. Bring second 6 cups of water to a boil. Add taro to water, then reduce heat to medium-low and simmer for 15 minutes or until softened. Add coconut milk and sugar to taro water, stirring until dissolved. Bring taro to a boil and stir in tapioca pearls. Stolen from thejanechannel .","tags":"Recipes","url":"https://blairconrad.com/pages/taro-coconut-milk-with-tapioca-pearls/","loc":"https://blairconrad.com/pages/taro-coconut-milk-with-tapioca-pearls/"},{"title":"Thai Squash Soup with Shrimp","text":"1 stalk lemongrass bruise and cut put in slow cooker cook 4-6 hours on low discard ginger and lemongrass immersion blend to make smooth cook on high for 15 minutes add 4 c cubed peeled butternut squash 1 can (400 mL) coconut milk 2 c sodium-reduced chicken stock 1 T Thai red curry paste 6 thin slices of ginger root 1½ t fish sauce (or ½ t salt) ¼ t brown sugar 1 lb large raw shrimp, peeled and deveined 2 T thinly sliced fresh mint or basil Hit lemongrass with the top of a knife blade along the stalk to bruise; cut into 1-inch lengths. In slow cooker, combine squash, coconut milk, chicken stock, curry paste, ginger, lemongrass, fish sauce, and brown sugar. Cook 4-6 hours on low. Discard ginger and lemongrass. Whisk, mash, or blend squash to make a smooth soup. Add shrimp and cook for 15 minutes on high. Stir in (or top with) basil/mint. Shamelessly stolen from the January 2008 issue of Canadian Living .","tags":"Recipes","url":"https://blairconrad.com/pages/thai-squash-soup-with-shrimp/","loc":"https://blairconrad.com/pages/thai-squash-soup-with-shrimp/"},{"title":"Three-Headed Rolls","text":"⅓ c sugar mix dissolve cool mix add stir let rise until double punch let rise half an hour Bake at 300°F for about 20 minutes 1 t salt 2 T margarine 1¼-1 ⅓ c boiling water 1 T yeast dissolve cool ⅓ c warm water ½ t sugar 1 egg beat 4½-5 c flour Pour boiling water over sugar, salt, and margarine. Dissolve and cool Dissolve yeast and sugar in warm water. Cool Combine sugar and yeast mixture Beat 1 egg and add to yeast/sugar mixture Add flour Let rise until double in volume Punch, list rise again for half an hour Bake at 300°F for about 20 minutes","tags":"Recipes","url":"https://blairconrad.com/pages/three-headed-rolls/","loc":"https://blairconrad.com/pages/three-headed-rolls/"},{"title":"Toasting Nuts","text":"To toast nuts, spread raw nuts on a baking sheet and toast in a preheated 350°F (175°C) oven for about 10 minutes, stirring once or twice to make sure they are browning evenly. To test doneness, snap open a nut: it should break cleanly and the inside should be lightly browned throughout.","tags":"Recipes","url":"https://blairconrad.com/pages/toasting-nuts/","loc":"https://blairconrad.com/pages/toasting-nuts/"},{"title":"Triple Coconut Cream Pie","text":"1 baked pastry crust; ideally the coconutty one below, although any will do in a pinch fill top 3 T all-purpose flour (or cornstarch) whisk whisk whisk whisk bring to a simmer, stirring simmer 4–7 minutes, until thickened take off heat whisk, a few chunks at a time whisk cool chill, with plastic wrap on top ¼ t kosher salt ½ c plus 2 T (125 g) granulated sugar 1 vanilla bean † 2 large eggs 14 oz unsweetened coconut milk 2 oz milk 1½ c unsweetened shredded coconut 4 T (60 g) unsalted butter, diced 2 t vanilla extract † 1–2 t dark rum (optional) fancy topping ingredients, like below † Use vanilla bean or vanilla extract, not both Whisk flour, salt, granulated sugar, vanilla bean seeds, if using, in a saucepan until combined Whisk in eggs Whisk in milk and coconut milk Whisk in shredded coconut Place over medium heat on the stove and bring to a simmer, stirring Stir until custard thickens, 4–7 minutes Take off heat Whisk in butter, a few chunks at a time Whisk in vanilla extract and rum, if using Scrape into bowl, and cool a bit Press plastic wrap against the surface of the custard, and chill in fridge for several hours or overnight until fully cool Fill crust with pastry cream and smooth the top Put fancy toppings on top, just before serving Makes one pie. Coconutty Crust ½ c (37 g) loosely packed unsweetened shredded coconut mince stir cut stir mash chill roll blind-bake cool 1 c plus 2 T (150 g) all-purpose flour 1½ t granulated sugar ½ t kosher salt ½ c (115 g) unsalted butter, cold, diced 3 T (45 g) very cold water, plus a splash more if needed On a cutting board, mince coconut then scrape into a large bowl Add flour, sugar, and salt and stir to combine Add cold butter and use a pastry blender or your fingertips to work butter into flour until mixture resembles small peas Drizzle cold water over and stir to combine Mash into a disk Chill for 1 hour–1 week Roll and place in 9″ pie pan. Flute edge Blind-bake the pie crust Cool completely Coconutty Topping 1½ c unsweetened large-flake coconut or &frac23 c sweetened shredded coconut toast coconut at 350°F for 7–8 minutes 1½ c heavy whipping cream (35%), chilled whip it 2 T granulated sugar 1 t vanilla extract Chunk of white chocolate peel into curls Gently toast coconut at 350°F for 7–8 minutes. Watch it! Whip cream, sugar, and vanilla, until firm enough that peaks hold their shape Make chocolate curls using a vegetable peeler Stolen from Smitten Kitchen's interpretation of Tom Douglas's Triple Coconut Cream Pie .","tags":"Recipes","url":"https://blairconrad.com/pages/triple-coconut-cream-pie/","loc":"https://blairconrad.com/pages/triple-coconut-cream-pie/"},{"title":"Ultimate Banana Bread","text":"1¾ c (248 g) flour whisk stir fold put in prepared pan shingle sprinkle bake at 350°F 55–75 minutes, turning halfway cool 10 minutes in pan, then on rack 1 t baking soda ½ tsp salt 5 very ripe large bananas, peeled (with bonus banana below should total about 1 kg) cover and microwave until liquid is released, about 5 minutes drain 15 minutes, stirring occasionally, in a fine-mesh strainer. Collect juice (½–¾ c) collected juice reduce over medium-high heat to ¼ c mash whisk drained bananas ½ c (115 g) unsalted butter, melted and cooled 2 large eggs ¾ c packed (148 g) light brown sugar 1 t vanilla extract ½ c walnuts, toasted and coarsely chopped (optional) 1 additional large ripe banana, peeled (optional) 2 t granulated sugar Cover and microwave first 5 bananas about 5 minutes, until they are soft and have released their liquid. Drain bananas in fine-mesh strainer set over a bowl, stirring occasionally. After 15 minutes you should have between ½ and ¾ c liquid. Whisk together flour, baking soda and salt. Set aside. Reduce collected banana juice over medium-high heat until you have about ¼ c. Mash reduced juice back into bananas. Whisk butter, eggs, sugar, and vanilla into bananas. Barely stir flour mixture into banana mixture. Fold in walnuts, if using. Put the batter in a greased (or parchment papered) 8½″×4½″ loaf pan. If desired, shingle batter by layering 2 rows of banana slices down the pan, leaving a 1½″ gap in the middle. Keep banana slices away from edges of the pan, or they stick. Sprinkle sugar on top of batter. Bake at 350°F for 55–75 minutes, rotating pan halfway through the baking. Remove from oven when a toothpick inserted in the loaf comes out clean. Let loaf cool in pan for 10 minutes. Let load cool on rack for 1 hour before serving. Or cool completely and store at room temperature up to 3 days. Variation: from frozen bananas (at least the first 5) Instead of microwaving and draining fresh bananas, defrost frozen bananas in a fine mesh sieve over a bowl, collecting the liquid. Shamelessly stolen from America's Test Kitchen Ultimate Banana Bread","tags":"Recipes","url":"https://blairconrad.com/pages/ultimate-banana-bread/","loc":"https://blairconrad.com/pages/ultimate-banana-bread/"},{"title":"Whole Wheat Bread","text":"28g butter (2 T) Put in breadmaker Bake using whole wheat course 2 t salt 8 g milk powder (2 T) 20 g molasses (1 T) 46 g sugar (4 T) 450 g water (1&frac78 c) 570 g whole wheat flour (4¾ c) 32 g vital wheat gluten (4 T) 2 t active dry yeast Shamelessly stolen from Zojirushi model BB-PAC20 breadmaker recipe book.","tags":"Recipes","url":"https://blairconrad.com/pages/whole-wheat-bread/","loc":"https://blairconrad.com/pages/whole-wheat-bread/"},{"title":"\"Yard Waste Collection Schedule\"","text":"As I [first discussed in April 2014][user-sourced-yard-waste], here are a few Google Calendars that I created in order to track residential yard waste collection days for (sections of) the Region of Waterloo. I created the calendars from information I found on the Region's web site . I've made every effort to ensure that the calendars are correct, but nobody's perfect. I hope they're useful. I welcome any questions or comments. Day of Week Waterloo and Cambridge Monday iCal (good for importing into Calendar apps) XML HTML Tuesday iCal XML HTML Wednesday iCal XML HTML Thursday iCal XML HTML Friday iCal XML HTML [user-sourced-yard-waste]: {% post_url 2014-04-16-user-sourced-calendar-feeds-for-waterloo-yard-waste-pickup %}","tags":"pages","url":"https://blairconrad.com/pages/yard-waste-collection-schedule/","loc":"https://blairconrad.com/pages/yard-waste-collection-schedule/"},{"title":"Yorkshire Pudding","text":"3 large room temperature eggs whisk sift, in 3 stages rest stir bake 20 minutes at 450°F bake 10 minutes at 350°F pierce 1½ c milk, at room temperature ¾ t table salt (1½ t kosher salt) 1½ c all-purpose flour 1 T beef fat from your roast plus… another 2 T beef fat divide among muffin cups cook at 450°F until smoking In a large bowl, whisk together eggs, milk, and salt Sift in flour in three stages, each time whisking until flour disappears before adding more Let rest for 30 minutes–3 hours When the roast is done, spoon out 3 t fat Turn oven to 450°F Stir 1 T of the fat into the batter Add ½ t fat to each of 12 muffin cups Put fat in oven for 3 minutes until smoking hot Take out pan and fill each cup ⅔ full of batter Immediately return to oven and bake 20 minutes. Do not open oven door Reduce heat to 350°F and bake an additional 10 minutes until golden brown Remove from oven and pierce each pudding with toothpick to allow steam to escape and prevent collapse Makes 12 puddings Shamelessly stolen from Jaden's Steamy Kitchen","tags":"Recipes","url":"https://blairconrad.com/pages/yorkshire-pudding/","loc":"https://blairconrad.com/pages/yorkshire-pudding/"},{"title":"Automatically Sync nupkg and project.json Dependencies","text":"Recently while working on an open source .NET project, I forgot to update the .nuspec after changing a package dependency in my project.json . Of course the resulting nupkg contained the wrong dependency. Fortunately, the package wasn't published in that state, but I didn't want to risk such a thing happening again. I want the project to be buildable in Visual Studio immediately after cloning, but there's no such constraint on producing the NuGet package, so this means the project.json has to be the source of truth. I opted to have the project's simple-targets-csx build script scrape the project.json for the version of the dependent package and supply the matching version as part of the nuget pack properties option. My initial implementation used a regular expression to extract the version, but my colleague Thomas Levesque suggested parsing the JSON to find the proper value. I liked the idea, but pulling in something like Json.NET seemd heavy. A little Googling later, I found Brandur Leach's Using the Little-known Built-in .NET JSON Parser that described the built-in JsonReaderWriterFactory . This seemed like just the ticket. A few minutes later, I was up and running with these sections of the build script targets.Add( \"pack\", DependsOn(\"build\", \"outputDirectory\"), () => { var fakeItEasyVersion = GetDependencyVersion(\"FakeItEasy\"); Cmd(nuget, $\"pack {nuspec} -Version {version} -Properties FakeItEasyVersion={fakeItEasyVersion} -OutputDirectory {outputDirectory} -NoPackageAnalysis\"); }); … public string GetDependencyVersion(string packageName) { byte[] buffer = File.ReadAllBytes(projectJsonPath); XmlReader reader = JsonReaderWriterFactory.CreateJsonReader(buffer, new XmlDictionaryReaderQuotas()); XElement root = XElement.Load(reader); return root.Element(\"dependencies\").Element(packageName).Value; } which find the \"3.0.0-rc001-build000097\" from the project.json: { \"dependencies\": { \"FakeItEasy\": \"3.0.0-rc001-build000097\", \"StyleCop.Analyzers\": \"1.1.0-beta001\" }, … and combine it with this portion of the .nuspec : <dependencies> <dependency id=\"FakeItEasy\" version=\"[$FakeItEasyVersion$,4)\" /> </dependencies> Of course, a property could be added for each dependency. Do this, and you can rest easy, knowing you'll never get a dependency mismatch again.","tags":"Development","url":"https://blairconrad.com/2017/02/21/automatically-sync-nupkg-and-project.json-dependencies/","loc":"https://blairconrad.com/2017/02/21/automatically-sync-nupkg-and-project.json-dependencies/"},{"title":"Select May Not be Broken, But it's Bent","text":"Earlier this week at the day job I ran into an interesting problem working with a DataTable . A view that's supposed to show a subset of the table's rows showed nothing. I dropped into the debugger and became even more confused. Visually inspecting the DataTable showed that there was a row that matched the filter the view was using, but running Select still returned nothing. I didn't bring work's code home, but here's some code that reproduces the problem: Console.Out.WriteLine(\"0th book's Library:\\t{0}\", books[0].Library); Console.Out.WriteLine(\"# WPL books by Select:\\t{0}\", library.Books.Select(\"Library = 'WPL'\").Length); Console.Out.WriteLine(\"# WPL books by LINQ:\\t{0}\", library.Books.Count(book => book.Library == \"WPL\")); And the output: 0th book's Library: WPL # WPL books by Select: 0 # WPL books by LINQ: 1 So, the table contains at least one book from WPL , Select ing for that library doesn't find the book, yet iterating over all the rows and Count ing them does find it. Just in case you think there's some whitespace trickery going on or something, debug with me: How is this happening? The row's Library property is actually DBNull , but the dataset defines both DefaultValue and NullValue : So even though there was DBNull in the row, whenever I examined the Library property, via code (such as the LINQ statements) or visually in the debugger, it appeared to be \"WPL\". Select , however, wasn't fooled. It knew the value was DBNull and wouldn't match. I'm of two minds about this. It's arguably correct behaviour, as the stored value is not \"WPL\", but I'm not sure that it's desirable to be able to configure the table to present data in a way that's not supported by the query.","tags":"Development","url":"https://blairconrad.com/2015/10/05/select-may-not-be-broken-but-its-bent/","loc":"https://blairconrad.com/2015/10/05/select-may-not-be-broken-but-its-bent/"},{"title":"App Engine + External Authentication: Exposing Handlers to Cron, Tasks, and Admins","text":"Since Google is deprecating OpenID 2.0 support , I decided to update LibraryHippo to authenticate via OAuth 2.0 , which is a story in itself, but I'm here to talk about what happened next. LibraryHippo has a set of handlers that are accessed primarily via the Cron and Task Queue mechanisms, but every once in a while need to be triggered ad hoc by a human administrator. Up 'til now, these request handlers were protected from the rabble by requiring administrator status via the application's app.yaml . Unfortunately, externally-authenticated users have no special standing within App Engine, so this restriction had to be relaxed. My first thought was to remove the restriction from app.yaml and check for access in the handler like so: if (users.is_current_user_admin() or self.is_external_user_admin()) # application code that understands the logged-in users # do stuff else: self.abort(403) Unfortunately, this fails miserably. When the handler is executed by a task or cron job, users.is_current_user_admin returns False . This behaviour seems not to be widely reported; I couldn't find it mentioned in the App Engine issues list , but a web search eventually turned up App Engine: Google fails users.is_current_user_admin() test by Ben Davies, an article written nearly 5 years ago. In this article, Mr. Davies suggests that the best alternative to users.is_current_user_admin is to \"check the easily spoofed request user-agent\". I was skittish of this approach, especially since Google is now recommending checking X-AppEngine-Cron when securing URLS for cron . The App Engine documentation explains how X-AppEngine-Cron is protected against spoofing, but I'm still uneasy. I ended up taking a different approach. I added two routes for the affected handlers. One route is in the old \"admin\" subdirectory (subpath?) and the other in a new one for system commands, \"system\". The latter is secured in the app.yaml, just as before. Thus I have: # in app.yaml - url: /system/.* script: libraryhippo.application login: admin # in the application's Python source handlers = [ # other handlers ('/admin/notify/(.*)$', Notify), ('/system/notify/(.*)$', Notify), ] # and later, in the Notify handler request_path = urlparse.urlsplit(self.request.url).path if (self.is_external_user_admin() or not request_path.startswith('/admin/')) # do stuff else: self.abort(403) Thus the handler is executed if the user has admin rights or the URL isn't locked down by virtue of being below '/admin/'. The '/system/' URLs are all assumed to be protected by the app.yaml setting. Perhaps this is technically no better than checking a header in the request, but it works for me, at least until I see what happens with Issue 11576: have users.is_current_user_admin return true for tasks and cron jobs .","tags":"Development","url":"https://blairconrad.com/2015/01/05/app-engine-external-authentication-exposing-handlers-to-cron-tasks-and-admins/","loc":"https://blairconrad.com/2015/01/05/app-engine-external-authentication-exposing-handlers-to-cron-tasks-and-admins/"},{"title":"Hasty Impressions: flake8","text":"python flake8 HastyImpressions A little while ago, I was fixing a LibraryHippo issue in an area of the code that didn't have very good unit test coverage. As part of my fix, I moved one class from the main libraryhippo.py file to its own file. I integration-tested the fix, deployed the new version, and moved on. A day or so later, I noticed that I'd broken a very important side effect of the card-checking operation. The results of the check are cached and used later when sending out notifications. Because we still want to deliver a live report to users even if the caching fails, the failure is not detectable from the web page. Of course better test coverage would've picked this up, but it's the sort of error that just as easily would've been caught in a compiled language such as C#, or by a static analysis tool. So I decided to try out such a tool. A few web searches later, and it looked like flake8 was the thing to try. It bundles together PyFlakes pep8 , and mccabe None of which I'd heard of before, (at least as packages—I'd known of PEP 8 for a long time), but they claimed to do what I wanted. So I gave flake8 a go. Initial impressions Installation was as easy as running pip install flake8 . It picked up the missing dependencies automatically, and I was ready to go in seconds. Too impatient to read any docs, I ran it: LibraryHippo> flake8 Usage: flake8 [options] input ... flake8: error: input not specified A helpful error message. I reran, specifying the application directory ( flake8 App ) and got… rather a lot of output. It looked like this: app\\BeautifulSoup.py:100:1: E265 block comment should start with '# ' app\\BeautifulSoup.py:107:1: E302 expected 2 blank lines, found 1 app\\BeautifulSoup.py:114:1: E302 expected 2 blank lines, found 1 &vellip app\\cardchecker.py:61:13: F841 local variable 'name' is assigned to but never used app\\cardchecker.py:63:80: E501 line too long (96 > 79 characters) app\\cardchecker.py:71:37: F821 undefined name 'clock' app\\cardchecker.py:74:80: E501 line too long (84 > 79 characters) app\\cardchecker.py:75:1: W391 blank line at end of file &vellip and so on. Success! Too many complaints! Things seemed to be working, and it did identify the cause of my error ( app\\cardchecker.py:71:37: F821 undefined name 'clock' ), but it was buried in way too much other information. flake8 reported 823 problems in my application (and that didn't even include the test code). This is a common problem when running any kind of analysis tool for the first time. Since we've not been running a tight ship, there are all kinds of problems in the code. Fortunately, like many tools of its ilk, flake8 lets us tailor the problems that are reported. I had two immediate goals: ignore complaints about the library BeautifulSoup (over half the complaints were from BeautifulSoup.py), and ignore (for now) less severe errors and style warnings Now, how to do that? Command-line help I hoped that the tool would give me useful help if I asked, and it did: LibraryHippo>flake8 -h Usage: flake8 [options] input ... Options: --version show program's version number and exit -h, --help show this help message and exit -v, --verbose print status messages, or debug with -vv -q, --quiet report only file names, or nothing with -qq --first show first occurrence of each error --exclude=patterns exclude files or directories which match these comma separated patterns (default: .svn,CVS,.bzr,.hg,.git,__pycache__) --filename=patterns when parsing directories, only check filenames matching these comma separated patterns (default: *.py) --select=errors select errors and warnings (e.g. E,W6) --ignore=errors skip errors and warnings (e.g. E4,W) --show-source show source code for each error --show-pep8 show text of PEP 8 for each error (implies --first) --statistics count errors and warnings --count print total number of errors and warnings to standard error and set exit code to 1 if total is not null --max-line-length=n set maximum allowed line length (default: 79) --hang-closing hang closing bracket instead of matching indentation of opening bracket's line --format=format set the error format [default|pylint|<custom>] --diff report only lines changed according to the unified diff received on STDIN -j JOBS, --jobs=JOBS number of jobs to run simultaneously, or 'auto' --exit-zero exit with code 0 even if there are errors --builtins=BUILTINS define more built-ins, comma separated --doctests check syntax of the doctests --max-complexity=MAX_COMPLEXITY McCabe complexity threshold --install-hook Install the appropriate hook for this repository. Testing Options: --benchmark measure processing speed Configuration: The project options are read from the [flake8] section of the tox.ini file or the setup.cfg file located in any parent folder of the path(s) being processed. Allowed options are: exclude, filename, select, ignore, max-line-length, hang-closing, count, format, quiet, show- pep8, show-source, statistics, verbose, jobs, builtins, doctests, max- complexity. --config=path user config file location (default: C:\\PortableApps\\Home\\.flake8) Seems comprehensive. I bet a bunch of those would be quite handy in time, but right now, it looked like I wanted --select to limit the kinds of complaints. Based on the example, I figured I could use a single letter to include a whole class of complaints. And it looked like --exclude would keep flake8 from examining BeautifulSoup: LibraryHippo>flake8 App --select=F --exclude=BeautifulSoup.py App\\cardchecker.py:6:1: F401 'DeadlineExceededError' imported but unused App\\cardchecker.py:61:13: F841 local variable 'name' is assigned to but never used App\\cardchecker.py:71:37: F821 undefined name 'clock' There were complaints about other files, but the total count was down to 31, which was quite manageable. The \"F\" codes come from PyFlakes, and don't necessarily mean \"fatal\" like I first thought. Some, such as F841, are valid problems, but hardly catastrophic. Still, it was a small matter to fix all the \"F\"s in the code. Later on, I went back, expanding to changing the --select to F,E and eventually omitting it altogther. Command-line arguments are so tedious Always specifying options to ignore files or set maximum line lengths (79 is just too short even for an editor taking up the left half of my screen) can get old fast, so flake8 lets you put ever-repeated settings in a file. As promised, the following setup.cfg file lets me just run flake8 App and get the same results as above: [flake8] exclude = BeautifulSoup.py select = F Handy extension: naming It's possible to write extensions for flake8, and a number already exist. In fact, mccabe looks to be implemented as an extension (and maybe the others, I didn't check). On a lark, I went looking and found the pep8-naming extension, which checks PEP 8 naming conventions (something not fully covered by pep8, I guess). Installation via pip was simple and now flake8 --version reports 2.2.2 (pep8: 1.5.7, pyflakes: 0.8.1, mccabe: 0.2.1, naming: 0.2.2) CPython 2.7.2 on Windows (And just running flake8 shows a whole bunch of \"N8*\" complaints such as App\\wpl.py:227:17: N806 variable in function should be lowercase App\\gael\\memcache.py:16:11: N801 class names should use CapWords convention But that's my problem to deal with.) Writing new extensions doesn't look to be too difficult, if anyone listening is interested in helping the tool spell-check my symbol names (and maybe comments). Impressions A very useful static analysis tool that is easy to set up, runs quickly, and is configurable enough to start working on almost any codebase. So far the complaint descriptions seem good, and I can quickly resolve reported problems without consulting a manual. The extension ecosystem provides even more power, if you need it. Will I Use It? Absolutely, and you should too. I'm so committed to it that I wrote a \"deploy.bat\" file that will only deploy to Google App Engine if flake8 doesn't complain.","tags":"Development","url":"https://blairconrad.com/2014/09/15/hasty-impressions-flake8/","loc":"https://blairconrad.com/2014/09/15/hasty-impressions-flake8/"},{"title":"Making a Duck-Dog using FakeItEasy's CallsBaseMethod(s)","text":"A while back, Roman Turovskyy wrote FakeItEasy: Be Careful When Wrapping an Existing Object , an interesting post highlighting some of the difficulties of faking classes (as opposed to interfaces, which by virtue of having no behaviour of their own, are quite a bit more predictable). It's well-written and I enjoyed it, but he overlooked a small point. I figured others may easily make the same omission, so I'd like to explain why the example from that post works as it does, and to provide an alternative solution. Mr. Turovskyy supposes we want to fake out the following Dog class: public class Dog { public virtual string Bark () { return \"Bark!\" ; } public virtual string BarkBark () { return Bark () + Bark (); } } He notes that the default behaviour of a fake Dog, as made by dog = A.Fake<Dog>() , is for both Bark and BarkBark to return the empty string, which is not always desirable. His next step is to create a fake by wrapping a Dog object : Dog realDog = newDog(); Dog dog = A.Fake<Dog>(x => x.Wrapping(realDog)); Now Bark and BarkBark return the original (expected) strings. Then Mr. Turovskyy addresses customizing the fake object to change the way it barks. Here, things break down a little bit. His desired goal, of using A.CallTo to override Bark to return \"Quack!\" works, but when BarkBark is called, it still returns \"Bark!Bark!\" . He comments For those who know how virtual methods work, this looks very counter-intuitive. And that's completely true. The problem is that when a fake wraps an object, we're using a composition model, not inheritance . Thus the fake Dog knows to call the real dog's BarkBark method, but the real dog doesn't know about the fake Dog at all , so it just calls its own Bark method, which returns \"Bark!\". Using the Wrapping option and then overriding Bark on the fake is equivalent to writing this manual wrapper: public class WrappingDog: Dog { private readonly Dog realDog; public WrappingDog(Dog realDog) { this.realDog = realDog; } public override string Bark() { return \"Quack!\"; } public override string BarkBark() { return this.realDog.BarkBark(); } } Mr. Turovskyy suggests getting the desired behaviour by writing a manual FakeDog that overrides Bark , which will work, but is tedious and discards the benefits that FakeItEasy can provide. Another Way to Access Original Behaviour FakeItEasy can be used to get the desired behaviour. It provides a CallsBaseMethod method when configuring a fake. It does just what you'd hope it would. Witness: Dog dog = A.Fake<Dog>(); A.CallTo(() => dog.BarkBark()).CallsBaseMethod(); This tells the fake Dog to call the real Dog.BarkBark when its BarkBark method is invoked. When this is combined with an override for Bark , we can write this passing test: [Test] public void BarkBark_CallsBaseMethod_UsesOverriddenBark() { Dog dog = A.Fake<Dog>(); A.CallTo(() => dog.BarkBark()).CallsBaseMethod(); A.CallTo(() => dog.Bark()).Returns(\"Quack!\"); string result = dog.BarkBark(); Assert.That(result, Is.EqualTo(\"Quack!Quack!\")); } Call Base Methods More Conveniently As of FakeItEasy 1.24.0 , there's an additional way to do this, and it may appeal more to users who want many methods on their fake to call the original class's version. There's a new fake creation option called CallsBaseMethods . It was proposed by Aleksander Heintz , who also provided nearly the complete implementation. When used, it will cause every method on a fake to delegate to the faked type's implementation, if there is one. So the previous test could be written as [Test] public void BarkBark_CallsBaseMethod_UsesOverriddenBark() { Dog dog = A.Fake<Dog>(options => options.CallsBaseMethods()); A.CallTo(() => dog.Bark()).Returns(\"Quack!\"); string result = dog.BarkBark(); Assert.That(result, Is.EqualTo(\"Quack!Quack!\")); } The change in the first line means that when dog is created, every method will delegate to the version on Dog . Then Bark is overridden, and the base BarkBark is able to use the new version. Now we can realize our dream of having a Seussian DuckDog :","tags":"Development","url":"https://blairconrad.com/2014/08/29/making-a-duck-dog-using-fakeiteasys-callsbasemethods/","loc":"https://blairconrad.com/2014/08/29/making-a-duck-dog-using-fakeiteasys-callsbasemethods/"},{"title":"Automatically Printing Rake (or other Ruby) Variables","text":"The FakeItEasy rakefile contains a vars target (brainchild of Adam Ralph ) that can be used to print out the local variables defined in the script. Mostly these are static variables, such as the path to the NUnit command, but some, such as the upcoming FakeItEasy version, are computed. Logging these computed variables can help debug misbehaving builds. If ever something goes wrong, we can check the TeamCity build log and see something like this: assembly_info: Source/CommonAssemblyInfo.cs mspec_command: Source/packages/Machine.Specifications.0.8.0/tools/mspec-clr4.exe nuget_command: Source/packages/NuGet.CommandLine.2.8.0/tools/NuGet.exe nunit_command: Source/packages/NUnit.Runners.2.6.3/tools/nunit-console.exe nuspec: Source/FakeItEasy.nuspec output_folder: Build repo: FakeItEasy/FakeItEasy solution: Source/FakeItEasy.sln ssl_cert_file_url: http://curl.haxx.se/ca/cacert.pem version: 1.21.0 integration_tests: Source/FakeItEasy.IntegrationTests/bin/Release/FakeItEasy.IntegrationTests.dll Source/FakeItEasy.IntegrationTests.VB/bin/Release/FakeItEasy.IntegrationTests.VB.dll release_body: * **Changed**: _<description>_ - _#<issue number>_ * **New**: _<description>_ - _#<issue number>_ * **Fixed**: _<description>_ - _#<issue number>_ With special thanks for contributions to this release from: * _<user's actual name>_ - _@<github_userid>_ release_issue_body: **Ready** when all other issues forming part of the release are **Done**. - [ ] run code analysis in VS in *Release* mode and address violations (send a regular PR which must be merged before continuing) - [ ] check build, update draft release in [GitHub UI](https://github.com/FakeItEasy/FakeItEasy/releases) including release notes, mentioning non-owner contributors, if any … Originally , the vars task was hand-written, so whenever we added a new variable we had to update the task. Not too long ago, I added a new variable, and (surprisingly) remembered to update vars . However, Adam noticed that I had put the puts statement in the task in the wrong place, so the declaration order didn't match the printed order. A small thing, but the small things matter. So, we had a chat about the best way to present the variables. Declaration order is attractive, but I pushed a different approach: first, separating the variables with short values, such as assembly_info , from variables with long values, such as release_body . This keeps the short values from becoming lost in the noise of the longer ones. Second: sort lexicographically within the groups, to aid scanning. We came to an agreement, but as I started to make the change, I thought, \"Why make humans worry about this? Computers are good at partitioning and sorting.\" So, after a quick search for something that would allow printing of local Ruby variables, I found local_variables , and rewrote the task: desc \"Print all variables\" task :vars do print_vars(local_variables.sort.map { |name| [name.to_s, (eval name.to_s)] }) end def print_vars(variables) scalars = [] vectors = [] variables.each { |name, value| if value.respond_to?('each') vectors << [name, value.map { |v| v.to_s }] else string_value = value.to_s lines = string_value.lines if lines.length > 1 vectors << [name, lines] else scalars << [name, string_value] end end } scalar_name_column_width = scalars.map { |s| s[0].length }.max scalars.each { |name, value| puts \"#{name}:#{' ' * (scalar_name_column_width - name.length)} #{value}\" } puts vectors.each { |name, value| puts \"#{name}:\" puts value.map {|v| \" \" + v } puts \"\" } end Points of interest: The task delegates to a function right away, to avoid creating new variables that would be found by local_variables . The first thing the method does is partition variables into \"scalars\", to be rendered on the same line as the variable name, and \"vectors\", which have multiple elements or lines, and are rendered below the variable name. As a bonus, the scalar variable names padded so the values can all land on a \"tab stop\" Best of all, now we can add rake variables willy-nilly, with nary a thought about printing them out. It just happens.","tags":"Development","url":"https://blairconrad.com/2014/06/05/automatically-printing-rake-or-other-ruby-variables/","loc":"https://blairconrad.com/2014/06/05/automatically-printing-rake-or-other-ruby-variables/"},{"title":"Debugging a Pickle of a Stack Overflow on Google App Engine","text":"A few days ago, a user e-mailed me a bug report for LibraryHippo . For the previous 55 days, she'd been receiving the same (erroneous) morning e-mail about what library materials she had due. However, when she checked her family's account summary on the web page, it was correct. (The exact cause of the problem won't be of interest to many people, but I thought the way it presented, and how I diagnosed it, might help someone out. Read on!) I checked the logs, and sure enough, there was an error. Sort of like this: I 2014-04-28 14:10:43.429 checking [patron name redacted] Waterloo I 2014-04-28 14:10:47.550 saving checked card for [patron name redacted] E 2014-04-28 14:10:51.061 Failed to save checked card. Continuing. Traceback (most recent call last): File \"/base/data/home/apps/s~libraryhippo27/1.368776574735783966/libraryhippo.py\", line 380, in save_checked_card checked_card.payload = card_status File \"/base/data/home/runtimes/python27/python27_lib/versions/1/google/appengine/ext/db/__init__.py\", line 614, in __set__ value = self.validate(value) File \"/base/data/home/apps/s~libraryhippo27/1.368776574735783966/gael/objectproperty.py\", line 11, in validate result = pickle.dumps(value) File \"/base/data/home/runtimes/python27/python27_dist/lib/python2.7/pickle.py\", line 1374, in dumps Pickler(file, protocol).dump(obj) File \"/base/data/home/runtimes/python27/python27_dist/lib/python2.7/pickle.py\", line 224, in dump self.save(obj) File \"/base/data/home/runtimes/python27/python27_dist/lib/python2.7/pickle.py\", line 286, in save f(self, obj) # Call unbound method with explicit self File \"/base/data/home/runtimes/python27/python27_dist/lib/python2.7/pickle.py\", line 725, in save_inst save(stuff) File \"/base/data/home/runtimes/python27/python27_dist/lib/python2.7/pickle.py\", line 286, in save f(self, obj) # Call unbound method with explicit self [about 80 lines of stack trace elided] File \"/base/data/home/runtimes/python27/python27_dist/lib/python2.7/pickle.py\", line 663, in _batch_setitems save(v) File \"/base/data/home/runtimes/python27/python27_dist/lib/python2.7/pickle.py\", line 286, in save f(self, obj) # Call unbound method with explicit self File \"/base/data/home/runtimes/python27/python27_dist/lib/python2.7/pickle.py\", line 725, in save_inst save(stuff) File \"/base/data/home/runtimes/python27/python27_dist/lib/python2.7/pickle.py\", line 286, in save f(self, obj) # Call unbound method with explicit self File \"/base/data/home/runtimes/python27/python27_dist/lib/python2.7/pickle.py\", line 649, in save_dict self._batch_setitems(obj.iteritems()) File \"/base/data/home/runtimes/python27/python27_dist/...(length 98720) I 2014-04-28 14:10:51.275 Saved; key: __appstats__:043300, part: 190 bytes, full: 65479 bytes, overhead: 0.004 + 0.005; link: http://libraryhippo27.appspot.com/_ah/stats/details?time=1398708643356 Whenever LibraryHippo checks a patron's library card, it saves the results to the datastore to be used to construct the next day's e-mails, but an inability to save doesn't keep the results from being displayed on the web page. So that part made sense. The next step was to figure out what was going wrong with the save. The logs indicated that pickle was using more stack frames than were available. I try to debug it I added a copy of the offending card to my family's account on the live site. Same problem. Then I fired up the dev environment at home and did the same thing. Everything worked like a charm. That was unexpected. So, as a last resort, I started thinking. Why so deep? The structure that was being pickled ( CardStatus ) was quite flat. Here are the involved classes' definitions. Unless noted otherwise, everything is a string or datetime: class CardStatus: def __init__(self, card, items=None, holds=None): self.library_name = card.library.name self.patron_name = card.name self.items = items or [] # Items self.holds = holds or [] # Holds self.info = [] # strings self.expires = datetime.date.max class Thing(): def __init__(self, library, card): self.library_name = library.name self.user = card.name self.title = '' self.author = '' self.url = '' self.status = '' self.status_notes = [] # strings class Hold(Thing): def __init__(self, library, card): Thing.__init__(self, library, card) self.pickup = '' self.holds_url = '' self.expires = datetime.date.max class Item(Thing): def __init__(self, library, card): Thing.__init__(self, library, card) self.items_url = '' So, one level for the CardStatus , one for Thing , one for Hold (or Item ), one for a list, and one for the status_notes strings in the list. That's 5 levels. And probably pickle encodes a dict in a few of the of the complex types. Let's be generous and say 10 levels, each of which take maybe 5 nested pickle functions. That's about 50. Plus however deep we are in the stack before the pickling happens. That shouldn't be more than 100 . How deep is too deep? Popular wisdom on the web seems to be to increase the recursion limit when pickle runs into these kinds of problems. I was loath to do this, as I'd be constantly worrying about what depth to allow and whether it'd be enough and so on. While I dithered over that, my wife called to me from the television room, \"Just set the limit higher. That should help your user for now and it will give you more time to work on the problem.\" So I did. I was worried that the App Engine runtime wouldn't let me change the recursion limit, so I used sys.getrecursionlimit to log the depth, sys.setrecursionlimit , and sys.getrecursionlimit again to verify. Turns out that: The default recursion limit on App Engine's environment is 800 . You can change the limit. I went up to 20000 . In the dev environment the limit is 1000 . The higher limit on the production server fixed things. I relaxed a little, and started thinking. Maybe the 800/1000 difference accounted for things working at home, but not in production. I used sys.setrecursionlimit to change the limit at home, and reproduced the error. Huzzah! Now I could move more quickly. Inserting diagnostics into pickle Back to the question of why pickle was recursing so deeply. I don't routinely debug Python, relying instead on the power of logging statements. Thus, I decided to provide a custom pickler that did normal pickling things, but that also, for every object pickled, logged the stack depth, the object type, and its representation: import pickle import logging import traceback class SpyingPickler(pickle.Pickler, object): def save(self, obj): logging.info(\"depth: %d, obj_type: %s, obj: %s\", len(traceback.extract_stack()), type(obj), repr(obj)) super(SpyingPickler, self).save(obj) I ran this, and got reams of data. Scads and scads and scads, including what looked like to be large HTML documents. So I took out the repr(obj) and repeated. This was more manageable. … depth: 176, obj_type: depth: 179, obj_type: depth: 179, obj_type: depth: 182, obj_type: depth: 182, obj_type: depth: 182, obj_type: … Already we can see that we're quite deep in the stack, but the real surprise was the BeautifulSoup.NavigableString . LibraryHippo uses BeautifulSoup to scrape the libraries' web pages. A NavigableString is basically a Unicode string plus navigation. To support the navigation, the instance points at what looks to be a DOM model of the entire parsed HTML page . That explains the deep deep recursion. The offending objects were in the pickup field of the Hold class. A quick fix to ensure we're storing a plain string, and the problem was resolved . What a difference Aside from the obvious effect of correct e-mails, I was curious to see what other differences this change made. Before the fix, the serialization descended to a stack depth of 892 , and produced a 546221 -byte blob. After, the maximum depth is 52 , and the blob size is 10779 bytes. So everyone should benefit a little from lower resource usage and quicker card-checking. Fortunately, LibraryHippo isn't popular enough to exceed the free quotas, so I wasn't paying extra for compute or storage. Then again, if I had been, maybe I would've noticed the problem before a user did.","tags":"Development","url":"https://blairconrad.com/2014/05/05/debugging-a-pickle-of-a-stack-overflow-on-google-app-engine/","loc":"https://blairconrad.com/2014/05/05/debugging-a-pickle-of-a-stack-overflow-on-google-app-engine/"},{"title":"User-Sourced Calendar Feeds for Waterloo Yard Waste Pickup","text":"(Thanks to Jon Udell , whose insightful posts are the only reason this would ever have occurred to me.) I live in Waterloo, Ontario , a city with weekly residential waste collection. My collection day is Monday. In the warm months, there's an additional biweekly (that's every 2 weeks, not twice a week) collection of what's called yard waste : grass clippings, raked-up leaves, tree branches, and so on. People in my area (including me) have trouble remembering which weeks are yard waste weeks. This means that every two weeks I get to see collections of plant material left at the curb outside people's homes and then brought in a day or two later, only to reappear the next week. I use tools to augment my memory. Every year, at the beginning of the warm season, I visit the Region of Waterloo's website where I can learn about their Yard waste residential collection program . Here I find links to two PDFs: a pretty version of the schedule for Waterloo , and an \"accessible\" version. I look at one of those schedules, and I create a recurrning Google Calendar event. Then my phone tells me on Sunday night whether to haul the tree parts to the street. It works really well. However, every year, I've wished that the Region provided an .iCalendar feed or created a Google Calendar for me so I wouldn't have to do this. But I haven't done more than wish. This year, I'm doing more. For starters, I created a few Google Calendars. One for me (and my fellow Monday-pickup people), and four for everyone else in Waterloo (and Cambridge). The Region's web site gave me start and end weeks for pickups, so it was very easy to make a biweekly repeating event, using these Google Calendar settings: Lo and behold, it shows up in my calendar: I created the Tuesday through Friday calendars by making the obvious modification to the schedule above. I put [all the calendars on a separate page][yard-waste-collection-schedule] that I intend to maintain until the Region provides replacements. Or I move. Go! Get one for your collection day. Next, I'm going to pester the Region, showing them how easy it was to do this and to see if they'd be willing to carry on with the work next year. If something comes of it, I'll let you know. [yard-waste-collection-schedule]: {{ site.url }}/yard-waste-collection-schedule/","tags":"Miscellany","url":"https://blairconrad.com/2014/04/16/user-sourced-calendar-feeds-for-waterloo-yard-waste-pickup/","loc":"https://blairconrad.com/2014/04/16/user-sourced-calendar-feeds-for-waterloo-yard-waste-pickup/"},{"title":"Limit FakeItEasy extension scanning with a bootstrapper","text":"As of version 1.18.0 , a client-supplied bootstrapper can be used to determine which external assembly files are scanned during startup. Last time, I talked about how [FakeItEasy extension scanning had improved in version 1.13.0][betterscan]. While this change has dramatically improved startup times in many situations, we recently received a comment from one of our valued clients (and subsequently a pull request with a proposed solution ), detailing a situation where startup was taking about 13 seconds, mostly due to a huge number of assemblies in the working directory. Disabling shadow copy creation by the test runner alleviated the pain, but the incident prompted a re-examination of the issue. While disabling shadow copies should resolve most slow startup problems caused by excessive working directory assemblies, and it may improve performance in other ways , recommending this to clients has always felt like a bit of a dodge to me, essentially pushing the problem off to someone else. There was also the lingering fear that someone would come back with a reason why the shadow copies were necessary. We wanted to provide FakeItEasy's clients with a little more control over the process of scanning for assemblies. So, we've implemented the originally-proposed bootstrapper solution. Using a custom bootstrapper By default, after scanning all FakeItEasy-referencing assemblies currently loaded in the AppDomain, FakeItEasy 1.18.0 will examine all DLLs in the working directory. This behaviour can be changed by including in the AppDomain a class that implements FakeItEasy.IBootstrapper . As I write, this is the only behaviour that the bootstrapper controls: /// <summary> /// Provides a list of assembly file names to scan for extension points, such as /// <see cref=\"IDummyDefinition\"/>s, <see cref=\"IArgumentValueFormatter\"/>s, and /// <see cref=\"IFakeConfigurator\"/>s. /// </summary> /// <returns> /// A list of absolute paths pointing to assemblies to scan for extension points. /// </returns> IEnumerable<string> GetAssemblyFileNamesToScanForExtensions();</code></pre> The best way to implement the interface is to **extend `FakeItEasy.DefaultBootstrapper`**. This class defines the default FakeItEasy setup behaviour, so using it as a base allows clients to customize only those aspects of the initialization that matter to them. While any list of assembly files can be provided by `GetAssemblyFileNamesToScanForExtensions`, I expect that most extensions that are defined will already be loaded in the current AppDomain, so the most common customization will be to disable external assembly scanning, like so: <pre><code class=\"csharp\">public class NoExternalScanningBootstrapper : FakeItEasy.DefaultBootstrapper { public override IEnumerable<string> GetAssemblyFilenamesToScanForExtensions() { return Enumerable.Empty<string>(); } } Of course, if there were extensions defined in an external assembly file or two, the GetAssemblyFilenamesToScanForExtensions implementation could return the paths to just those assemblies. [betterscan]: {% post_url 2013-07-08-better-formatter-auto-discovery-in-fakeiteasy-1.13.0 %}","tags":"Development","url":"https://blairconrad.com/2014/03/06/limit-fakeiteasy-extension-scanning-with-a-bootstrapper/","loc":"https://blairconrad.com/2014/03/06/limit-fakeiteasy-extension-scanning-with-a-bootstrapper/"},{"title":"Better formatter auto-discovery in FakeItEasy 1.13.0","text":"A few weeks ago, I wrote about the problems that FakeItEasy's assembly scanning was causing while it was looking for user-defined extensions. To recap, FakeItEasy was scanning all assemblies in the AppDomain and the working directory, looking for types that implemented IArgumentValueFormatter , IDummyDefinition , or IFakeConfigurator . This process was quite slow. Worse, it raised LoaderLock exceptions when debugging, and Runtime errors anytime I ran my tests using the ReSharper test runner. At that time, I'd opened issue130 , intended to allow configuration of the scanning procedure. I'm happy to say that the issue has been closed \"no fix\". Instead, I've contributed the fix for Issue 133 — Improved performance of assembly scanning . It doesn't introduce any configuration options, but streamlines the scanning process. The original behaviour was: find all the DLLs in the application directory load all the found DLLs find the distinct assemblies among those loaded from the directory and those already in the AppDomain scan each assembly and add all the types to a list The new behaviour , heavily inspired by Nancy 's bootstrapper-finding code, is: find all the DLLs in the application directory discard DLLs that are already part of the AppDomain - We don't even have to crack these files open again, since we already know everything about them. Note that this check examines the absolute paths to the DLL and the loaded assembly, and will be fooled by shadow copying . So, if your test runner makes shadow copies, this time won't be saved. I turned off shadow copying with no ill effects (and a tremendous speedup), but your mileage may vary. load each remaining DLL for reflection only - This may be faster, and it may not, but it has another big advantage - it doesn't cause any of the code in the assembly to execute . (It was the execution of the assembly code that caused my LoaderLock and Runtime errors.) for each assembly that references FakeItEasy, fully load it - If we don't do this, we can't scan for all the types in the assembly because When using the ReflectionOnly APIs, dependent assemblies must be pre-loaded or loaded on demand through the ReflectionOnlyAssemblyResolve event according to the error I got when I tried it . Note that excluding assemblies that don't reference FakeItEasy means we only examine assemblies that could possibly define formatting/dummy/configuration extensions , cutting down on the scanning time. scan each of the following, remembering all contained types: the assemblies we just loaded from files, the AppDomain assemblies that reference FakeItEasy, and FakeItEasy - We need to include FakeItEasy explicitly because it defines its own formatter extensions, and since we're otherwise only looking at assemblies that reference FakeItEasy, we'd miss it. This new scanning behaviour has been released in the FakeItEasy 1.13.0 build , and has been a boon to me already. I'm enjoying the faster test runs (0.534 seconds for my first test, versus 1.822 (or more)) and the improved stability of the test runner. NuGet it now.","tags":"Development","url":"https://blairconrad.com/2013/07/08/better-formatter-auto-discovery-in-fakeiteasy-1.13.0/","loc":"https://blairconrad.com/2013/07/08/better-formatter-auto-discovery-in-fakeiteasy-1.13.0/"},{"title":"Watch your spaces - HTTP Error 500.19 - Internal Server Error","text":"Late last week at the Day Job, a colleague came to me with a problem. The web service he was trying to hit was throwing an error he'd never seen before: HTTP Error 500.19 - Internal Server Error The requested page cannot be accessed because the related configuration data for the page is invalid. I'd never seen it before either, at least not in this exact incarnation. Take a look In case the text isn't so clear, here are the details: Module IpRestrictionModule Notification BeginRequest Handler WebServiceHandlerFactory-Integrated-4.0 Error Code 0x80072af9 Requested URL http://localhost:80/My.Virtual.Directory/Service.asmx Physical Path C:\\inetpub\\wwwroot\\My.Virtual.Directory\\Service.asmx Logon Method Not yet determined Logon User Not yet determined The errors suggested that we have problems with the configuration file, but the web.config was present (and well-formed), and there were no obvious permission problems, so it seems the file was being read. There was nothing in the event logs. Web searches yielded nothing that matched the 0x80072af9 error code or the description of the error. Even ERR.exe, recommended by Troubleshooting HTTP 500.19 Errors in IIS 7 , failed me. Fortunately, there were sibling virtual directories on the server, and they were working fine, even under the same App Pool. I knew that this virtual directory, unlike the others, restricted access to a whitelist of IP addresses. So, I changed the security/ipSecurity node's allowUnlisted to true , just in case for some reason the clients' IP addresses weren't being detected properly. No change. Frustrated, I removed the whole security node. The service worked! So I took a closer look at the node: <security> <ipSecurity allowUnlisted=\"false\"> <add ipAddress=\"127.0.0.1\" allowed=\"true\" /> <add ipAddress=\"1.2.3.4 \" allowed=\"true\" /> </ipSecurity> </security> Check out that \"1.2.3.4\" ipAddress. Now check it again. It's actually \"1.2.3.4 \", with a space at the end. (I bolded the space there, just so you wouldn't miss it.) It seems that this messes up the IP parsing, and IIS is completely flummoxed. Remove the space, and all is well.","tags":"Development","url":"https://blairconrad.com/2013/07/02/watch-your-spaces-http-error-500.19-internal-server-error/","loc":"https://blairconrad.com/2013/07/02/watch-your-spaces-http-error-500.19-internal-server-error/"},{"title":"Fixated on Fixie - the birth of a new unit test runner","text":"I enjoy reading about how software is made, and I like unit testing frameworks. So, when I heard about Patrick Plioi 's new project Fixie , I rushed to check it out. In this case, \"check it out\" doesn't mean \"clone the repo and dig around the source code\". Nor does it mean \"install the NuGet package and build something\". Although I may do those things in the future. Nope. It means I read Mr. Plioi's articles about Fixie and its development. And I am having a great time. Moreso than hearing about Fixie's features (or more often lack of features), I'm enjoying seeing Mr. Plioi's approach to setting up a new project , including: prototyping the scariest integration points first the importance of starting out with a one-click build, for himself and for potential future contributors streamlining, automating, or eliminating as much ceremony as possible bootstrapping, and more! The articles are well-written and articulate, and mildly funny. They're trending a little more into the implementation of Fixie itself, rather than guiding philosophies, but I still find them interesting. And it's worth noting that all the while I was enjoying the articles, I was thinking in the back of my head \"this is a great exercise, and very instructive, but I've no interest in actually using Fixie—I'm content with [NUnit](http://www.nunit.org/)\". Until I read DRY Test Inheritance . I really liked the low-ceremony way conventions are used to locate test setups and teardowns. It hooked me. Even though I am usually not a fan of test class inheritance and the scheme described in this article has more weight than the Default Convention . Of course, we'll probably never switch at the Day Job, at least not until the ReSharper test runner supports Fixie, but it might be fun to use for a small home project.","tags":"Development","url":"https://blairconrad.com/2013/06/24/fixated-on-fixie-the-birth-of-a-new-unit-test-runner/","loc":"https://blairconrad.com/2013/06/24/fixated-on-fixie-the-birth-of-a-new-unit-test-runner/"},{"title":"FakeItEasy's argument formatter auto-discovery - boon and inconvenience","text":".NET FakeItEasy Hi again. At the Day Job, we've recently dropped Typemock Isolator and NMock2 as the mocking frameworks of choice in the products that I work on. We've jumped on the FakeItEasy bandwagon. So far, we're enjoying the change. FakeItEasy is powerful enough and the concepts and syntax fit the mind pretty well. Today I'm going to focus on one feature that I've really enjoyed but that has been an occasional thorn in the side. This is a feature that Patrik Hägne has blogged about before , but that I think is still not well known. I found it accidentally, and have benefited from it. You can provide custom argument renderers to improve the messages you get when FakeItEasy detects an error due to missing or mismatched calls. Check out Mr. Hägne's post for the full details, but if I may be so bold as to rip off some of his examples, here's the gist (original meaning, not fancy github one). Define a class that extends ArgumentValueFormatter<Person> (where Person is a class in your project), override GetStringValue with something that renders a Person, and FakeItEasy errors that need to talk about a Person change from this Assertion failed for the following call: 'FakeItEasy.Examples.IPersonRepository.Save()' Expected to find it exactly never but found it #1 times among the calls: 1. 'FakeItEasy.Examples.IPersonRepository.Save( personToSave: FakeItEasy.Examples.Person)' to Assertion failed for the following call: 'FakeItEasy.Examples.IPersonRepository.Save()' Expected to find it exactly never but found it #1 times among the calls: 1. 'FakeItEasy.Examples.IPersonRepository.Save( personToSave: Person named Patrik Hägne, date of birth 1977-04-05 (12227,874689919 days old).) ' It's very easy to use, and quite helpful. However, lately I've had a few difficulties with some test projects and have tracked it back to an aspect of this feature. Specifically, for certain very large projects My test fixtures are taking a long time to start up - several extra seconds while waiting for the first test to run. Specifically, the delay was happening in my first A.Fake call. During this delay, several \" LoaderLock was detected \" popups appear, which have no obvious ill effect, but are very annoying, and Finally, after a recent upgrade of dependent libraries, when I run the tests using the Resharper test runner , I see a \"Microsoft Visual C++ Runtime Library Runtime Error! \" in JetBrains.ReSharper.TestRunner.CLR4.exe. It claims that I'm trying to \"use MSIL code from this assembly during native code initialzation\". The tests continue to run, but the TestRunner process never exits, and needs to be killed before test can be run again. The reasons all these things are happening during the first FakeItEasy call is due to the way that FakeItEasy finds the custom ArgumentValueFormatter implementations. It scans all available assemblies , looking for any implementations. In this case, \"all available assemblies\" means every assembly in the AppDomain as well as all *.dll files in the current directory. This actually makes the feature a little more powerful than Mr. Hägne indicated—you can define your extensions in other assemblies than the test project's. In fact, this is how FakeItEasy finds its own built-in ArgumentValueFormatter s (one for null , one for System.String , and one for any System.Object that doesn't have its own extensions). FakeItEasy is in the AppDomain, so its extensions are located by the scan. One benefit of doing such a wide scan is that it's possible to define the formatter extension classes in a shared library that can be used across test projects. It's the scanning that's causing my pain. First, some of the solutions at the Day Job are quite large, with dozens of assemblies in the test project's AppDomain and build directory. Even if everything went well, it would take seconds to load and scan all those assemblies. Second, some of the DLLs in the directory aren't under our control. Some aren't managed. Some don't play well with others. It's these ones that are causing the other problems I mentioned above. Loading these assemblies causes them to be accessed in ways that they were never planned to be , which causes the LoaderLocks and Runtime Error. What now? We're investigating the assemblies we're using to see if we can't access them in a better way, but that's probably going to be a slow operation, and one that may not bear fruit. In the meantime, I've forked FakeItEasy and am using the custom build in the one project that it was causing the most pain. The custom version only loads extensions from the FakeItEasy assembly . It's kind of a terrible hack, and means that we can't define custom extensions, but we hadn't for that project anyhow, so it's not yet causing pain. On the brighter side, there are no more errors or popups, and the tests start much more quickly. Longer term, I've created FakeItEasy issue 130 to make the extension location a little more flexible . Once accepted and implemented, it will give the user control over how extension classes are located during FakeItEasy startup. (Then I can resume using the vanilla FakeItEasy at the Day Job.) If you're curious, pop on over and take a look.","tags":"Development","url":"https://blairconrad.com/2013/06/17/fakeiteasys-argument-formatter-auto-discovery-boon-and-inconvenience/","loc":"https://blairconrad.com/2013/06/17/fakeiteasys-argument-formatter-auto-discovery-boon-and-inconvenience/"},{"title":"ReportGenerator indexing your whole drive? Check the case of your fullPaths.","text":"[Update on 2013-06-22: I should've mentioned this a while ago, but the issue and patch I submitted were accepted and built into ReportGenerator 1.7.3.0, so if you have anything newer, you should be good.] Recently I was working on a project at the Day Job, using OpenCover 1.7.1.0 and ReportGenerator 4.0.804 to report my test coverage, as is my wont , when the report generation started taking figuratively forever . Investigating, I saw something like found report files: D:/sandbox/project/src/buildlogs/temp_test_coverage/Project.UnitTest.coverage.xml Loading report 'D:\\sandbox\\project\\src\\buildlogs\\temp_test_coverage\\Project.UnitTest.coverage.xml' Preprocessing report Indexing classes in directory 'D:\\sandbox\\project\\src\\Module1\\SubPath\\' Added coverage information of 370/370 auto properties to module 'Module1' Indexing classes in directory 'D:\\' My D: drive isn't the hugest, but it's big enough, so that explained the delay. And of course, I certainly didn't want anything above D:\\sandbox\\project\\src indexed. I took a peek at my .coverage.xml file and the ReportGenerator code and until I found the offending lines <Module dhash=\"9A-A3-0A-C0-1D-57-BA-2A-C2-D4-5B-9E-08-DE-BD-2D-46-04-AF-32\"> <FullName>D:\\Sandbox\\project\\src\\Module\\UnitTest\\bin\\Release\\Module.dll</FullName> <ModuleName>Module</ModuleName> <Files> … <File uid=\"803\" fullPath=\"D:\\sandbox\\project\\src\\Module\\File1.cs\" /> <File uid=\"806\" fullPath=\"D:\\Sandbox\\project\\src\\Module\\File2.cs\" /> <File uid=\"808\" fullPath=\"D:\\sandbox\\project\\src\\Module\\File3.cs\" /> … Note the \"Latin capital letter S\" at the beginning of \"Sandbox\" on the line with uid 806. All the other lines had a \"Latin small letter S\". When ReportGenerator goes looking for *.cs files to scan, it starts at the directory whose name is the longest common prefix of all the fullPaths. Because \"S\" isn't \"s\", it came up with \"D:\\\". I submitted an issue on the ReportGenerator CodePlex project , so maybe we'll see a fix soon. Of course I wondered \"Why does the S differ for that entry?\" but I figured I'd look at one thing at a time, and locating the fix for ReportGenerator was quicker.","tags":"Development","url":"https://blairconrad.com/2012/12/07/reportgenerator-indexing-your-whole-drive-check-the-case-of-your-fullpaths./","loc":"https://blairconrad.com/2012/12/07/reportgenerator-indexing-your-whole-drive-check-the-case-of-your-fullpaths./"},{"title":"Moving LibraryHippo to Python 2.7 - OpenID edition","text":"Now that Google has announced that Python 2.7 is fully supported on Google App Engine , I figured I should get my act in gear and make convert LibraryHippo over. I'd had a few aborted attempts earlier, but this time things are going much better. How We Got Here - Cloning LibraryHippo One of the requirements for moving to Python 2.7 is that the app must use the High Replication Datastore , and LibraryHippo did not. Moreover, the only way to convert to the HRD is to copy your data to a whole new application . So I bit the bullet, and made a new application from the LibraryHippo source. When you set up a new application, you have the option of allowing federated authentication via OpenID . I'd wanted to do this for some time, so I thought, \"While I'm changing the datastore, template engine, and version of Python under the hood, why not add a little complexity?\", and I picked it. The Simplest Thing That Should Work - Google as Provider In theory, LibraryHippo should be able to support any OpenID provider, but I wanted to start with Google as provider for a few reasons: concentrating on one provider would get the site running quickly and I could add additional providers over time I need to support existing users - they've already registered with App Engine using Google, and I want things to keep working for them, and I wanted to minimize my headaches - I figure, if an organization supports both an OpenID client feature and an OpenID provider, they must work together as well as any other combination. Even though there's been official guidance around using OpenID in App Engine since mid-2010, I started with Nick Johnson's article for an overview - he's never steered me wrong before. And I'm glad I did. While the official guide is very informative, Nick broke things down really well. To quote him, Once you've enabled OpenID authentication for your app, a few things change: URLs generated by create_login_url without a federated_identity parameter specified will redirect to the OpenID login page for Google Accounts. URLs that are protected by \"login: required\" in app.yaml or web.xml will result in a redirect to the path \"/_ah/login_required\", with a \"continue\" parameter of the page originally fetched. This allows you to provide your own openid login page. URLs generated by create_login_url with a federated_identity provider will redirect to the specified provider. That sounded pretty good - the existing application didn't use login: required anywhere, just create_login_url (without a federated_identity , of course). So, LibraryHippo should be good to go - every time create_login_url is used to generate a URL, it'll send users to Google Accounts. I tried it out. It just worked, almost. When a not-logged-in user tried to access a page that required a login, she was directed to the Google Accounts page. There were cosmetic differences, but I don't think they're worth worrying about: standard Google login page federated Google login page After providing her credentials, the user was redirected to a page that asked her if it was okay for LibraryHippo to know her e-mail address. After that approval was granted, it was back to the LibaryHippo site and everything operated as usual. However, login: admin is still a problem . I really shouldn't have been surprised by this, but login: admin seems to do the same thing that login: required does - redirect to /_ah/login_required, which is not found. This isn't a huge problem - it only affects administrators (me), and I could workaround by visiting a page that required any kind of login first, but it still stuck in my craw. Fortunately, the fix is very easy - just handle /_ah/login_required . I ripped off Nick's OpenIdLoginHandler , only instead of offering a choice of providers using users.create_login_url , this one always redirects to Google's OpenId provider page. With this fix, admins are able to go directly from a not-logged-in state to any admin required page. class OpenIdLoginHandler(webapp2.RequestHandler): def get(self): continue_url = self.request.GET.get('continue') login_url = users.create_login_url(dest_url=continue_url) self.redirect(login_url) ... handlers = [ ... ('/_ah/login_required$', OpenIdLoginHandler), ... ] Using Other Providers With the above solution, LibraryHippo's authentication system has the same functionality as before - users can login with a Google account. It's time to add support for other OpenID providers. username .myopenid.com I added a custom provider picker page as Nick suggested, and tried to login with my myOpenID account, with my vanity URL as provider - blair.conrad.myopenid.com. The redirect to MyOpenID worked just as it should , and once I was authenticated, I landed back at LibraryHippo, at the \"family creation\" page, since LibraryHippo recognized me as a newly-authenticated user, with no history. myopenid.com Buoyed by my success, I tried again, this time using the \"direct provider federated identity\" MyOpenID url - myopenid.com. It was a complete disaster . Once MyOpenID had confirmed my identity, and I was redirected back to the LibraryHippo application, App Engine threw a 500 Server Error. There's nothing in the logs - just the horrible error on the screen. In desperation, I stripped down my login handler to the bare minimum, using the example at Using Federated Authentication via OpenID in Google App Engine as my guide. I ended up with this class that reproduces the problem: class TryLogin(webapp2.RequestHandler): def get(self): providers = { 'Google' : 'www.google.com/accounts/o8/id', 'MyOpenID' : 'myopenid.com', 'Blair Conrad\\'s MyOpenID' : 'blair.conrad.myopenid.com', 'Blair Conrad\\'s Wordpress' : 'blairconrad.wordpress.com', 'Yahoo' : 'yahoo.com', 'StackExchange': 'openid.stackexchange.com', } user = users.get_current_user() if user: # signed in already self.response.out.write('Hello %s ! [ sign out ]' % ( user.nickname(), users.create_logout_url(self.request.uri))) else: # let user choose authenticator self.response.out.write('Hello world! Sign in at: ') for name, uri in providers.items(): self.response.out.write('[ %s ]' % ( users.create_login_url(dest_url= '/trylogin', federated_identity=uri), name)) ... handlers = [ ('/trylogin$', TryLogin), ('/_ah/login_required$', OpenIdLoginHandler), ... ] Interestingly, both Yahoo! and WordPress work, but StackExchange does not. If it weren't for Yahoo!, I'd guess that it's the direct provider federated identities that give App Engine problems (yes, Google is a direct provider, but I consider it to be an exception in any case). Next steps For now, I'm going to use the simple \"just Google as federated ID provider\" solution that I described above. It seems to work, and I'd rather see if I can find out why these providers fail before implementing an OpenID selector that excludes a few providers. Also, implementing the simple solution will allow me to experiment with federated IDs on the side, since I don't know how e-mail will work with federated IDs, or how best to add federated users as families' responsible parties. But that's a story for another day.","tags":"Development","url":"https://blairconrad.com/2012/03/12/moving-libraryhippo-to-python-2.7-openid-edition/","loc":"https://blairconrad.com/2012/03/12/moving-libraryhippo-to-python-2.7-openid-edition/"},{"title":"Best all-around .NET coverage tool - OpenCover","text":"This is the gala awards show, where my chosen coverage tool is announced. If you've come this far, you've probably already read the title, and it won't surprise you to learn that I've chosen OpenCover . It offered the best fit for my requirements - the only areas where I found it lacking were in the \"nice to haves\". Witness: OpenCover is pretty easy to run from the command line - second only to NCover. It can (with the help of ReportGenerator) generate coverage reports in XML and HTML . OpenCover has an integrated auto-deploy , so it can be bundled with the source tree and new developers or build servers just work - dotCover has no such option, and I was not able to use NCover this way. I've been able to link with TypeMock Isolator with little trouble, and the new Isolator may obviate the need for my small workaround. It's free . Aside from the obvious benefit, it's nice to not have to count licenses when adding developers and/or build server nodes. There's no GUI integration , but this was a nice to have. If some developer is absolutely dying to have this, my boss's boss has indicated that money could be available for individual licenses of something like dotCover. There's no support for integrating with IIS . We don't need this right now, so that's okay. Again, if we one or two developers find a need, we have the option of buying a license of some other tool. Even better, support may be coming soon . After considering OpenCover's strengths in the areas I absolutely needed, and its weaknesses, which all appear to be in areas that I care a little less about, I recommended it the boss's boss, who agreed with the assessment and was happy to keep a little money in his pocket for now. So, I grabbed 2.0.802, incorporated it into one product's build, and out popped coverage numbers. Very exciting. I did notice a few things, though: Branch coverage has been added since I last evaluated the product! One fairly complicated integration-style testfixture is not runnable under OpenCover - the class tested creates a background thread and starting the thread results in a System.AccessViolationException . I was unable to determine the cause of this, and have temporarily removed the test from coverage, instead executing it with NUnit directly. I'm going to continue investigating this problem. Since I'm XCopy deploying, I was bitten by the dependency on the Microsoft Visual C++ 2010 Redistributable Package - I ended up including the DLLs in my imported bundle, and all was well, but I worry a little about the stability of this solution. The time taken to execute our tests (there are over 5000, and many hit a database) increased from about 7 minutes to about 8. This is an acceptable degradation, since the test run isn't the bottleneck in our build process. The number of \"Cannot instrument as no PDB could be loaded\" messages is daunting. I'm hoping that things will be improved once I get a build that contains a fix for issue 40 .","tags":"Development","url":"https://blairconrad.com/2011/12/15/best-all-around-.net-coverage-tool-opencover/","loc":"https://blairconrad.com/2011/12/15/best-all-around-.net-coverage-tool-opencover/"},{"title":"Hasty Impressions: NCover","text":"[TOC] I tried NCover 3.4.18.6937. The Cost NCover Complete is $479 plus $179 for a 1-year subscription (which gives free version updates). I thought this was a little steep . NCover Classic is $199/$99. I looked at NCover Complete, because that's the kind of trial version they give out. Also, the feature set for Classic was too similar to that offered by other tools that cost less. Check out the feature comparison , if you like. Support I haven't had enough problems to really stress the support network, but I will say this - the NCover chaps are really keen on keeping in touch with people who have trial copies of the program. I've received 3 separate e-mails from my assigned NCover rep in the 2 weeks since I first installed my trial copy. I replied to one of these, asking for a clarification on the VS integration (see below), and got a speedy response. It's nice to see such a high level of customer support , but I do feel just a little bit smothered… VS integration The best advice from the NCover folks is to create an external tool to launch NCover . That's an okay solution if you want to run all the unit tests in a project and profile them, but it lacks flexibility . Then to actually look at the report, you have to launch the NCover Explorer and load the report. There's additional advice at the end of the Running NCover from Visual Studio video - if you want a more integrated Visual Studio experience, you should obtain TestDriven.Net . That probably works well enough, but I'm not wild about paying an additional $189 per head (roughly) for a test runner that (in my opinion, and excepting the NCover integration of course) is a less robust solution than the one that comes bundled with ReSharper . Oh. There's one more feature that I found - once you are examining a coverage report, you can Edit in VS.NET , which opens the appropriate file in Visual Studio. This is somewhat convenient, but doesn't warp you to the correct line, which is a bit of a letdown. Command Line Execution The command line offers many and varied options for configuring the coverage run. Here's a sample invocation: NCover.Console.exe //exclude-assemblies BookFinder.Tests //xml ..\\..\\coverage.nccover nunit-console.exe bin\\debug\\BookFinder.Tests.dll Upon execution, NCover tells me this: Adding the /noshadow argument to the NUnit command line to ensure NCover can gather coverage data. To prevent this behavior, use the //literal argument. I really like that it defaults to passing the recommended /noshadow to NUnit. The // switches are also a good touch - it makes providing arguments to the executable being covered a lot easier. These features make the command line invocation the best I've seen among coverage tools. GUI Runner The GUI runner looks just like a GUI wrapper on top of the command line options - they appear to support the same level of configuration. After the tests have been run, the NCoverExplorer allows one to browse the results and to save a report as XML or HTML. XML Report Reports are generated either from the GUI runner or by using the NCover.Reporting executable, which has a plethora of options for choosing XML or HTML reports of various flavours. XML reports contain all the information you might want to summarize for inclusion in build output, but they're hard to understand . Witness: <stats acp=\"95\" afp=\"80\" abp=\"95\" acc=\"20\" ccavg=\"1.5\" ccmax=\"5\" ex=\"0\" ei=\"1\" ubp=\"12\" ul=\"40\" um=\"10\" usp=\"39\" vbp=\"63\" vl=\"89\" vsp=\"105\" mvc=\"18\" vc=\"2\" vm=\"22\" svc=\"120\"> If you stare at this long enough (and correlate with a matching HTML report), you figure out that this means that there are 39 u nvisited s equence p oints, and 105 v isited s equence p oints along with various other stats, so using attribute extraction and Math, we could see that 105/144 or 72.9% of the sequence points are covered. It's odd that there are many more reports available for HTML than XML. Notably absent from the XML offering: \"Summary\". What is it about summaries that make them unsuitable for rendering as XML when HTML is fine? Reports of Auto-Deploy My Support Guy explained that you can xcopy deploy NCover using the //reg flag, but I did not find any documentation on how to do this. Support Guy claims there is an \"honour system\" kind of licensing model that supports this, but the trial copy I had did not work this way. I eventually abandoned this line of investigation. Mature Isolator Support From Visual Studio, under the Typemock menu, configure Typemock Isolator to Link with NCover&nsbsp3.0. When using the TypeMockStart MSBuild task, use <TypeMockStart Link=\"NCover3.0\" ProfilerLaunchedFirst=\"true\"> and it just works , assuming you have TypeMock Isolator installed or set to auto-deploy . IIS IIS coverage is available, simply by selecting it from the GUI runner options or from the command line using the //iis switch. Other Windows Services can be covered in the same manner. Note though, that these features are only available in the Complete flavour of NCover 3.0. Sequence Point coverage Supported , as well as branch point coverage and other metrics, including cyclomatic complexity . Nice options to have, although probably a little advanced for my team's current needs and experience. Conclusion Pros: sequence point and branch coverage large feature set, including trends, cyclomatic complexity analysis, and much much more commercial product with strong support report merging easy IIS profiling supports Isolator Cons: costly weak IDE integration inconsistent (comparing XML to HTML) report offerings confusing auto-deploy I expected to be blown away by NCover—from all reports, it's the Cadillac of .NET coverage tools. After demoing it, I figured I'd end up desperately trying to make a case to the Money Guy to shell out hundreds of dollars per developer (and build server), but this did not happen. While NCover definitely has lots of features, it's lacking some pretty important ones as well, notably IDE integration. Other features just weren't as I expected - the cornucopia of report types is impressive, but overkill for a team just starting out, and many of the report types aren't available in XML and/or are very minor variations on other report types. Ultimately, I don't see what NCover offers to justify its price tag, especially across a large team. If ever I felt a need to have one of the specialized report, I'd consider obtaining a single license for tactical use, but I can't imagine any more than that.","tags":"Development","url":"https://blairconrad.com/2011/11/09/hasty-impressions-ncover/","loc":"https://blairconrad.com/2011/11/09/hasty-impressions-ncover/"},{"title":"Hasty Impressions: OpenCover","text":"OpenCover is developed by Shaun Wilde. He was a developer on (and is the only remaining maintainer of) PartCover. He's used what he learned working on PartCover to develop OpenCover, but OpenCover is a new implementation, not a port. I tried OpenCover 1.0.514. Since I downloaded a couple weeks ago there have been 3 more releases, with the 1.0.606 release promising a big performance improvement. The Cost Free! And you can get the source. VS integration None that I can find. Command Line Execution Covering an application from the command line is easy , and reminiscent of using PartCover the same way. I used this command to see what code my BookFinder unit tests exercised: OpenCover.Console.exe -arch:64 -register target:nunit-console.exe -targetargs:bin\\debug\\BookFinder.Tests.dll \\ -output:..\\..\\opencover.xml -filter:+[BookFinder.Core]* Let's look at that. -arch:64 - I'm running on a 64-bit system. I didn't get any results without this. -register - I'm auto-deploying OpenCover. More on that later. -target:nunit-console.exe - I like NUnit -targetargs:bin\\debug\\BookFinder.Tests.dll - arguments to NUnit to tell it what assembly to test, and how. -output:..\\..\\opencover.xml - where to put the coverage results. This file is not a report - it's intended for machines to read, not humans. -filter:+[BookFinder.Core]* - BookFinder.Core is the only assembly I was interested in - it holds the business logic. GUI Runner There isn't one, but I have to wonder if there won't be. Otherwise, why call the command line coverer OpenCover.Console.exe ? XML Report OpenCover doesn't generate a human-readable report. Instead, you can postprocess the coverage output. ReportGenerator is the recommended tool , and it works like a charm. ReportGenerator.exe .\\opencover.xml XmlReport Xml generates an XML report in the Xml directory. The summary looks like this: <?xml version=\"1.0\" encoding=\"utf-8\"?> <CoverageReport scope=\"Summary\"> <Summary> <Generatedon>2011-08-05-2011-08-05</Generatedon> <Parser>OpenCoverParser</Parser> <Assemblies>1</Assemblies> <Files>5</Files> <Coverage>71.6%</Coverage> <Coveredlines>126</Coveredlines> <Coverablelines>176</Coverablelines> <Totallines>495</Totallines> </Summary> <Assemblies> <Assembly name=\"BookFinder.Core.DLL\" coverage=\"71.6\"> <Class name=\"BookFinder.BookDepository\" coverage=\"85.7\" /> <Class name=\"BookFinder.BookListViewModel\" coverage=\"50\" /> <Class name=\"BookFinder.BoolProperty\" coverage=\"50\" /> <Class name=\"BookFinder.BoundPropertyStrategy\" coverage=\"0\" /> <Class name=\"BookFinder.ListProperty\" coverage=\"75\" /> <Class name=\"BookFinder.Property\" coverage=\"100\" /> <Class name=\"BookFinder.StringProperty\" coverage=\"100\" /> <Class name=\"BookFinder.ViewModelBase\" coverage=\"81\" /> </Assembly> </Assemblies> </CoverageReport> ReportGenerator also generates Html and LaTeX output, with a \"summary\" variant for each of the three output types. The XML report would be most useful for inclusion in build result reports, but I found the HTML version easy to use to examine coverage results down to the method level. I appreciate the coverage count by each of the lines - not as fancy as dotCover's \"which tests cover this\", but it could be a helpful clue when you're trying to decide what you need to do to improve your coverage. Joining Coverage Runs Perhaps your test are scattered in space or time and you want to get an overview of all the code that's covered by them. OpenCover doesn't really do anything special for you, but ReportGenerator has your back . Specify multiple input files on the command line, and the results will be aggregated and added to a comprehensive report: ReportGenerator.exe output1.xml;output2.xml;output3.xml XmlReport Xml DIY Auto-Deploy There's no built-in auto-deploy for OpenCover. However, I made my own auto-deployable package like so: install OpenCover copy the C:\\Program Files (x86)\\OpenCover directory somewhere - call this your package directory uninstall OpenCover - you won't need it any more Then I just made sure my coverage build step knew where the OpenCover package directory was (for the build system at the Day Job, I added it to our \"subscribes\") used the -register flag mentioned above to register OpenCover before running the tests That's it. No muss, no fuss. I did a similar (but easier, since there's no registration needed) trick with ReportGenerator, and all of a sudden I have a no-deploy system. In less than an hour's work, I could upgrade a project so the build servers and all the developers could run a coverage target, with no action on their part, other than pulling the updated source tree and building. (Which is pretty much what the build server does all day long anyhow...) DIY (for now) Coverage with Isoloator Isoloator and OpenCover don't work together out of the box, but thanks to advice I got from Igal Tabachnik , Typemock employee, it was not hard to change this. Isolator's supported coverage tools are partly configurable. There is a typemockconfig.xml under the Isolator install directory - typically %ProgramFiles (x86)%\\Typemock\\Isoloator\\6.0 (or %ProgramFiles% , I suppose). Mr. Tabachnik had me add <Profiler Name=\"OpenCover\" Clsid=\"{1542C21D-80C3-45E6-A56C-A9C1E4BEB7B8}\" DirectLaunch=\"false\"> <EnvironmentList /> </Profiler> to the ProfilerList element, and everything meshed. His StackOverflow answer provides full details and suggests that official support for OpenCover will be added to Isolator. IIS I can't find any special IIS support. I'm not saying OpenCover can't be used to cover an application running in IIS, only that I didn't find any help for it. I may investigate this later. Sequence Point coverage OpenCover counts sequence points, not statements. Yay! Conclusion Pros: free open source active project XML/HTML/LaTeX reports (via ReportGenerator) report merging (via ReportGenerator) Isolator support is easy to add (and may be included in future Isolators) auto-deploy package is easy to make Cons: no IDE integration no help with IIS profiling I really like OpenCover. It's easy to use, relatively full-featured, and free. In a work environment, where there's a tonne of developers who want the in-IDE profiling experience, it may not be the best bet, but I'd use it for my personal .NET projects in a flash.","tags":"Development","url":"https://blairconrad.com/2011/08/15/hasty-impressions-opencover/","loc":"https://blairconrad.com/2011/08/15/hasty-impressions-opencover/"},{"title":"Hasty Impressions: PartCover","text":"Technical stuff PartCover has a GUI runner as well as a command-line mode. It integrates with Isolator, but doesn't offer any help for those wanting to profile IIS-hosted applications. There are some XSL files provided that allow one to generate HTML reports, but probably the better way is to use ReportGenerator to make HTML or XML reports. PartCover claims to be auto-deployable, but I did not try this. Project Concerns The hardest thing about working with PartCover is learning about PartCover - finding definitive information about the project's state is quite difficult. Searching with Google finds the SourceForge project which contains a note to see latest news on the PartCover blog , which hasn't been updated since 17 June 2009. Back at SourceForge, you can download a readme written by Shaun Wilde, which says that he's the last active developer and has moved development to a GitHub project . At last! A project with recent (26 June 2011) updates. Unfortunately, my trials did not end here. I tried a number of versions, each with their own quirks. Unfortunately, I did not keep as careful track of which version had which problem as I should, and can't say which version (from either GitHub or SourceForge) had which problems, but I can describe the problems. At first I thought things were working really well, but then noticed that I had abnormally high coverage levels on my projects - one legacy project that I knew had about 5% coverage was registering as over 20%! I looked at one assembly's summary and found 6 classes with 0% coverage and one with 80%, and the assembly was registering an 80%. It turns out that completely uncovered classes were not counting against the total . I tried other versions, with either the same results, or failures to run altogether. Ultimately, I gave up. A Successor It turns out that PartCover has a successor of sorts - Shaun Wilde, the last surviving maintainer of PartCover, has started his own coverage tool - OpenCover . It already seems be a viable PartCover replacement, and is in active development, so I'll be checking it out as a free, non-IDE-integrated coverage tool. Conclusion Pros: free! XML/HTML via ReportGenerator report merging via ReportGenerator Isolator support auto-deployable (reported) sequence point coverage Cons: no IDE integration no special IIS support forked implementations, each with their own warts not quite abandoned, but not a lot of interest behind the project Until I noticed the high coverage levels, I didn't mind PartCover. I figured its lack of price and its Isolator support made it a viable candidate. Unfortunately, the high coverage reports and other problems soured me on the deal, as did the lack of maintenance. I'm going to look at OpenCover instead.","tags":"Development","url":"https://blairconrad.com/2011/08/05/hasty-impressions-partcover/","loc":"https://blairconrad.com/2011/08/05/hasty-impressions-partcover/"},{"title":"Hasty Impressions: dotCover 1.1","text":"I tried JetBrains dotCover 1.1, integrated with ReSharper 5.1 running in VS2008. The Cost A lifetime license, with 1 year of free upgrades is $199 $149 - a special introductory price. This isn't usurious, but considering that ReSharper C# edition, a tool that changes the way I work every single day, is $249, it's enough. VS integration This is where I expected dotCover to shine, and it didn't disappoint - the integration with Visual Studio (and with ReSharper) was excellent . The first thing I noticed was an extra \"Cover with dotCover\" item in the ReSharper test menu (triggered from the yellow and green ball things). I clicked it, and it ran my tests, bringing up the familiar Unit Test results window. Once the tests ran, there was pause while dotCover calculated the coverage info, and then the bottom pane filled in with coverage results: green/red bars by every method in the covered assemblies. Clicking on the methods warps to the source code, which is also highlighted - covered statements have a green background, and uncovered statements have red. In fact, every source file opened in the IDE has the highlighting. Finding tests that cover code The most interesting feature that dotCover has is the ability to identify which tests covered which lines of code. I'm not entirely sold on this, thinking it more of a gimmick than anything else. When I first heard about it, I thought \"I don't care which test covered which line, so long as the lines are covered. I'm here to see what isn't covered.\". Yes, I think in italics sometimes. Still, I gave it a go. Right-clicking on a line of code (once coverage has been run) brought up a tiny menu full of covered lines of code. I don't know why, but it made me happy. I suppose one could use this from time to time to make sure a new test case is exercising what it's supposed to, but normally I can tell that by how a new test fails, or by what I've typed just before the test starts working. Worst case, I could always debug through a single test - something made very easy by the ReSharper test runner. There was one aspect of this feature that I could imagine someone using - the ability to run the tests that cover a line of code. All that's needed is to hit the \"play\" button on the \"Show Covering Tests\" popup. If the full suite of tests takes a very long time to run, this could be useful. Still, it doesn't do much for me personally - if my tests took that long to run, I'd try speed them up. If nothing else, I would probably just run the test fixture designed to test the class or method in question, instead of my entire bolus of tests. So, running tests that cover some code is a cool feature, but it's not that useful . I'd rather see something like the automatic test runs and really cool \"what's covered\" information provided by Mighty-Moose . Command Line Execution Covering an application from the command line is pretty straightforward . I used this command to see what code my BookFinder unit tests exercised: dotcover cover /TargetExecutable=nunit-console.exe /TargetArguments=.\\BookFinder.Tests.dll /Output=dotCoverOutput /Filters=+:BookFinder.Core BookFinder.Core is the only assembly I was interested in - it holds the business logic. \"cover\" takes multiple include and exclude filters, even using wildcards for assemblies, classes, and methods. One quite cool feature is to use the help subcommand to generate an XML configuration file , which can be used to specify the parameters for the cover command: dotCover help cover coverSettings.xml will create a coverSettings.xml file that can be edited to specify the executable, arguments, and filters. Then use it like so: dotCover cover coverSettings.xml without having to specify the same batch of parameters all the time. Joining Coverage Runs Multiple coverage snapshots - perhaps from running tests on different assemblies, or just from performing different test runs on the same application - can be merged together into a comprehensive snapshot: dotCover merge /Source snapshot1;snapshot2 /Output mergedsnapshot Just include all the snapshots, separated by semicolons. XML Report After generating snapshots and optionally merging them, they can be turned into an XML report using the report command : dotcover report /Source=.\\dotCoverOutput /Output=coverageReport.xml There are options to generate HTML and JSON as well. Note that if there's only one snapshot, the \"merge\" step is not needed. In fact, there's even a separate analyse command that will cover and generate a report in one go. No Auto-Deploy There's no auto-deploy for dotCover - it needs to be installed . And since it's a plugin, Visual Studio is a requirement . This is a small inconvenience for developers and our build servers. Having to put VS on all our test machines is a bit of a bigger deal - definitely a strike against dotCover. TypeMock Isolator support in the future The dotCover 1.1 doesn't integrate with Isolator 6. Apparently dotCover's hooks are a little different than many other profiles (nCover, PartCover, …). I've been talking to representatives from both TypeMock and JetBrains, though, and they tell me that the problem is solved, and an upcoming release of Isolator will integrate with dotCover. Even better, a pre-release version that supports the latest dotCover EAP is available now . IIS dotCover covers IIS, but only by using the plugin - this means that the web server has to have Visual Studio and dotCover installed, and it's a manual step to invoke the coverage. In the JetBrains developer community there's a discussion about command-line IIS support , but no word from JetBrains staff on when this might come. Statement-level coverage As Kevin Jones notes , dotCover reports coverage of statements coverage, not sequence points. This means that a line like this: return value > 10 ? Colors.Red : Colors.White; Will report as completely covered, even if it's executed only once - in order to ensure an accurate coverage report for this idea, the ?: would have to be replaced by an if-else block. This isn't necessarily a major strike against the tool, but it's worth knowing, as it will skew the results somewhat. Conclusion Pros: awesome IDE integration XML/HTML/JSON reports report merging IIS profiling Cons: moderate price no auto-deploy no Isolator support—yet Overall, I like the tool. I'm a little disappointed by the lack of auto-deploy and the inability to run IIS coverage from the command line, but those problems can be worked around. I was very impressed with the in-IDE support as well as the automatically generated configuration files using the \"help\" subcommand. Ordinarily, the I'd say the current lack of Isolator support is a deal-breaker, but I recently demoed the product to some colleagues, and they went bonkers for the IDE integration . I guess I'll be writing JetBrains and TypeMock looking for the betas.","tags":"Development","url":"https://blairconrad.com/2011/07/29/hasty-impressions-dotcover-1.1/","loc":"https://blairconrad.com/2011/07/29/hasty-impressions-dotcover-1.1/"},{"title":"Can you cover me? Looking for a .NET coverage tool","text":"Recently at the Day Job, my boss's boss has been on a \"code confidence\" kick. We've always done various levels of automated and manual unit, integration, issue, system, and regression testing, but he's looking to improve the process. Part of this push involves getting better at measuring which tests exercise what parts of the code. We want to know this for the usual reasons: we can identify gaps in our testing, or more likely find opportunities to cover some areas earlier in the testing cycle. It'd be nice to know that a particularly critical section of code has been adequately exercised by the per-build unit tests, without having to wait for nightly integration testing or wait even longer for a human to get their grubby mitts on it. To that end, I'm looking for a .NET coverage tool to dazzle us with tales of test runs. Over the next little while, I'll look at a few candidates, summarize my findings, and hopefully come up with a winner. Considerations Here are some factors that will influence me. Some of these may be negotiable, if a candidate really shines in other areas. We'd like to see coverage information in our build reports, so the tool should run from the command line . It'd be easier to put the coverage info our our build reports if the coverage reports were in XML . I really prefer a product that has an auto-deploy , so it can be bundled with the source tree and new developers or build servers just work. You may remember the pains I went to to auto-deploy TypeMock Isolator . While I'm on the subject, one of our products uses Isolator as its mocking framework, so the coverage tool should be able to link with TypeMock Isolator . We have a web services layer, which will be exercised by unit tests, but if we could gather stats on the layer as it's being exercised by the client-facing portion, that would be gravy. To that end, it should be possible to cover IIS . When I used TestDriven.NET + NCover a few years ago, I enjoyed being able to quickly see what my tests covered. This isn't a requirement of our current initiative, but IDE integration would be a bonus. Price is a factor. Money's available, but why spend if you don't have to? Or at least, why not pay less for an equivalent product. The Candidates Googling has lead me to these candidates, which I'll be examining in the next little while: dotCover ( my impression ) NCover ( my impression ) PartCover ( my impression ) OpenCover ( my impression ) Update : I picked one .","tags":"Development","url":"https://blairconrad.com/2011/07/18/can-you-cover-me-looking-for-a-.net-coverage-tool/","loc":"https://blairconrad.com/2011/07/18/can-you-cover-me-looking-for-a-.net-coverage-tool/"},{"title":"Prime Time Programming, Part 2","text":"Last time I presented a truly horrible prime number generator I was using for Project Euler problems. Then I presented a revamped generator that used trial division. By adding various refinements to the generator, we saw the time required to generate primes less than 10 7 shrink from hours to 123 seconds. Today I'll describe a different approach that's even more effective. Attempt 2: Sieve of Eratosthenes The Sieve of Eratosthenes is another method for finding prime numbers. The algorithm is basically this: make a big array of numbers, from 2 to the highest prime you're hoping to find look for the next number that's not crossed off this number is your next prime cross off every multiple of the number you just found so long as the prime you just found is less than the square root of your limit, go to step 2 the uncrossed numbers are prime Suppose we want primes less than or equal to 20. We start with this list: 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 The first uncrossed off number is 2. That's our first prime. Cross off all the multiples of 2 (believe it or not, the 4 is crossed off): 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 The next uncrossed off number is 3. Cross off the multiples: 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Next, we have 5. Cross off its multiples (actually, they're already crossed off because they're all also multiples of either 2 or 3): 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 5 is greater than √20, so stop looping. All the uncrossed off numbers are prime: 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 A \"borrowed\" implementation The wikipedia article even provides a Python implementation, which I reproduce here in slightly altered form: def eratosthenes_sieve(m): # Create a candidate list within which non-primes will be # marked as None; only candidates below sqrt(m) need be checked. candidates = range(m+1) fin = int(m**0.5) # Loop over the candidates, marking out each multiple. for i in xrange(2, fin+1): if not candidates[i]: continue candidates[2*i::i] = [None] * (m//i - 1) # Filter out non-primes and return the list. return [i for i in candidates[2:] if i] I ran this to find my standard list of primes less than 10 7 , and was surprised at the results. The time to completion varied wildly on successive runs. Sometimes over 50 seconds, and sometimes as low as 13 . I noticed that when the run times were high, the laptop's hard drive was thrashing, and just afterward my other applications were unresponsive. I reran the test and, with a little help from PerfMon, found out that the memory usage was off the chart. No, seriously. Right off the top. I had to rescale the graph to get everything to fit. the Private Bytes went way up over 200MB. On my 512 MB laptop with Firefox, emacs, and a few background processes, things slow to a crawl. With a smaller set of primes, or on more powerful iron, this implementation may work, but it's not going to meet my needs. Attempt 3: The Sieve, but slowly cross out the composites If allocating a big array of numbers just to cross most of them out right away doesn't work, how about we start by allocating nothing and just cross out numbers at the last moment? The idea is pretty simple: start counting at 2, and keep a record of upcoming composite numbers that we've discovered by looking at multiples of primes so far. Essentially we maintain a rolling wave of the next multiples of 2, 3, 5, …: let composites = {} , an empty associative array, where each key is a composite number and its value is the prime that it's a multiple of let n = 2 if n is a known composite, remove it and insert the next multiple in the list otherwise, it's prime. Yield it and insert a new composite, n 2 increment n go to step 3 Let's see how it works n composites 2 {} 2 isn't in composites, so yield it. Then insert 2 2 = 4 and increment n. 3 {4:2} 3 isn't in composites, so yield it. Then insert 3 2 = 9 and increment n. 4 {4:2, 9:3} 4 is in composites, with value 2. Remove it, insert 4 + 2 = 6 and increment n. 5 {6:2, 9:3} 5 isn't in composites, so yield it. Then insert 5 2 = 25 and increment n. 6 {6:2, 9:3, 25:5} 6 is in composites, with value 2. Remove it, insert 6 + 2 = 8 and increment n. 7 {8:2, 9:3, 25:5} 7 isn't in composites, so yield it. Then insert 7 2 = 49 and increment n. 8 {8:2, 9:3, 25:5, 49:7} 8 is in composites, with value 2. Remove it, insert 8 + 2 = 10 and increment n. 9 {9:3, 10:2, 25:5, 49:7} 9 is in composites, with value 3. Remove it, insert 9 + 3 = 12 and increment n. 10 {10:2, 12:3, 25:5, 49:7} 10 is in composites, with value 2. Remove it, and... wait . 12 is already in the list, because it's a multiple of 3. We can't insert it. We'll have to amend the algorithm to account for collisions. If a multiple is already accounted for, keep moving until we find one that isn't in the list. In this case, add another 2 to 12 to get 14 and insert it. Then increment n. n composites 11 {12:3, 14:2, 25:5, 49:7} 11 isn't in composites, so yield it, insert 11 2 = 121, increment n, and continue… Show me the code Here's an implementation of the naive algorithm presented above def sieve(): composites = {} n = 2 while True: factor = composites.pop(n, None) if factor: q = n + factor while composites.has_key(q): q += factor composites[q] = factor else: # not there - prime composites[n*n] = n yield n n += 1 This implementation takes 26.8 seconds to generate all primes below 10 7 , close to ¼ the time the best trial division algorithm took. Why is this method so great? Using the associative array values to remember which \"stream\" of multiples each composite comes from means that the array is no bigger than the number of primes we've seen so far The primes can be yielded as soon they're found instead of waiting until the end Adding p 2 when we find a new prime cuts down on collisions but still ensures that we'll keep find all multiples of p, because 2p, 3p, …, (p-1)p will be weeded out as multiples of lower primes. There's very little wasted work - finding a new prime number takes O(1) operations - just checking that the number isn't in the associative array and adding the square to the array. Many composites will take more work, but an amount proportional to the number of distinct prime factors the number has. For example, 12 has prime factors 2, 2, and 3. We tried to add 12 to the array twice, once as a multiple of 2 and once as a multiple of 3. Fortunately, the number of distinct factors is severely limited. For numbers less than 10 7 , 9699690 has the most distinct prime factors: 2, 3, 5, 7, 11, 13, 17, 19. This sure beats the 446 divisions trial division took to find that 9999991 was prime. The method already incorporates some of the advantages of the souped-up trial division methods from last time. We only worry about multiples of primes, so there's no need to cut out the composite factors. And when checking to see if n is prime, we never consider prime factors larger than √n. Still, there are some optimizations to make. That's Odd In the sample runthrough above, the algorithm checks 4, 6, 8, 10, … for primality, even though no even number larger than 2 are prime. Here's an implementation that avoids that: def odd_sieve(): composites = {} yield 2 n = 3 while True: factor = composites.pop(n, None) if factor: q = n + 2 * factor while composites.has_key(q): q += 2 * factor composites[q] = factor else: # not there - prime composites[n*n] = n yield n n += 2 This method generates primes less than 10 7 in 13.4 seconds. This is about half the time it took when we didn't pre-filter the evens. In the trial division case, when we cut out the even numbers, we were saving almost no work - one division per even number, out of potentially dozens or hundreds of divisions being performed. This time, we cut out an associative array lookup and insertion, and most numbers are checked by using only a small number of these operations. Let's see what else we can do. What about 3? If skipping the numbers that are divisible by 2 paid off, will skipping those divisible by 3 as well? Probably. def sixish_sieve(): composites = {} yield 2 yield 3 step = 2 n = 5 while True: factor = composites.pop(n, None) if factor: q = n + 2 * factor while q % 6 == 3 or composites.has_key(q): q += 2 * factor composites[q] = factor else: # not there - prime composites[n*n] = n yield n n += step step = 6 - step Now the time to generate primes less than 10 7 is 11.9 seconds. Again, I think we've hit diminishing returns. We didn't get the 1/3 reduction I'd hoped, probably due to the more complicated \"next multiple\" calculation. YAGNI Things are going pretty well. There's only one thing that bothers me about the latest generator - we're storing too many composites in the associative array. Every time we find a prime number, its square is inserted in the array. Even 9999991 2 is put in the array, even though we'll never check to see if any number greater than 10 7 is prime. So, modifying the algorithm to omit storing composites that are too large, we get: def sixish_sieve_max(): composites = {} yield 2 yield 3 step = 2 n = 5 while True: factor = composites.pop(n, None) if factor: q = n + 2 * factor while q % 6 == 3 or composites.has_key(q): q += 2 * factor composites[q] = factor else: # not there - prime if n n <= highest: composites[n n] = n yield n n += step step = 6 - step This generator takes 10.8 seconds to generate primes below 10 7 - modest improvement, and one I'd keep anyhow, since the code is barely more complicated than the previous version. However, the real boost, if there is any, is in the memory usage. When sixish_sieve generates primes below 10 7 , the private bytes climb up to 52MB, but sixish_sieve_max stays below 25MB. The advantage continues as the problem set grows - when the upper limit is 2*10 7 , sixish_sieve takes 100MB, but sixish_sieve_max remains at a cool 25MB - I guess that's the difference between storing 1270607 composites and 607. Conclusion This was a fun and interesting exercise. Being able to look bad at your old code and say \"Boy, that was horrible. I'm glad I'm smarter now,\" makes me happy. And embarrassed. I enjoyed seeing how applying incremental changes and even rudimentary profiling yielded provably better results, right up until they showed that I probably needed to abandon my current path (trial division) and jump on a new one. I'm sticking with sixish_sieve_max for now. It's fast enough to meet my current needs, and will likely remain so until \"CPU inflation\" forces the Project Euler team to increase the size of their problem sets. Of course, maybe by then I'll have a faster processor and I won't care.","tags":"Development","url":"https://blairconrad.com/2011/05/09/prime-time-programming-part-2/","loc":"https://blairconrad.com/2011/05/09/prime-time-programming-part-2/"},{"title":"Prime Time Programming, Part 1","text":"From time to time, I find myself caught up in the heady world of Project Euler . It's almost like playing Professor Layton or some other puzzle-solving game - mathematical or programming brain teasers, served in bite-sized pieces. If you look at the Project Euler problems for any length of time, you'll notice common themes. One theme is prime numbers - many problems can't be solved without generating varying quantities of primes. To that end, I'd written a prime generator to be shared between problem solutions. Over time, I added functionality and \"optimized\" the generator to improve the running time of my solutions. Everything was great, until I tried Problem 315 , whose solution required a list of primes between 10 7 and 2×10 7 . My solution worked, but it ran really slowly - something like 12 minutes. Now, I'm not doing myself any favours by writing in Python and running on a 7-year-old laptop, but that's still too long. My Original Generator This is the slow-performing generator that I replaced when working on Problem 315. The squeamish reader may want to avert his eyes. class PrimeGenerator: __primes_so_far = [5] __increment = 2 def __init__ ( self ): self . __next_index = - 1 def is_prime ( self , n ): if n == 2 or n == 3 : return True elif n % 2 == 0 or n % 3 == 0 : return False elif n <= PrimeGenerator . __primes_so_far [ - 1 ]: # bisecting didn ' t really help return n in PrimeGenerator . __primes_so_far else : return self . __is_prime ( n ) def __is_prime ( self , n ): limit = math . sqrt ( n ) g = PrimeGenerator () g . next () # skip 2 g . next () # skip 3 p = g . next () while p <= limit : if n % p == 0 : return False p = g . next () return True def next ( self ): self . next = self . __next3 return 2 def __next3 ( self ): self . next = self . __next5 return 3 def __next5 ( self ): self . __next_index += 1 if self . __next_index < len ( PrimeGenerator . __primes_so_far ): return PrimeGenerator . __primes_so_far [ self . __next_index ] candidate = PrimeGenerator . __primes_so_far [ - 1 ] while True : candidate += PrimeGenerator . __increment PrimeGenerator . __increment = 6 - PrimeGenerator . __increment if self . __is_prime ( candidate ): PrimeGenerator . __primes_so_far . append ( candidate ) return candidate def __iter__ ( self ): return self </ code ></ pre > My eyes! When I first went back to this code, I exclaimed, \"What was I thinking?\". I can think of two things: The is_prime member was intended to help out for problems where I didn't have to create too many primes, but just had to check a few number for primality. This doesn't really belong here, and clutters the class. I'd be better off focusing on prime generation. I was optimizing at least partly for the case where I'd want to get lists of primes a couple times—hence the indexing into an already-generated list of primes. If the generator were fast enough, this wouldn't be necessary. Things I can't understand, even today. I'll have to blame them on then-ignorance, foolishness, and hasty modifications to the class: Why the mucking about with __next3 and __next5 ? What did I have against yield ? Why is is_prime fussing with a whole new PrimeGenerator and skipping 2 and 3? Why not just go straight for the saved list of primes starting with 5? In fact, just about the only defensible things (as we'll see below) in the whole class are: ending the modulus checks once the candidate divisor is greater than the square root of the candidate number, and the bit where the next __increment is formed by subtracting the current one from 6 - I was exploiting the fact that, past 3, for any prime p , p ≡ 1 (mod 6) or p ≡ 5 (mod 6). How slow was it? The generator took 551.763 seconds to generate primes less than 10 7 , as measured by the following: def run(f): global highest start = datetime.datetime.now() count = 1 for p in f: count += 1 if p > highest: break end = datetime.datetime.now() elapsed = end-start return highest, count, elapsed.seconds + (elapsed.microseconds/1000000.0) Where f is an instance of PrimeGenerator passed into the run method, and highest is a global that's been set to 10 7 . Moving forward Based on the extreme slowness and general horribleness of the code, I figured I'd be better off starting over. So, I chucked the generator and started afresh with the simplest generator I could write. I resolved to make incremental changes, ensuring that at each step, the code was: correct (otherwise, why bother) faster than the previous version simple enough for me to understand Let's see what happened. Attempt 1: Trial Division Trial division is one of the simplest methods for generating primes—you start counting at 2, and if no smaller positive integers (other than 1) divide the current number, it's prime. The naive implementation is very simple. def naive_trial(): n = 2 while True: may_be_prime = True for k in xrange(2, n): if n % k == 0: may_be_prime = False break if may_be_prime: yield n n += 1 This method takes 113.804 seconds to generate primes below 100000 —I couldn't wait for the full 10 7 - it would probably be over 3 hours. Trial until root Fortunately, there are some pretty obvious optimizations one can make to the algorithm. The first comes from the observation that if there is a number k, 1 < k < n, that divides n, then there is a number j that divides n with 1 < j ≤ √n, so we can stop our trial once we've hit that point. def trial_until_root(): n = 2 while True: may_be_prime = True for k in xrange(2, int(n**0.5)+1): if n % k == 0: may_be_prime = False break if may_be_prime: yield n n += 1 This method takes 468 seconds to generate primes up to 10 7 . A definite improvement (and already faster my original generator), but there's still room for more. Trial by primes Here's another observation about divisors of n: if there's a number k that divides n, then there's a prime number p ≤ k that divides n, since either k is prime or has prime factors. So if we keep a list of the primes found so far, we only need to check prime divisors that are less than √n. def trial_by_primes(): primes_so_far = [] n = 2 while True: may_be_prime = True for p in primes_so_far: if n % p == 0: may_be_prime = False break if p * p > n: # it's prime break if may_be_prime: primes_so_far.append(n) yield n n += 1 Now we're down to 136 seconds to generate primes below 10 7 . That was a worthwhile change, but we have to balance it against the fact that the generator now requires additional storage - the list of primes encountered so far. In this case, we're storing over 660,000 prime numbers in a list. Even an older laptop can handle this burden, but it's something to keep in mind. That's odd And by \"that\", I mean \"all the prime numbers except for 2\". There's no point checking the even numbers to see if they're prime. Let's see what happens when we skip them. The only tricky part (and it's not that tricky) is to make sure we still return 2 as our first prime. def odd_trial_by_primes(): primes_so_far = [] yield 2 n = 3 while True: may_be_prime = True for p in primes_so_far: if n % p == 0: may_be_prime = False break if p * p > n: # it's prime break if may_be_prime: primes_so_far.append(n) yield n n += 2 This method takes 127 seconds to generate primes less than 10 7 . Not a huge improvement, but better than nothing, and it doesn't really complicate the code that much. I'll keep it. The reason we don't get a huge improvement here is that checking the even numbers for primeness doesn't take that much effort - they were eliminated as soon as we modded them by the first prime in primes_so_far . Still, it's a little quicker to jump right over them than to perform the division. What about 3? If skipping the numbers that are divisible by 2 paid off, will skipping those divisible by 3? As I noted above, every prime p greater than 3 satisfies p ≡ 1 (mod 6) or p ≡ 5 (mod 6) . Let's use that. We'll take advantage of these facts: if p ≡ 1 (mod 6) , then p+4 ≡ 5 (mod 6) if p ≡ 5 (mod 6) , then p+2 ≡ 1 (mod 6) So we want to alternate our step between 2 and 4. Fortunately 6 - 4 = 2 and 6 - 2 = 4, so we can use 6 - step as our next step. def sixish_trial_by_primes(): primes_so_far = [] yield 2 yield 3 step = 2 n = 5 while True: may_be_prime = True for p in primes_so_far: if n % p == 0: may_be_prime = False break if p * p > n: # it's prime break if may_be_prime: primes_so_far.append(n) yield n n += step step = 6 - step Now the time drops to 123 seconds to generate primes less than 10 7 . Clearly we've hit diminishing returns - we're saving two modulus operations on numbers that are divisible by 3 (but not 2), at the cost of a more complicated \"step\" calculation. We could continue on in this vein, but the gains are not likely to be large, and the complexity increases rapidly. Consider the next step: we'd make sure we don't test numbers divisible by 2, 3, or 5. That means (after 5) we only consider numbers whose remainders when divided by 30 are one of 1, 7, 11, 13, 17, 19, 23, or 29. The steps between numbers are 6, 4, 2, 4, 2, 4, 6, and 2. Who has the energy? The problem with Trial Division Trial division has a few things going for it: it's simple to understand there are some obvious optimizations that can make its performance tolerable Ultimately, though, its downfall is that it takes a lot of work to verify that a large number is prime. Consider the largest prime number below 10 7 : 9999991. In order to verify that this is prime, we have to consider all prime factors less than √9999991. There are 446 of these. That's 446 divisions, just to verify that one number is prime. We're unlikely to radically improve performance by continuing to tinker with trial division. It's time to throw the whole thing away again and try a new approach. Next time we'll do just that.","tags":"Development","url":"https://blairconrad.com/2011/04/25/prime-time-programming-part-1/","loc":"https://blairconrad.com/2011/04/25/prime-time-programming-part-1/"},{"title":"How to completely disable Autofac components","text":"This week I started working with the Autofac Inversion of Control container at the Day Job. The first project I tried to introduce Autofac to needed a plugin system. I figured this was a perfect use of Autofac's implicit relationship handlers . Sure enough, a container.Resolve<IEnumerable<IPlugin>>() did the trick - I got a nice list of plugin instances for the application to use. This isn't enough, though. We need to disable certain components via configuration. One option would be to remove the components from the configuration file, but I wanted to make it easy to restore the plugins (and their original configuration) should the need arise. After poring over the Autofac documentation, it seemed like adding an \"Enabled\" flag in the components' metadata would be the best way to handle toggling them between on and off. Setting up the config file was straightforward, <autofac defaultAssembly=\"DisableComponents\"> <components> <component type=\"DisableComponents.Plugin1\" service=\"DisableComponents.IPlugin\"> <metadata> <item name=\"Enabled\" value=\"false\" type=\"System.Boolean\" /> </metadata> </component> <component type=\"DisableComponents.Plugin2\" service=\"DisableComponents.IPlugin\"> <metadata> <item name=\"Enabled\" value=\"true\" type=\"System.Boolean\" /> </metadata> </component> </components> </autofac> as was filtering the components list. var enabledComponents = container.Resolve<IEnumerable<Meta<IPlugin>>>() .Where(ComponentIsEnabled) .Select(c=>c.Value); ... private static bool ComponentIsEnabled<T>(Meta<T> component) { const string enabled = \"Enabled\"; return !component.Metadata.ContainsKey(enabled) || (bool)component.Metadata[enabled]; } They're still created, though This approach worked, but all all components are instantiated, including the disabled ones which are made just so we can throw them away. This seems a little wasteful. Worse, a particular installation may have a plugin disabled because it can't (or doesn't want to) support its creation. So I sought a way to prevent the instantiation of the unwanted plugins. I tried to find a way to remove or disallow registration based on the metadata, or to intercept component creation, but came up short. The best I could come up with was a modification to the approach above: var enabledComponents = container.Resolve<IEnumerable<Meta<Func<IPlugin>>>>() .Where(ComponentIsEnabled) .Select(c=>c.Value()); (I would have preferred to use a Lazy over a Func , but I'm working with .Net 35.) This works—the plugins are only created when they're enabled—but it feels inelegant. I can't help but think that my Autofac knowledge is too shallow to have discovered the \"right\" way to do this. Hopefully deeper understanding will come in time…","tags":"Development","url":"https://blairconrad.com/2011/02/27/how-to-completely-disable-autofac-components/","loc":"https://blairconrad.com/2011/02/27/how-to-completely-disable-autofac-components/"},{"title":"Growing an MVVM Framework in 2003, part V—Reflections and Regrets","text":"This post is from a series on my experiences starting to grow an MVVM Framework in .NET 1.1. * Part I—Event Handlers * Part II — Properties * Part III — Properties Redux * Part IV—Unit Tests * Part V—Reflections and Regrets Full source code can be found in my Google Code repository . I haven't added any articles to this series in a while. The main reason is that I've not done any more work on the framework. I was able to complete my application using the tools using the Framework So Far, and I've long since moved on to other projects. I wanted, though, to take a quick look back and evaluate the project. I did it! Way back in part 1, I said that I wanted to create an application that had testable logic, even in the GUI layer, had no \"codebehind\" in the view, and shunted the tedious wiring up of events and handlers into helpers (or a \"framework\") I'm very pleased with how all this turned out. Taking things in reverse order: The little framework does an excellent job of handling the tedious event-wiring. Handling an View event requires nothing more than declaring a method with a convention-following name and the correct signature, such as public void FindClick(object sender, EventArgs e) Properties are wired up in a similar way, by declaring a public field with a convention-following name and an appropriate type (StringProperty, BoolProperty, or ListProperty). Aside from setting some properties on View elements (for example, the Find button is initial disabled), there was no need to crack open the View's .cs file—I never saw the inside of it. The easily-invoked event handlers and the property bindings made writing unit test as easy as writing tests for a non-GUI component: set some initial properties, poke the ViewModel by invoking an event handler, and check the properties. Done! Injecting mock model components was the hardest thing, and that's no different than in any other test. If only I had There was one nagging problem that I left unresolved. My Model contains only synchronous operations, so the View doesn't update while we're accessing the data store. As it turns out, the operations are very quick, so the user is unlikely to notice. I could have implemented asynchronous operations on the view, or used delgates or background threads to explicitly invoke the model in the background. I really would've liked to implement something that would be applicable to a larger problem set. Something like Caliburn.Micro's IResult and Coroutines : Returning an IResult or a collection of them to be executed on background threads by the framework, while the GUI updates and the ViewModel is none the wiser. Ah well, time was running out, and there didn't seem to be that much benefit. Maybe next time... I would have liked to There are a few other \"features\" that I would've liked to add to the framework, but there wasn't time, nor did there seem to be an immediate need: a View binder — After writing a class to bind the ViewModel to the fake storage properties, I realized that that was a nicer approach than having the binding code in the ViewModelBase. I'd like create a \"production binder\" to hook up the ViewModel and View. composable ViewModels — BookFinder is very simple, with only a TextBox and a few ListBoxes and Buttons on its View, so a single View and ViewModel was sufficient. It would be useful to be able to build up a more complicated GUI by walking a tree of ViewModels and composing a GUI out of corresponding View components. deregistering event handlers — The framework registers event handlers between the ViewModel and View, with no provision for unregistering them when components are no longer needed. In BookFinder, the single View/ViewModel pair hang around until the application is closed, but in a more complicated application there might be an opportunity to leak resources. Summing up I'm happy with how the framework and tool turned out. I could probably have written the application more quickly if I hadn't bothered trying to extract the framework, but it wouldn't have been as testable (and therefore likely not as well tested). I think the extra effort was worthwhile both because it created a better application and because I learned more about WinForms programming and how I can leverage conventions to reduce programmer workload—if the framework were used for a second application, development would just fly. And the exercise was fun. Not only writing the framework, but using it — it's extremely liberating having event handlers just work by creating a properly-named method, and having the handler be immediately testable is a joy. If I had any expectations that I'd be writing similar tools on .NET 1.1 again, I'd definitely continue extending the framework.","tags":"Development","url":"https://blairconrad.com/2011/02/15/growing-an-mvvm-framework-in-2003-part-v-reflections-and-regrets/","loc":"https://blairconrad.com/2011/02/15/growing-an-mvvm-framework-in-2003-part-v-reflections-and-regrets/"},{"title":"BBC Top 100 Books","text":"A meme, from No Magic Here . Today's challenge, the BBC Top 100 . Instructions: Bold those books you've read in their entirety, italicize the ones you started but didn't finish or read an excerpt. Pride and Prejudice – Jane Austen The Lord of the Rings – JRR Tolkien Jane Eyre – Charlotte Bronte Harry Potter series – JK Rowling (all) To Kill a Mockingbird – Harper Lee The Bible Wuthering Heights – Emily Bronte Nineteen Eighty Four – George Orwell His Dark Materials – Philip Pullman Great Expectations – Charles Dickens Little Women – Louisa M Alcott Tess of the D'Urbervilles – Thomas Hardy Catch 22 – Joseph Heller Complete Works of Shakespeare Rebecca – Daphne Du Maurier The Hobbit – JRR Tolkien Birdsong – Sebastian Faulks Catcher in the Rye – JD Salinger The Time Traveller's Wife – Audrey Niffenegger Middlemarch – George Eliot Gone With The Wind – Margaret Mitchell The Great Gatsby – F Scott Fitzgerald Bleak House – Charles Dickens War and Peace – Leo Tolstoy The Hitch Hiker's Guide to the Galaxy – Douglas Adams Brideshead Revisited – Evelyn Waugh Crime and Punishment – Fyodor Dostoyevsky Grapes of Wrath – John Steinbeck Alice in Wonderland – Lewis Carroll The Wind in the Willows – Kenneth Grahame Anna Karenina – Leo Tolstoy David Copperfield – Charles Dickens Chronicles of Narnia – CS Lewis Emma – Jane Austen Persuasion – Jane Austen The Lion, The Witch and The Wardrobe – CS Lewis The Kite Runner – Khaled Hosseini Captain Corelli's Mandolin – Louis De Berniere Memoirs of a Geisha – Arthur Golden Winnie the Pooh – AA Milne Animal Farm – George Orwell The Da Vinci Code – Dan Brown One Hundred Years of Solitude – Gabriel Garcia Marquez A Prayer for Owen Meaney – John Irving The Woman in White – Wilkie Collins Anne of Green Gables – LM Montgomery Far From The Madding Crowd – Thomas Hardy The Handmaid's Tale – Margaret Atwood Lord of the Flies – William Golding Atonement – Ian McEwan Life of Pi – Yann Martel Dune – Frank Herbert Cold Comfort Farm – Stella Gibbons Sense and Sensibility – Jane Austen A Suitable Boy – Vikram Seth The Shadow of the Wind – Carlos Ruiz Zafon A Tale Of Two Cities – Charles Dickens Brave New World – Aldous Huxley The Curious Incident of the Dog in the Night-time – Mark Haddon Love In The Time Of Cholera – Gabriel Garcia Marquez Of Mice and Men – John Steinbeck Lolita – Vladimir Nabokov The Secret History – Donna Tartt The Lovely Bones – Alice Sebold Count of Monte Cristo – Alexandre Dumas On The Road – Jack Kerouac Jude the Obscure – Thomas Hardy Bridget Jones's Diary – Helen Fielding Midnight's Children – Salman Rushdie Moby Dick – Herman Melville Oliver Twist – Charles Dickens Dracula – Bram Stoker The Secret Garden – Frances Hodgson Burnett Notes From A Small Island – Bill Bryson Ulysses – James Joyce The Bell Jar – Sylvia Plath Swallows and Amazons – Arthur Ransome Germinal – Emile Zola Vanity Fair – William Makepeace Thackeray Possession – AS Byatt A Christmas Carol – Charles Dickens Cloud Atlas – David Mitchell The Color Purple – Alice Walker The Remains of the Day – Kazuo Ishiguro Madame Bovary – Gustave Flaubert A Fine Balance – Rohinton Mistry Charlotte's Web – EB White The Five People You Meet In Heaven – Mitch Albom Adventures of Sherlock Holmes – Sir Arthur Conan Doyle The Faraway Tree Collection – Enid Blyton Heart of Darkness – Joseph Conrad The Little Prince – Antoine De Saint-Exupery The Wasp Factory – Iain Banks Watership Down – Richard Adams A Confederacy of Dunces – John Kennedy Toole A Town Like Alice – Nevil Shute The Three Musketeers – Alexandre Dumas Hamlet – William Shakespeare Charlie and the Chocolate Factory – Roald Dahl Les Miserables – Victor Hugo","tags":"Miscellany","url":"https://blairconrad.com/2010/12/31/bbc-top-100-books/","loc":"https://blairconrad.com/2010/12/31/bbc-top-100-books/"},{"title":"Office Adventure: Hard Drive Shuffle","text":"It's fashionable to complain about the IT department at the Day Job, but sometimes we get pretty good service. I had occasion to contact them today for two unrelated problems. The first, an issue with a Lotus Notes upgrade, was quickly resolved over the phone. The other problem involved hardware, and I'm well on the way to a resolution, with one minor snag. While rebooting to pick up part of the Notes fix, I noticed something strange on my BIOS screen. I have two hard drives in my workstation, RAID-1ed together. The BIOS displayed one drive as green, and one red. Red is bad. I mentioned this to the Notes Fixer. Half an hour later I was contacted by a different IT guy. He wanted to come by and take the bad drive so he could order another one. \"Sure!\" said I. IT Guy rebooted my machine again so he could see which drive was faulty. Then powered it down, popped open the case, extracted the hard drive and blew on it, releasing a cloud of toxic dust right above my tea. All this in less time than it takes to type it. He popped closed the case, and off he went. I started booting. I also powered up my computer. The computer beeped funny as it started up. I checked out the BIOS screen. One red drive. No green drive. \"He took the wrong one!\" I said, and hared off after him. There I was, running through the halls, wearing one boot and one sock. Thump -thump. Thump -thump. Thump -thump. \"IT Guy,\" I called, as he hove into view. You see, I wanted to make sure I got him before he put the drive under the Big Magnet. Also, I didn't know where he sits. I explained the problem. B : You took the wrong drive. I'd like that one back. ITG : Oh! Right. Thanks for coming after me. B : No problem. Of course, it wasn't for your benefit. We walked back to my cubicle, where he swapped drives and all was well. Then he left, with a cheerful, \"We should have the new drive by tomorrow morning. Hey, where's your shoe?\"","tags":"Miscellany","url":"https://blairconrad.com/2010/12/09/office-adventure-hard-drive-shuffle/","loc":"https://blairconrad.com/2010/12/09/office-adventure-hard-drive-shuffle/"},{"title":"Growing an MVVM Framework in 2003, part IV—Unit Tests","text":"This post is from a series on my experiences starting to grow an MVVM Framework in .NET 1.1. * Part I—Event Handlers * Part II—Properties * Part III —Properties Redux * Part IV—Unit Tests * Part V—Reflections and Regrets Full source code can be found in my Google Code repository . In parts 1 and 3 (and 2, but I like part 3 better) I showed a tiny \"framework\" for binding View properties and events to properties and methods on a ViewModel. In addition to avoiding the tedium and noise of wiring up events by hand, I'd hoped to implement a structure that would make unit testing easier. Let's see how that went. Event handlers just work. Almost Recall that event handlers are defined on the ViewModel as plain old methods that happen to take a specific set of arguments—usually object and something that derives from EventArgs . This means that nothing special has to be done in order to exercise the methods during a unit test. The test doesn't have to trick the ViewModel into registering with an event or anything. The test just calls the method. And if the method doesn't care much about its arguments like FindClick doesn't, you can pass in nonsense: public class BookListViewModel { public void FindClick(object sender, EventArgs e) { ICollection books = bookDepository.Find(TitleText.Value); IList bookListItems = BookListItems.Value; bookListItems.Clear(); foreach ( string book in books ) { bookListItems.Add(book); } } } public class BookListViewModelTests { [Test] public void CallFindClick() { vm.FindClick(null, null); } } Of course, this isn't much of a test. Usually we'll want to set up some initial state for the ViewModel, and verify that the correct actions have been taken. In fact, as things stand, the property fields will all be null, so TitleText.Value and BookListItems.Value will error out. Putting something behind the properties Most event handlers will need to access the properties on the ViewModel, so the tests must hook up the properties. Provide stub properties Last time I mentioned that the PropertyStorageStrategy would bring value. This is it. Recall the definitions of the ListProperty and the PropertyStorageStrategy: public class ListProperty: Property { public ListProperty(PropertyStorageStrategy storage): base(storage) {} public IList Value { get { return (IList) storage.Get(); } set { storage.Set(value); } } } public interface PropertyStorageStrategy { object Get(); void Set(object value); } The ListProperty (and BoolProperty and StringProperty) merely consult a PropertyStorageStrategy to obtain a value and they cast it to the correct type. Providing a dumb strategy that, instead of proxying a property on a View control, just holds a field will produce a property that can be used in tests: public class ValuePropertyStrategy: PropertyStorageStrategy { private object obj; public ValuePropertyStrategy(object initialValue) { this.obj = initialValue; } public void Set(object value) { obj = value; } public object Get() { return obj; } } Then the test fixture setup can bind properties to the ViewModel: [SetUp] public void SetUp() { vm = new BookListViewModel(new Control(), new FakeBookDepository()); vm.TitleText = new StringProperty(new ValuePropertyStrategy(\"\")); vm.BookListItems = new ListProperty(new ValuePropertyStrategy(new ArrayList())); ... } And tests can be constructed to provide initial property values (if the default isn't good enough) and interrogate them afterward. [Test] public void FindClick_WithTitleG_FindsEndersGame() { vm.TitleText.Value = \"G\"; vm.FindClick(null, null); Assert.IsTrue(vm.BookListItems.Value.Contains(\"Ender's Game\")); } Auto-wiring the properties This works, and pretty well. There's not that much noise associated with setting up the fake properties. Still, why should there be any? After so much trouble to remove the tedious wiring up from the production code, it seems wrong to leave it in the testing code. Also, I'm against anything that adds a barrier to writing tests. And having to hand-wire a few (or a dozen) properties before you can start testing is definitely a barrier. So, let's write a little code to handle the tedium for us. public class ValuePropertyBinder { public static void Bind(ViewModelBase viewModel) { foreach ( FieldInfo field in viewModel.PropertyFields() ) { ValuePropertyStrategy propertyStorageStrategy = new ValuePropertyStrategy(MakeStartingValue(field.FieldType)); ConstructorInfo propertyConstructor = field.FieldType.GetConstructor(new Type[] {typeof (PropertyStorageStrategy)}); object propertyField = propertyConstructor.Invoke(new object[] {propertyStorageStrategy}); field.SetValue(viewModel, propertyField); } } private static object MakeStartingValue(Type fieldType) { Type propertyType = fieldType.GetProperty(\"Value\").PropertyType; if ( propertyType == typeof(IList) ) { return new ArrayList(); } if ( propertyType == typeof(string) ) { return \"\"; } if ( propertyType == typeof(bool) ) { return false; } else { throw new NotImplementedException(\"no known starting value for type \" + propertyType); } } } This is very similar to the wiring we've seen before—find property fields, construct an object to implement the property, and hook it up. The only thing likely to need attention in the future is MakeStartingValue . A new property type(like DateTime), will require an expansion to the if chain. But that should be very infrequent. Now it's much easier to use the ViewModel in tests: [SetUp] public void SetUp() { vm = new BookListViewModel(new Control(), new FakeBookDepository()); ValuePropertyBinder.Bind(vm); } An alternative: brute force and ignorance This approach didn't occur to me until the project was over. Sigh. The production code works by binding the ViewModel to a View. The test setup could do that. I'd taken pains to keep any kind of code or behaviour out of the View, so there shouldn't be any side effects, and there's no need to show any of the GUI elements. Honestly, the technical downsides seem pretty limited. Even so, I don't like this solution. For the BookFinder application, the View is simple enough that I'm confident the approach would work, but I have concerns over using it in a more complex application. Also, I prefer to reduce the amount of auxiliary production code that's used in tests. In the off chance that something does go wrong, it's nice to be able to have a small set of production code to look at Summing up With the ValuePropertyBinder (or much-maligned \"just bind the ViewModel to the actual Model\"), tests are really easy to set up and run. As easy as writing the production code. And they're readable. The only troublesome dependencies are the models. Totally worth the effort.","tags":"Development","url":"https://blairconrad.com/2010/11/30/growing-an-mvvm-framework-in-2003-part-iv-unit-tests/","loc":"https://blairconrad.com/2010/11/30/growing-an-mvvm-framework-in-2003-part-iv-unit-tests/"},{"title":"Growing an MVVM Framework in 2003, part III—Properties Redux","text":"This post is from a series on my experiences starting to grow an MVVM Framework in .NET 1.1. * Part I—Event Handlers * Part II – Properties * Part III – Properties Redux * Part IV—Unit Tests * Part V—Reflections and Regrets Full source code can be found in my Google Code repository . A Change of Plans Last time I showed how I managed the binding of ViewModel properties to the properties on the View's controls. I promised to talk this time about how the use of the mini-framework affected the testability of the code. I changed my mind—I want to return to the whole properties discussion. Festering Dissatisfaction The method I had for binding ViewModel properties to the View worked, but it left a bad taste in my mouth. A few things bothered me about the implementation. Recall that to add a bound property the ViewModel had to have code something like this: private Property bookListItems; public string BookListItems { get { return bookListItems.AsList(); } set { bookListItems.Value = value; } } I have a couple of problems with this. it's pretty chatty the client programmer has to know when to use .AsList() or not, since strings and bools don't require it the viewbinding code had to look for the private field, and that just felt gross Poor man's generics When I first wrote the code, I was bothered a little by the weaknesses in the property bindings. It wasn't until I wrote about the code here that the suck really started to get to me. And worse, I was unhappy with what I'd wrote. One phrase from the post kept coming back to me: At this point, I was really missing generics. What did I mean by that? Why did I miss generics? I hadn't explained that well, even to myself. So I thought about it. What would I do with the generics if I had them? And I thought for a bit longer. Then I had it. I'd make a Property class to proxy the view's properties—that would tighten up the code and relieve programmers of the burden of knowing when to use .AsList . Well, I don't have generics, but I do have Manual Type Creation. That's somewhat less convenient, but it's not like I'm going to need dozens of different property types—3 will do for a start. So I decided to see what I could do with a little Property type hierarchy. public abstract class Property { protected PropertyStorageStrategy storage; protected Property(PropertyStorageStrategy storage) { this.storage = storage; } } public class ListProperty: Property { public ListProperty(PropertyStorageStrategy storage): base(storage) {} public IList Value { get { return (IList) storage.Get(); } set { storage.Set(value); } } } public class StringProperty: Property { // pretty much what you'd expect } public class BoolProperty: Property { // pretty much what you expected above, only more Bool-y } There's not a terrible amount here, just a family of properties. Each concrete class is responsible for providing a Value property that will return (or accept) a typed value. The real work is done by the storage member—it keeps track of the untyped value that the concrete class will take or dole out. As the name PropertyStorageStrategy suggests, a Property can vary the source and sink for its value via the Strategy design pattern . I was holding it for a friend Let's look at the storage strategy that defers to a property on another object. public interface PropertyStorageStrategy { object Get(); void Set(object value); } public class BoundPropertyStrategy: PropertyStorageStrategy { private object obj; private PropertyInfo propertyInfo; public BoundPropertyStrategy ( object obj , PropertyInfo property ) { this . obj = obj ; this . propertyInfo = property ; } public void Set ( object value ) { propertyInfo . SetValue ( obj , value , null ); } public object Get () { return propertyInfo . GetValue ( obj , null ); } } Unsurprisingly, this looks a lot like the BoundProperty class from last time. After all, the core functionality is pretty much the same. So, inject a BoundProperty into one of ListProperty, StringProperty, or BoolProperty, and we get a strongly-typed proxy for the underlying object. Tying it together Of course the new classes required a change to the ViewModel/Model binding code. Locating the ViewModel fields to bind is pretty much the same as it was, except only public fields that derive from Property are considered. The BindFieldToControl becomes the slightly-better named BindPropertyToControl : private bool BindPropertyToControl(Control control, FieldInfo field) { string controlPropertyName = ControlAttributeName(control, field.Name); if ( controlPropertyName == null ) { return false; } PropertyInfo controlProperty = control . GetType (). GetProperty ( controlPropertyName , myBindingFlags ); if ( controlProperty == null ) { return false ; } BoundPropertyStrategy strategy = new BoundPropertyStrategy ( control , controlProperty ); ConstructorInfo constructor = field . FieldType . GetConstructor ( new Type [] { typeof ( PropertyStorageStrategy ) } ); object propertyField = constructor . Invoke ( new object [] { strategy } ); field . SetValue ( this , propertyField ); return true ; } The first part of the method just makes sure that the control we've found has a name that matches the first part of the property. Then we look for a property on the control that completes the name. Once those hurdles are past, the magic happens: create a new BoundProperty to proxy the control's property value take the property field type and find the constructor that takes a PropertyStorageStrategy make a new property object, passing in our BoundProperty set the property object onto the ViewModel How's it work? Overall, I think okay. Here's a sample of the ViewModel code. public StringProperty TitleText; public BoolProperty FindEnabled; public ListProperty BookListItems; public void TitleTextChanged(object sender, EventArgs e) { string newText = TitleText.Value; FindEnabled.Value = (newText != null & newText.Length > 0); } public void FindClick(object sender, EventArgs e) { ICollection books = bookDepository.Find(TitleText.Value); IList bookListItems = BookListItems.Value; bookListItems.Clear(); foreach ( string book in books ) { bookListItems.Add(book); } } The client developer has to remember to use the funny property types, but this isn't that much harder than, say Func . At least the names make some sense. The .Value could get a little old, but I prefer having it on both the get and the set even over just on the set. I like having the strong-typing built in to the type, rather than forcing the client developer to do the conversion in a property. On the downside, additional property types will have to be added to the framework by hand, but that shouldn't come up too often. Also, the storage strategy for the properties is maybe a little complicated, but at least clients of the framework never have to deal with it directly. The observant among you will probably criticize the strategy because so far there's no use for it. Bear with me. Next time I'll show you how the strategy adds value.","tags":"Development","url":"https://blairconrad.com/2010/11/21/growing-an-mvvm-framework-in-2003-part-iii-properties-redux/","loc":"https://blairconrad.com/2010/11/21/growing-an-mvvm-framework-in-2003-part-iii-properties-redux/"},{"title":"AutoTest.Net updated - now (and then) notices broken builds","text":"I received a useful comment on Friday's post about AutoTest.Net . In the wee hours of Saturday, Greg Young , wrote to say It should detect broken builds without any problem. We have been running it daily for about 1.5 months. Perhaps you could grab me via email and reproduce it? Well, I wasn't going to pass up that offer. Off to GMail! 7:15 I grabbed him 7:20 he was making specific requests for additional information, the output of test runs through the console runner, and the like. 8:00 he had dived into the code to verify that things were working as they should, and asked for a sample project that exhibited the bug. 8:20 I sent the code 8:31 I e-mailed that I'd accidentally sent a project that complied 8:34 Greg reproduced the problem 8:54 he sent me a replacement .zip file 9:04 it worked! As soon as I broke the compilation, the monitor lit up, showing me which project failed and where: [Info] 'AutoTest.Console.ConsoleApplication' Preparing build(s) and test run(s) [Info] 'AutoTest.Console.ConsoleApplication' Error: D:\\bconrad\\Documents\\Source\\BlogExamples\\2010-11-autotest\\BookFinder\\BookFinder.Core\\BookListViewModel.cs(50,17) CS1002: ; expected [D:\\bconrad\\Documents\\Source\\BlogExamples\\2010-11-autotest\\BookFinder\\BookFinder.Core\\BookFinder.Core.csproj] [Info] 'AutoTest.Console.ConsoleApplication' Ran 1 build(s) (0 succeeded, 1 failed) and 0 test(s) (0 passed, 0 failed, 0 ignored) It turns out that the bug had already been fixed on trunk version of the code, but for some reason hadn't been built into the Windows installer. Turnaround time: 1 hour 49 minutes from my initial e-mail, and that included: me drifting off to other tasks between e-mails, increasing delays a session of trying to work around GMail hating the zip file I tried to send a delay imposed by my having sent a bad test project I'm sure those things added a good half hour to the required time. Then he spent another 40 minutes on a non-existent problem that I reported. I'd left an older AutoTest.Net WinForms monitor running during the debugging, so when things finally settled down, I got a pair of toasts from Growl - one reporting build failures, and one reporting successful builds when there weren't any. When I discovered that, Greg was already installing a new Growl for Windows to try it out. And he was very gracious about my error and his wasted time. I'm hardly the first to point it out, but this is one of the great things about open software. It's great getting that kind of service so quickly. And on a weekend no less. Will this encourage me to use AutoTest.Net Sure. My primary complaint with it has been resolved. Moreover, I'd be even more inclined to see what comes of Mighty Moose, now that I see the dedication of the developers behind it.","tags":"Development","url":"https://blairconrad.com/2010/11/14/autotest.net-updated-now-and-then-notices-broken-builds/","loc":"https://blairconrad.com/2010/11/14/autotest.net-updated-now-and-then-notices-broken-builds/"},{"title":"Hasty impressions: Continuous testing using AutoTest.NET","text":"Rinat Abdullin recently posted about Mighty Moose and AutoTest.NET , two projects for continuous testing in the .NET/Mono space. My interest was immediately piqued, as I'm a huge fan of continuous testing. I've been using py.test to run my Python unit tests for years now, almost solely because it offers this feature . I'm taking a look at AutoTest.Net first. Mostly because it's free. If I'm going to use something at home, it won't be for-pay, and the Day Job has been notoriously slow at shelling out for developer tools. Update : there was a bug that had been fixed on trunk, but not in the installer that I used. AutoTest.Net is better at detecting broken builds than I report below. Setting up AutoTest.NET Download and installation were straightforward. I opted to use the Windows installer package, AutoTest.Net-v1.0.1beta (Windows Installer).zip . I just unzipped, ran the MSI, let it install both VS 2008 and VS 2010 Add-Ins (the other components are required, it seems), and that was that. Then I cracked open the configuration file (at c:\\Program Files\\AutoTest.Net\\AutoTest.config ). I just changed two entries: BuildExecutable , and NUnitTestRunner That's it. Well, for the basic setup. Running the WinForms monitor I opened a command prompt to the root of a small project and ran the WinForms monitor, telling it to look for changes in the current directory. & 'C:\\Program Files\\AutoTest.Net\\AutoTest.WinForms.exe' . The application started, presenting me with a rather frightening window I mean, it makes sense. I have neither built nor run yet, so what did I expect? Still, I was taken aback by the plainness of it. Only temporarily daunted, I then hit the tiny unlabelled button in the northeast corner and got a new window. This was less scary. Everything seemed to be in order. I hadn't specified MS Test or XUnit runners, nor a code editor. It says it's watching my files. So let's test it. Mucking with the source It's supposed to watch my source changes and Do The Right Thing. Let's see about that. A benign modification to one test file I changed the text in one of my test files. No functionality was changed - it was purely cosmetic. AutoTest.Net noticed, rebuilt the solution, and ran the tests! Pretty slick. Things moved quickly, but here's what I saw from the application: A benign modification to one \"core\" file Next I changed the text in one of the core files - this file is part of a project that's referenced by the BookFinder GUI project, and the test project. Again, this was a cosmetic change only, just to see what AutoTest.NET would do. It did what it should - built the three projects and ran the tests. See? A core change that breaks a test So, now I'll modify the core code in a way that breaks a test. It picks up the change, builds, tests, and does a really nice job of showing me the failure. I see the test that failed, and when I click it, am presented with the stack trace, including hyperlink to the source. Unfortunately, clicking the hyperlink didn't go so well: That was a little disappointing. On the brighter side, hitting \"Continue\" did continue, with no seeming ill-effects. Redemption Confession time. I hadn't checked the CodeEditor section of the configuration file. As it turns out, it had a slightly different path to my devenv than the correct one. I fixed up the path and tried again. This time, clicking on the hyperlink opened devenv at the right spot. So the problems was ultimately my fault, but I can't help but wish for more graceful behaviour - how about a \"I couldn't find your editor\" dialogue? Ah, well. The product's young. Polish will no doubt come. I repaired the code that broke the tests, and AutoTest.Net was happy again after rebuilding and rerunning the tests. Syntax Error For my last test, I decided to actually break the compile. This was kind of disappointing. It claimed to run the 3 builds and the tests, and said that everything passed. I'm not sure why this would be - I was really hoping for an indication that the compilation failed, but nope. Everything was rainbows and puppies. Spurious rainbows and puppies. The VS Add-In There's an add-in. You can activate it under the \"Tools\" menu. It looks and behaves like the WinForms app. The Console Monitor I am used to running py.test in the console, so I thought I'd check out AutoTest's console monitor next. I started it up, made a benign change, and then made a test-breaking change. Here's what I saw: [Info] 'Default' Starting up AutoTester [Info] 'AutoTest.Console.ConsoleApplication' Starting AutoTest.Net and watching \".\" and all subdirectories. [Warn] 'AutoTest.Console.ConsoleApplication' XUnit test runner not specified. XUnit tests will not be run. [Info] 'AutoTest.Console.ConsoleApplication' Tracker type: file change tracking [Warn] 'AutoTest.Console.ConsoleApplication' MSTest test runner not specified. MSTest tests will not be run. [Info] 'AutoTest.Console.ConsoleApplication' [Info] 'AutoTest.Console.ConsoleApplication' Preparing build(s) and test run(s) [Info] 'AutoTest.Console.ConsoleApplication' Ran 3 build(s) (3 succeeded, 0 failed) and 2 test(s) (2 passed, 0 failed, 0 ignored) [Info] 'AutoTest.Console.ConsoleApplication' [Info] 'AutoTest.Console.ConsoleApplication' Preparing build(s) and test run(s) [Info] 'AutoTest.Console.ConsoleApplication' Ran 3 build(s) (3 succeeded, 0 failed) and 2 test(s) (1 passed, 1 failed, 0 ignored) [Info] 'AutoTest.Console.ConsoleApplication' Test(s) failed for assembly BookFinder.Tests.dll [Info] 'AutoTest.Console.ConsoleApplication' Failed -> BookFinder.Tests.BookListViewModelTests.FindClick_WithTitleG_FindsEndersGame: [Info] 'AutoTest.Console.ConsoleApplication' Not bad, but I have no stack trace for the failed test. Just the name. I'm a little sad to lose functionality relative the WinForms runner. I know I wouldn't be able to click on source code lines, but still. Gravy - Hooking up Growl Undeterred by the disappointing performance in the Syntax Error test, I soldiered on. I use Growl for Windows for notifications, and I was keen to see the integration. I went back to the configuration file and input the growlnotify path. While I was there, I set notify_on_run_started to false (after all, I know when I hit \"save\"), and notify_on_run_completed to true . Then I fixed my compile error and saved the file. In addition to the usual changes to the output window, I saw some happy toast: Honestly, with a GUI or text-based component around, I'm not sure how much benefit this will be, but I guess I can minimize the main window and so long as tests keep passing, I can get some feedback. Still it's kind of fun. Impressions I really like the idea of this tool. I love the idea of watching my code and continuously running the tests. The first steps are very good - I like the clickonable line numbers to locate my errors, and I think the Growl support is cute, but probably more of a toy than an actual useful feature. Will I Use It? Not now, and probably never at the Day Job. The inability to detect broken builds is pretty disappointing. Also, at work, I have ReSharper to integrate my unit tests . I've bound \"rerun the previous test set\" to a key sequence, so it's just as easy for me to trigger as it is to save a file. At home? Maybe. If AutoTest.Net starts noticing when builds fail, then I probably will use it when I'm away from ReSharper and working in .NET.","tags":"Development","url":"https://blairconrad.com/2010/11/12/hasty-impressions-continuous-testing-using-autotest.net/","loc":"https://blairconrad.com/2010/11/12/hasty-impressions-continuous-testing-using-autotest.net/"},{"title":"Growing an MVVM Framework in 2003, part II—Properties","text":"This is second post in a series on my experiences starting to grow an MVVM Framework in .NET 1.1. * Part I—Event Handlers * Part II—Properties * Part III —Properties Redux * Part IV—Unit Tests * Part V—Reflections and Regrets Full source code can be found in my Google Code repository . Last time, I introduced a tiny Windows Forms application and described my efforts to make a small MVVM framework for it. At the end of that post, we'd seen one way to use convention to bind View events to ViewModel event handlers. Today I'll talk about properties. It's all very well to have a click on the \"Find\" button trigger the FindClick method on the ViewModel, but it's useless unless we know what to look for . I needed a way to pass the Title.Text value to the ViewModel so it could use it for the search. Then the FindClick method I showed last time would work: public void FindClick(object sender, EventArgs e) { ICollection books = bookDepository.Find(TitleText); BookListItems.Clear(); foreach ( string book in books ) { BookListItems.Add(book); } } A Failed Attempt First I tried using Windows Forms binding, with lamentable results. I wish I'd saved the intermediate steps, as I was probably doing something wrong and could've solicited help. Still, whether it was due to a lack of experience on my part, or a flaw in the system, the bindings just wouldn't work. I could bind bools and strings, but lists were right out. A Proxy for Properties I decided to rely on the storage objects that came with the View elements. This meant the ViewModel needed some way to proxy the properties on the View. Then a get or a set on the ViewModel object would flow right through, reading or writing the View's values. Here's what I came up with: public class BoundProperty: Property { private object obj; private PropertyInfo propertyInfo; public BoundProperty ( object obj , PropertyInfo property ) { this . obj = obj ; this . propertyInfo = property ; } public override object Value { get { return propertyInfo . GetValue ( obj , null ); } set { propertyInfo . SetValue ( obj , value , null ); } } } Ignore the Property base class for a bit. An instances p of type BoundProperty can be used to get and set values on the proxied object obj like so: p.Value = valueA; object valueB = p.Value; Not incredibly thrilling, but one can work with it. Using the .Value in order to access the value was a little cumbersome, so I added a little syntactic sugar in the Property base class: public abstract class Property { public abstract object Value { get; set; } public static implicit operator string ( Property prop ) { return ( string ) prop . Value ; } public static implicit operator bool ( Property prop ) { return ( bool ) prop . Value ; } public IList AsList () { return ( IList ) Value ; } } I really like the implicit operator functionality, which I'd never used before. I wish it could be used with interfaces, though. There's probably a good reason why it can't, but nothing comes to mind. Anyhow, I had to go another route for IList—the somewhat uninspiring AsList method. At this point, I was really missing generics. Still, it's nicer to be able to write string myString = p1; IList myList = p2.AsList(); instead of string myString = (string) p1.Value; IList myList = (IList) p2.Value; Hooking up the Properties This is pretty much the same as hooking up the events like the last time. All we have to do is define a field (yes, a field) of type Property in the ViewModel: private Property titleText; The ViewModelBase loops over all the Property fields and looks for View controls that have matching property names: foreach ( FieldInfo field in PropertyFields() ) { FindPropertyToBindTo(allControls, field); } private void FindPropertyToBindTo(ArrayList allControls, FieldInfo field) { foreach ( Control control in allControls ) { if ( BindFieldToControl(control, field) ) { return; } } } private bool BindFieldToControl(Control control, FieldInfo field) { string controlPropertyName = ControlAttributeName(control, field); if ( controlPropertyName == null ) { return false; } PropertyInfo controlProperty = control.GetType().GetProperty(controlPropertyName, myBindingFlags); if ( controlProperty != null ) { field.SetValue(this, new BoundProperty(control, controlProperty)); } return true; } Technically that's it, but the rest of the ViewModel's code is a little cleaner if we self encapsulate the field : public string TitleText { get { return titleText; } set { titleText.Value = value; } } Remarks Once the infrastructure was in place, I really started enjoying developing the application. It was very liberating to add a new event handler just by writing a method with the right name and signature. And even adding access to a new property wasn't so bad—writing the three lines of code to segregate the conversions and .Value s was worth it to keep the event handler bodies nice and clean. Next time, we'll see how the design affected the form of the application's unit tests.","tags":"Development","url":"https://blairconrad.com/2010/11/10/growing-an-mvvm-framework-in-2003-part-ii-properties/","loc":"https://blairconrad.com/2010/11/10/growing-an-mvvm-framework-in-2003-part-ii-properties/"},{"title":"An unanticipated benefit of using the Chrome Extension Gallery","text":"I've written 3 Google Chrome Extensions. The first two were for use at the Day Job, so I initially hosted them on an internal server. Eventually I moved them out to the Chrome Extension Gallery . There are a few benefits to doing this over self-hosting: better publicity - as it's the prime location for extensions, people will go here looking for them, and they may find your extension Google maintains the site, so uptime's pretty good the extension gallery maintains the update manifest The first two benefits aren't a big deal for the Day Job extensions. We've a team to keep the servers up, and internal advertising channels. Mostly I enjoyed being freed of the monotony of generating new update manifests. A bonus benefit Last night a new benefit reached out and figuratively grabbed me by the lapels and shook me. I got mail from Google Extensions. They wanted to warn me about a problem with my extension. Sort of. Here it is: From: Google Extensions To: Google Extensions Subject: Important: Your extension is broken for all Chrome users - Here's how to fix it Hello, You are receiving this mail because you are the owner of an extension on chrome.google.com/extensions that was broken by a recent update to Chrome. This affects ALL users of Chrome, so it is something you should fix as soon as possible. Fortunately, the fix is very simple. In earlier versions of Chrome, the following syntax was supported: <script src=\"example.js\"> In current versions, this is no longer legal and must be changed to: <script src=\"example.js\"></script> All you have to do is replace any instances of this pattern in your extension and re-upload, and it should work again. We try very hard to avoid ever making breaking changes to the extension system, but in this case we did not notice it until the release had already been shipped. Since this is affecting users right now, we thought you'd appreciate details on what happened, and how to fix it yourself immediately. Thanks, -- The Chrome extensions team There's a new version of Chrome. It has a flaw, or at least a regression - the old syntax for including a script isn't supported. I'm a little disappointed that that change went in, but I'll get over it. The point is, once a bug was discovered, Google scanned all the extensions, identified those that were affected by the bug, and alerted the owners. I appreciate that they took this step. It's much better than just letting us sit around, waiting for users to tell us that our extension is broken. I wish at the Day Job we had this kind of ability. As things are, when a problem is discovered at a customer's site, we don't have an automated way of investigating their installation to see if their data will be problematic, or if the product wizards they've written are going to cause a problem. We sometimes obtain copies of sites' databases and configuration settings to diagnose a pernicious bug or to evaluate whether a proposed upgrade will be harmful. Unfortunately these instances are infrequent and are always a manual process. At the risk of seeming ungrateful I appreciate the notification, and will get right on the update so my user isn't affected. However, if I had my druthers, it would have been nice to have been told which extensions are affected - Google've already looked at all the extension. They know which ones I need to change. Save me the trouble of grepping. Of course, this would necessitate customized e-mails, rather than an all-purpose message. which versions of Chrome were affected (I mean something more specific than \"current versions\") whether this bug is fixed, or if it's going to be fixed in the future. I'm left not knowing if this is a temporary aberration, or if I need to use the syntax forever","tags":"Development","url":"https://blairconrad.com/2010/11/06/an-unanticipated-benefit-of-using-the-chrome-extension-gallery/","loc":"https://blairconrad.com/2010/11/06/an-unanticipated-benefit-of-using-the-chrome-extension-gallery/"},{"title":"Growing an MVVM Framework in 2003, part I—Event Handlers","text":"This is one post in a series on my experiences starting to grow an MVVM Framework in .NET 1.1. * Part I—Event Handlers * Part II—Properties * Part III —Properties Redux * Part IV—Unit Tests * Part V—Reflections and Regrets Full source code can be found in my Google Code repository . At the Day Job I usually work on web services, but I recently had the opportunity to write a customer-facing tool that had a GUI. Previously, I expressed my excitement over the Rob Eisenberg \"Build Your Own MVVM framework\" talk . Ever since, I've been dying to try my hand at an MVVM application. I wanted to create an application that had testable logic, even in the GUI layer, had no \"codebehind\" in the view, and shunted the tedious wiring up of events and handlers into helpers (or a \"framework\") Unfortunately, the application was intended to work at our established customers' sites, so I couldn't depend on WPF, or even .NET 2.0—it's 1.1 all the way. The Goal I'll demonstrate with a simpler app than the one from work, but will cover the the relevant concepts. For the purpose of this post, I'll be writing a book-finding app. The user will be able to enter a substring to use to search a database; the matching entries will be displayed in a ListBox and when one of them is selected, some notes will be displayed in a TextBox. I didn't want to have to riddle my ViewModel with += s just to be able to react to button presses and item selections from the view. I wanted to write something like: public void FindClick(object sender, EventArgs e) { ICollection books = bookDepository.Find(TitleText); BookListItems.Clear(); foreach ( string book in books ) { BookListItems.Add(book); } } and have the method run when the Click event on the Find button was raised. The method should use the value of the Text property of the Title TextBox to find a list of books and put them in the Items collection on the BookList ListBox. Wiring up Event Handlers I created a ViewModelBase class to handle all the infrastructure, so the BookListViewModel code could focus on app-related functions. The first thing ViewModelBase.BindToView does is seek out event handlers to bind to on the supplied View (which can be any Controller object): ArrayList allControls = AllControlsDescendingFrom(View); foreach ( MethodInfo handler in EventHandlers() ) { FindEventToListenTo(allControls, handler); } AllControlsDescendingFrom recursively looks through all the controls rooted at the View and returns them as a flat list. EventHandlers uses reflection to locate public methods on the ViewModel that have event-like signatures: private IEnumerable EventHandlers() { ArrayList eventHandlers = new ArrayList(); foreach ( MethodInfo method in this.GetType().GetMethods(BindingFlags.Instance | BindingFlags.Public) ) { if ( isEventHandler(method) ) { eventHandlers.Add(method); } } return eventHandlers; } private bool isEventHandler(MethodInfo info) { ParameterInfo[] parameters = info.GetParameters(); return (info.ReturnType == typeof (void) && parameters.Length == 2 && parameters[0].ParameterType == typeof(object) && (typeof(EventArgs)).IsAssignableFrom(parameters[1].ParameterType)); } Note the last line. I'd originally just checked that the second parameter was of type EventArgs . This worked for many event types, like the Click event on a Button and the SelectedIndexChanged event on a ListBox, but failed to match others, such as a TextBox's KeyPress event: public delegate void KeyPressEventHandler(object sender, KeyPressEventArgs e) FindEventToListenTo looks through the allControls list. If there's a control with name Controlname and an event Eventname , it will bind to a handler named ControlnameEventname . For example method SearchClick would be hooked up to the Click event on a control called Search. private void FindEventToListenTo(ArrayList allControls, MethodInfo handler) { foreach ( Control control in allControls ) { if ( ListenToEvent(control, handler) ) { return; } } } private bool ListenToEvent(Control control, MethodInfo method) { string eventName = ControlAttributeName(control, method); if ( eventName == null ) { return false; } EventInfo eventInfo = control.GetType().GetEvent(eventName, BindingFlags.Instance | BindingFlags.Public); if ( eventInfo == null ) { return false; } eventInfo.GetAddMethod().Invoke(control, new object[] { Delegate.CreateDelegate(eventInfo.EventHandlerType, this, method.Name) }); return true; } This is pretty straightforward, with two exceptions. Creating the delegate to wrap the ViewModel method was a little tricky—I had to reference the specific EventHandlerType that matched the event. Similarly to the EventArgs problem above, I'd originally tried to create an EventHandler , which failed for certain events. The last piece is the ControlAttributeName method, which builds the desired attribute (in this case an event) name from a control and the ViewModel member that we want to bind to. The method assumes that the name of the ViewModel member (the handler) will start with the name of the control. If there's a match, it returns the rest of the member name. Otherwise, null. The name comparison ignores case, which wasn't necessary to hook up method handlers, but proved to be useful when wiring up properties. private string ControlAttributeName(Control control, MemberInfo viewModelMember) { if ( viewModelMember.Name.ToLower().StartsWith(control.Name.ToLower()) ) { return viewModelMember.Name.Substring(control.Name.Length); } return null; } What's next? After wiring the event handlers, the ViewModelBase binds to the View's interesting properties. Details to follow .","tags":"Development","url":"https://blairconrad.com/2010/10/29/growing-an-mvvm-framework-in-2003-part-i-event-handlers/","loc":"https://blairconrad.com/2010/10/29/growing-an-mvvm-framework-in-2003-part-i-event-handlers/"},{"title":"Animating Google Chrome Extension Page Action Icons","text":"I'm enjoying using (and working on) Library Lookup , but I'm not entirely satisfied with the Page Action icons that pop up when searching, or when a book is found, or not found. In particular, I wanted a small animation while the search was ongoing, something like this: . Unfortunately, the animated GIF didn't work - Google Chrome Extensions don't support them. Briefly deterred, I regrouped and tried a different tack - something I like to call A Bunch o' PNGs and Some Javascript . First, I got myself three PNGs to display (okay, that's not entirely true - they're what I made the GIF from to begin with) searching_eyes_right.png searching_eyes_down.png searching_eyes_left.png Next, I needed a way to switch between the frames. I put the image names in an array, initialized an index, and wrote a small function that uses window.setTimeout to switch to a new icon every 0.3 seconds. var searching_images = ['searching_eyes_down.png', 'searching_eyes_right.png', 'searching_eyes_down.png', 'searching_eyes_left.png']; var image_index = 0; var keep_switching_icon = true; function rotateIcon() { if ( keep_switching_icon ) { chrome.pageAction.setIcon({tabId: sender.tab.id, path: searching_images[image_index]}); image_index = (image_index + 1) % searching_images.length; window.setTimeout(rotateIcon, 300); } } Then I start the rotation just before hitting the web server to see if the book's available and stop it when a result is found. Flipping the keep_switching_icon flag as soon as the search completes ensures that the animating thread doesn't overwrite a \"found\" or \"not found\" icon. window.setTimeout(rotateIcon, 300); var xhr = new XMLHttpRequest(); xhr.open(\"GET\", searchurl, true); xhr.onreadystatechange = function() { if (xhr.readyState == 4) { keep_switching_icon = false; if ( xhr.status != 200 ) { chrome.pageAction.setIcon({tabId: sender.tab.id, path: 'my_book_error_19.png'}); // other error handling } // process found and not found cases }; xhr.send();","tags":"Development","url":"https://blairconrad.com/2010/08/08/animating-google-chrome-extension-page-action-icons/","loc":"https://blairconrad.com/2010/08/08/animating-google-chrome-extension-page-action-icons/"},{"title":"Library Lookup - find books in your library","text":"I'm an avid reader, but not an avid collector of objects, so I prefer to get books from the library. As you might imagine, I was initially thrilled to discover the greasemonkey user script version of Jon Udell's LibraryLookup bookmarklet . The ability to visit web pages about books and be told whether the books are in one's library is just incredibly convenient. After a while, I wanted more - I wanted the script to work on more pages, and I wanted it to tell me if the book was in any of the three libraries that are available to me. So I reworked the script, modularizing it so it was easy to plug in additional libraries and source web pages. The resulting XISBN Library Lookup script has served me well for years. Recently, though, I've been using Google Chrome as my browser, and the user script (for whatever reason) doesn't work with Chrome's greasemonkey-to-extension translator. So, I've been \"libary lookupless\", and keenly felt the lack. I figured this was an excellent opportunity to learn how to write Chrome extensions, and it was not too difficult. The first incarnation of the new Library Lookup Chrome Extension is available in the extension gallery. It's in its infancy, but it supports the these libraries: Waterloo Public Library Kitchener Public Library Region of Waterloo Library And it will start looking up libraries when you browse to a book's page at: any site that has the ISBN in the URL , including (but not limited to) Amazon.com (and country-specific variants), Chapters/Indigo , and Powell's Books , Goodreads , All Consuming , and LibraryThing Try it now! Install the extension, visit a book page , and (if it's in the libraries) click on the handy \"book found\" page icon in the URL bar to see where your book is:","tags":"Miscellany","url":"https://blairconrad.com/2010/07/25/library-lookup-find-books-in-your-library/","loc":"https://blairconrad.com/2010/07/25/library-lookup-find-books-in-your-library/"},{"title":"libraryhippo.com lives","text":"For a while now, I've been dithering over getting LibraryHippo its own domain name. I hadn't, mostly because I'm a little lazy and cheap and afraid of the domain registration process. Yesterday at the Day Job, a co-worker (whose name just might be an anagram of Yen Waster ), on learning that libraryhippo.com was available, plunked a $10 bill on my desk and insisted that I buy the domain. I called him foolish and tried to return the money, but he was adamant. So, after an hour's work last night, and as a result of Mr. Waster's generosity, I invite you to check out the majesty of www.libraryhippo.com","tags":"Miscellany","url":"https://blairconrad.com/2010/07/13/libraryhippo.com-lives/","loc":"https://blairconrad.com/2010/07/13/libraryhippo.com-lives/"},{"title":"Using XSL to arbitrarily order strings - lessons from Professor Layton","text":"At the Day Job, I usually work on a middleware component that contains a component that monitors the state of the system. A \"health check\", if you will. The component can be monitored automatically so notifications can be triggered on error conditions, or it can be used by a human. In the latter case, the user sees a list of tests performed on the system, sorted first by test outcome and then by test name. To help the user identify problems, any errors encountered are pushed to the top of the results page. Here's a sample: Result Test Notes Error Nacelle Polarization unpolarized OK Dilithium Crystals OK Jefferies tube OK Warp Coils The actual report is an HTML page built from XML using an XSL transform - the main health check page queries various subcomponents that provide XML document sections. The sections are gathered and the XSLT sorts the results according to severity. The XSLT sorts the entries alphabetically by result string, using this XSL: <xsl:apply-templates select=\"//Operation\"> <xsl:sort order=\"ascending\" select=\"Result\" /> <xsl:sort order=\"ascending\" select=\"Test\" /> </xsl:apply-templates> Up 'til now, that worked great, but recently we had a need to add a third status - \"Warning\". The new report looked like this: Result Test Notes Error Nacelle Polarization unpolarized OK Dilithium Crystals OK Warp Coils Warning Jefferies tube partly blocked It would be better for Warning to be grouped between Error and OK. Unfortunately, it wasn't obvious how to do this. A few Google searches later, I'd found a post by Nick Fitzsimons that described his solution to the problem . After trying his approach, I was struck by a feeling of deja vu: I'd seen this, and recently, but where? Professor Layton to the Rescue Then it hit me. It's a classic puzzle. I'm sure it's appeared in many places, but I recently saw it in the game Professor Layton and the Diabolical Box . The Fake Coins puzzle asks There are 10 coins in each of the five bags below. One of these bags is filled with fake coins that are lighter than the real ones. A real coin weighs 10 units, but a false coin is one unit lighter. If you're using a scale that can register up to 200 units, what is the fewest number of times you could use the scale to find the one bag filled with fake coins? I'm going to spoil the puzzle, so if you want to figure it out yourself, stop reading now. The answer is \"one\". The interesting part is the approach: take 1 coin from bag 1, 2 coins from bag 2, and so on. Weigh them. There's a total of 15 coins, so if they were all genuine, the weight would be 150 units, but we know that each counterfeit coin is one unit less. So, if bag 1 contains the fakes, the total weight will be 150 - 1 = 149 if bag 2 contains the fakes, the total weight will be 150 - 2 = 148 if bag 3 contains the fakes, the total weight will be 150 - 3 = 147 if bag 4 contains the fakes, the total weight will be 150 - 4 = 146 if bag 5 contains the fakes, the total weight will be 150 - 5 = 145 It's a nice trick - coins from each bag contribute either 10 or 9 units - the weight difference between a good and a bad coin is 1, so we magnify that constant difference by different amounts to produce a single value that identifies which group the fake(s) come from. From coins to result severity The puzzle's fun, but what's the connection with the string ordering? The XSLT sort function operates on a single sort key generated from the input nodes, kind of like the single value (the weight) generated from a set of coins in the puzzle. It's still not clear how to generate a \"weight\" for the strings. Like in the coin puzzle, we want to sum up a series of values that are mostly the same, but that differ for a single result severity. We're helped by the fact that the number function converts Boolean true values to 1 and false to 0. If we compare each result severity in the source XML to \"Error\", \"Warning\", and \"OK\" in turn, exactly one of these will give a true (1) response, and the rest will be false (0). So, like the coin puzzle, where all weights are the same except for the counterfeits, we have a situation where all comparisons give the same value except for the true one. If we treat the sorting groups—Error, Warning, and OK—like the bags of coins, we can see how to rank the results. Multiplying the 0s and 1s by a factor that gives the preferred sort order produces a sum that acts as the perfect sort key: <xsl:apply-templates select=\"//Operation\"> <xsl:sort data-type=\"number\" order=\"ascending\" select=\"(number(Result='Error') * 1) + (number(Result='Warning') * 2) + (number(Result='OK') * 3)\" /> <xsl:sort order=\"ascending\" select=\"Result\" /> </xsl:apply-templates> a result severity of Error maps to 1 × 1 + 0 × 2 + 0 × 3 = 1 a result severity of Warning maps to 0 × 1 + 1 × 2 + 0 × 3 = 2 a result severity of OK maps to 0 × 1 + 0 × 2 + 1 × 3 = 3 The select code is a little long, and not obvious when starting from an empty slate, but it has some nice features: extending the sort for new result severities is straightforward - just add a term with the appropriate multiplier if we introduce a new severity without adding it to the sort, it sorts to the top - probably the best possible default action most importantly, it works. We now get a good health check result: Result Test Notes Error Nacelle Polarization unpolarized Warning Jefferies tube partly blocked OK Dilithium Crystals OK Warp Coils","tags":"Development","url":"https://blairconrad.com/2010/07/02/using-xsl-to-arbitrarily-order-strings-lessons-from-professor-layton/","loc":"https://blairconrad.com/2010/07/02/using-xsl-to-arbitrarily-order-strings-lessons-from-professor-layton/"},{"title":"Auto-deploying TypeMock Isolator Without Trashing the Installation","text":"At the Day Job, we use TypeMock Isolator as the isolation framework for the client portion of our flagship product. Historically we'd used version 3, but recently I had the opportunity to upgrade the code and build system to use the 2010 (or \"version 6\") edition. Backward Compatibility I was very pleased to see that no code changes were required with the upgrade. Sure, we'd like to start using the new Arrange-Act-Assert API , and to trade in the method name strings for the type-safe lambda expressions, but I didn't want to have to run back and convert everything today. And I didn't. Typemock Isolator appears to be backward compatible (at least as far as the feature set we use goes). Auto-Deployment In fact, the whole exercise of moving up to 2010 would've been over in almost no time were it not for one thing—we need to auto-deploy Isolator. The reasons are several: we have many dozen people working on the product, spread across four teams and three offices all over the world, so coordinating the installation is tricky some people have a need to occasionally build the product, but don't actively develop it - imposing an install on them seems rude some of our developers actively oppose unit testing, and I didn't want to give them any more ammunition than I had to We'd had a home-grown auto-deploy solution working with Isolator 3, but it was a little clunky and some of the details of the Isolator install had changed, so it wasn't really up to auto-deploying 6. Fortunately, I found a Typemock Insider blog post about auto-deploying . We use Apache Ant for our builds, but it was no trouble to shell out to an MSBuild task to auto-deploy Isolator: <Project xmlns=\"http://schemas.microsoft.com/developer/msbuild/2003\"> <PropertyGroup> <TypeMockLocation>path\\to\\TypeMock\\Isolator\\files</TypeMockLocation> <NUNIT>path\\to\\nunit-console.exe</NUNIT> </PropertyGroup> <Import Project=\"$(TypeMockLocation)\\TypeMock.MSBuild.Tasks\"/> <Target Name=\"RegisterTypeMock\"> <TypeMockRegister Company=\"MyCompany\" License=\"XXX-XXX\" AutoDeploy=\"true\"/> <TypeMockStart/> <Exec ContinueOnError=\"false\" Command=\"$(NUNIT) $(TestAssembly)\"/> <TypeMockStop Undeploy=\"true\"/> </Target> </Project> Build Server Licenses This worked really well - I was testing the tests on my local machine, watching Isolator auto-deploy and auto-undeploy. Everything was great, until I realized: we have two licenses—one for developers, and one for build servers. It only seemed right to use the appropriate one depending on whether we were building on a developer's machine or a build server. Fortunately, all our build servers set a specific environment variable, so it was a simple matter to have MSBuild pick the correct one. Undeploying Isolator Mangles the Installed Instance Even though we're providing a mechanism for auto-deploying Isolator, some developers will prefer to install it in order to use the Visual Studio AddIn to aid debugging. I'd heard that undoing the auto-deployment could wreak havoc with the installed version of Typemock Isolator, and that it's sometime necessary to repair the installed instance. A little testing, with the help of a coworker, showed this to be the case. Worse, it appeared that the auto-deploy/undeploy broke his ability to run the product in the IDE - as soon as the process started, it would end, with a \"CLR error 80004005\". Disabling the Isolator AddIn made the error go away. So it looked like we'd need to figure out how not to break installed Isolator instances while still supplying auto-deployment when it's needed. Searching found nothing promising, so I resorted to Registry spelunking. Unfortunately, the installed Isolator and auto-deployed Isolator make very similar Registry entries - there was nothing that I felt confident basing \"Is Isolator installed?\" on. After poking around and coming up short, I fell back to using the filesystem. By default, Isolator is installed in %ProgramFiles%\\TypeMock\\Isolator\\6.0 , so I decided to use that as the determinant. I'd feel dirty doing this for code destined for a customer's site, but I can live with telling our own developers that if they choose to install Isolator, they should install it in the default location or face the consequences. Still, if anyone comes up with a more reliable way to determine if Isolator is installed, please post it in the comments. Putting it all Together Here's the MSBuild file I ended up with. It uses the correct license based on machine type, and only auto-deploys/undeploys when Isolator isn't installed - existing installations are left alone. <Project xmlns=\"http://schemas.microsoft.com/developer/msbuild/2003\"> <PropertyGroup> <TypeMockLocation>path\\to\\TypeMock\\Isolator\\files</TypeMockLocation> <NUNIT>path\\to\\nunit-console.exe</NUNIT> <!-- Used to detect TypeMock installs. --> <UsualTypeMockInstallDir>$(ProgramFiles)\\TypeMock\\Isolator\\6.0</UsualTypeMockInstallDir> <!-- Only deploy Typemock if it's not already in the usual install dir. If developers install Typemock, they should install it in the default location in order to help the build system decide whether or not we need to auto-deploy (since auto-deploy and undeploy can corrupt the TypeMock VisusalStudio Add-In, and interfere with the ability to run programs in the IDE. --> <DeployTypeMock>false</DeployTypeMock> <DeployTypeMock Condition=\"!Exists('$(UsualTypeMockInstallDir)')\">true</DeployTypeMock> <License>XXX-XXX</License> <License Condition=\"'$(BuildServer)' != ''\">YYY-YYY</License> </PropertyGroup> <Import Project=\"$(TypeMockLocation)\\TypeMock.MSBuild.Tasks\"/> <Target Name=\"RegisterTypeMock\"> <TypeMockRegister Company=\"MyCompany\" License=\"$(License)\" AutoDeploy=\"$(DeployTypeMock)\"/> <TypeMockStart/> <Exec ContinueOnError=\"false\" Command=\"$(NUNIT) $(TestAssembly)\" /> <TypeMockStop Undeploy=\"$(DeployTypeMock)\"/> </Target> </Project>","tags":"Development","url":"https://blairconrad.com/2010/06/06/auto-deploying-typemock-isolator-without-trashing-the-installation/","loc":"https://blairconrad.com/2010/06/06/auto-deploying-typemock-isolator-without-trashing-the-installation/"},{"title":"When Fields are Initialized, or \"Lies Reflector Told Me\"","text":"The other day a coworker came to me with a Tricky Language Question. He and another chap had just finished working through a bug that had arisen due to a misunderstanding of C# constructor and field initialization order. The question? In a derived class, when does field initialization occur, relative the derived and base constructor code? Specifically, what does this output? class Print { public Print(string message) { Console.Out.WriteLine(message); } } class Base { public Print baseField = new Print(\"Base Field\"); public Base() { new Print(\"Base Constructor\"); } } class Derived: Base { public Print derivedField = new Print(\"Derived Field\"); public Derived() { new Print(\"Derived Constructor\"); } } class Program { static void Main() { new Derived(); } } I don't often give much thought to the \"field vs. base class constructor\" thing, but I knew that the Base constructor would be called before the Derived constructor, and I'd seen disassembled code in Reflector that showed field initialization as if it were the first code executed in a constructor. My guess was: Base Field Base Constructor Derived Field Derived Constructor \"Not so,\" said my coworker. The actual order is Derived Field Base Field Base Constructor Derived Constructor The reason for this is given in C# Language Specification section 17.10.3, Constructor execution : Variable initializers are transformed into assignment statements, and these assignment statements are executed before the invocation of the base class instance constructor. This ordering ensures that all instance fields are initialized by their variable initializers before any statements that have access to that instance are executed. \"What's the problem here?\" you may be wondering - the Base code doesn't know anything about the Derived fields, so why go out of our way to make sure the field initializers are called before the Derived constructor? Vitual methods Virtual methods are the problem. If a virtual method is defined in Base and overridden in Derived, the overridden method may reference the new fields added to Derived. If the virtual method is called from the Base constructor, then we need those fields to be initialized before the constructor is called. Initializing fields even before calling base class constructors ensures that this is so. Or does it? What if the field I'm accessing in a overridden method in the derived class doesn't have a field initializer, that method is called from the base constructor, and the field value is set in the derived constructor? In this case, the field won't be initialized before the method is called - it will still have the default value for its type. So how to do we safely call virtual methods in constructors? We don't. You can't guarantee what code is going to go into a derived class's virtual method, so you never know what's going to happen. Back to Reflector Remember a few paragraphs ago when I said that Reflector told me that field initialization acted like it was an assignment statement at the beginning of a constructor? Well, I did, and I wanted to see whether I was misremembering, so I compiled my sample code and threw the assembly into Reflector. Here's what I saw: I felt somewhat vindicated - this matched my memory. For a lark, I took this code (and the matching code Reflector showed me for the Base class), compiled it, ran it, and got: Base Field Base Constructor Derived Field Derived Constructor The more I thought about this, though, the worse I felt. How could Reflector let me down like this? Isn't it just looking at the IL and translating into C#? I poked around a little more, and instead of just double-clicking on the Derived constructor, I right-clicked on the Derived class node in the navigation tree and picked Disassemble . Lo and behold: So, Reflector does know what's going on—you just have to ask nice. To recap, if you know which Reflector action to choose, you remember about field initializers running before even base class constructors, and you keep careful track of virtual methods called from constructors Reflector can tell you what's going on in your code. Forget any of those things, and you're lost.","tags":"Development","url":"https://blairconrad.com/2010/05/22/when-fields-are-initialized-or-lies-reflector-told-me/","loc":"https://blairconrad.com/2010/05/22/when-fields-are-initialized-or-lies-reflector-told-me/"},{"title":"Quickly make editable diagrams with yUML","text":"I'm always on the lookout for convenient tools for creating diagrams that can be used for software development. Balsamiq is my tool of choice for UI mockups - it's great for whipping up stylized interfaces in very little time. Balsamiq does a great job of simplifying mockup creation, since it takes a lot of choices away from the user - you don't have the ability to change fonts or line thicknesses or colours. It really lets you focus on the aspects of the interface that matter when you're just getting started - the types and relative positions of the screen elements. Recently I discovered yUML - an online tool for creating class diagrams, activity diagrams, and use case diagrams. It's quite simple and produces attractive results. The best part is that you just specify the relationship between components - you don't have to position them yourself. Using an example from the site, you can create a diagram that shows that: a single customer aggregates several orders,each of which uses 0 or 1 PaymentMethods, and is composed of some number of LineItems by entering this code: [Customer]+1->*[Order] [Order]++1-items >*[LineItem] [Order]-0..1>[PaymentMethod] And out pops this diagram: Over the years, I've become accustomed to using WYSIWYG tools to create written documents and images, but I often miss text-based tools. They: give consistent, repeatable results, allow easy diffing between versions, and don't encourage time-wasting as we fiddle to adjust every single pixel or line break so I was really happy to find yUML - even though the syntax takes a little getting used to, it makes it easy to generate diagrams with a minimum of fuss. One wrinkle I've had using tools like Balsamiq and yUML is going back to modify diagrams after I've saved them off as a PNG. The last time I posted a Balsamiq PNG, I resorted to embedding the text that represented the diagram in the post comments . Actually, don't bother clicking that link - I'm sure I had the source saved in the post as an HTML comment, but I don't see it there, not even in edit mode. I've unwittingly demonstrated a point I was trying to make - many of these design tools don't allow you to save the \"source\" of the diagram, and it could be lost. This isn't a disaster for simple diagrams like the one above, but it could be very inconvenient for larger ones. Another problem is that typing out the code, pasting it into the conversion tool (in yUML's case a web page), and converting it and saving the result can become tedious as you make many adjustments. To address some of these inconveniences, I created a tiny Python script that accepts a class diagram description, hits the yUML website to create an image from it, and saves the image to disk, embedding the diagram \"source code\" in the PNG's iTXt chunk : import urllib import urllib2 import png def add_yuml_to_png(yuml, in_stream, out_stream): signature = png.read_signature(in_stream) out_stream.write(signature) for chunk in png.all_chunks(in_stream): if chunk.chunk_type == 'IEND': break chunk.write(out_stream) itxt_chunk = png.iTXtChunk.create('yuml', yuml) itxt_chunk.write(out_stream) # write the IEND chunk chunk.write(out_stream) def create(yuml, output_filename): baseUrl = 'http://yuml.me/diagram/scruffy/class/' url = baseUrl + urllib.quote(yuml) original_png = urllib2.urlopen(url) output_file = file(output_filename, 'wb') add_yuml_to_png(yuml, original_png, output_file) output_file.close() if __name__ == '__main__': import sys sys.exit(create(*sys.argv[1:3])) The png module is a very rudimentary PNG handling module that I wrote just for this script. There are ready-made Python PNG modules out there, but I thought they'd be too heavy to pull in for this, and that it'd be fun to write the PNG-handling code. It was. Later on, when you want to adjust the diagram, we can the following command on the PNG: read_yuml_from_png.py yuml_order_example.png [Customer]+1->*[Order], [Order]++1-items>*[LineItem], [Order]-0..1>[PaymentMethod] And here's read_yuml_from_png.py: import png def read(pngFilename): yuml = '<<no yuml found>>' pngFile = file(pngFilename, 'rb') png.read_signature(pngFile) for chunk in png.all_chunks(pngFile): if chunk.chunk_type == 'iTXt': chunk = png.iTXtChunk(chunk) if chunk.keyword == 'yuml': yuml = chunk.text break pngFile.close() return yuml if __name__ == '__main__': import sys sys.exit(read(sys.argv[1])) and out pops the original class diagram description. The png module is kind of long to paste here, but you can get all of the source for this post from my Google code project ( direct link to source ).","tags":"Development","url":"https://blairconrad.com/2010/05/01/quickly-make-editable-diagrams-with-yuml/","loc":"https://blairconrad.com/2010/05/01/quickly-make-editable-diagrams-with-yuml/"},{"title":"Watch, even if you're not building an MVVM App","text":"I'm about two weeks behind the wave, but this week I found the time to watch Rob Eisenberg's Mix 10 Build your own MVVM Framework talk (who says having the flu is all bad?). It's well worth the hour and twenty minutes. If you haven't yet, grab a tea, fire up the video, and sit back. Seriously, I can't describe how much I enjoyed it. My impressions, in no particular order: The warning about needing MVVM experience may be overstated - I've never worked with MVVM (or GUIs, really), and only had it described to me by The Guy in the Next Cubicle, and I felt like I followed along well enough. If you know what the M, the V, and the VM are, that may be all you need. Having computers perform repetitive tasks for humans is cool - we should try more of that. Even if you don't care about MVVM or GUI programming, the section about using .NET IEnumerables to implement Coroutines (starting at around minute 48) is worth the price of admission - this looks like an extremely powerful technique to (among other things) remove some of the complexity of performing aynchronous calls. I'm going to keep an eye out for places that this could help me. I really liked the message that Mr. Eisenberg kept hitting - the mini-framework is (just?) a way of crystalizing existing conventions and making them work for you. This is a very powerful point - presumably you have conventions that you and your coworkers follow, so why not try to get more out of them? Perhaps incongruously, this reminds me a little of the strongest argument in favour of using whitespace for flow control in Python - in many languages (say C), we use braces to tell the compiler where a block begins and ends, and whitespace to tell the humans - why not use one mechanism for both? The mini framework does a similar thing - when we name classes SearchView and SearchViewModel, we do it so the programmers know how the classes are related, so why require an additional statement or constructor parameter or whatever to link them up? If the framework understands the convention, it can do the work for you. The live presentation certainly helped to understand things - it was much easier for me to follow this than to try to understand Effectus from text and source, not to denigrate Ayende Rahien's efforts. Another point I enjoyed - the framework is there to make 90% of the cases easier. If it turns out that something needs to be a little different because it's no performing well enough or something, you can do that too. Somewhat tied to the previous point, but I can't remember if it was mentioned - having a framework implement your conventions not only saves you work, but it can strengthen your conventions. If the easiest thing to do to get something to work is to follow a set of conventions, then people will follow the conventions - in addition to everything else, the framework can serve as a reminder to experienced developers and a teaching aid to the tyros. One thing I'd like to see sometime, and this would likely be better-suited to a short book or long paper, would be the evolution of an application from frameworkless to frameworkful. Mr. Eisenberg kept stressing how the framework grew out of existing conventions in the Game Library application. I'd really like to see a case study - at what point is a convention recognized and the decision made to formalize it? Did everything happen at once, at the beginning (\"we'll call our classes BlahView and BlahViewModel, so we'll need a ViewModelToViewHookerUpper\"), or as development progressed (\"this is the fifth button I've created that's called Save and calls the Save method on the ViewModel - why can't the computer know that's what I want?\")? Anyhow, don't just listen to me. Watch for yourself. Now I'm off to resist poring over the Day Job source code so see how we can fatten up our framework layer...","tags":"Development","url":"https://blairconrad.com/2010/04/02/watch-even-if-youre-not-building-an-mvvm-app/","loc":"https://blairconrad.com/2010/04/02/watch-even-if-youre-not-building-an-mvvm-app/"},{"title":"IE binds to id attributes, or \"How I learned to love var\"","text":"I recently converted the LibraryHippo \"Family Status\" page to use AJAX to fetch individual card statuses, instead of having the server aggregate all the statuses and send the complete summary back to the user. It was fairly straightforward, with one notable exception – Internet Explorer. When using Firefox or Chrome, as soon as the page loaded, the user would see a list of cards that LibraryHippo was checking, complete with throbbers . As results came in, the matching progress line would disappear and other tables would fill in, holding the results – books that have to go back, holds ready for pickup, etc. I don't mind admitting that I was a little proud of my first foray into AJAXy web programming. The morning after I finished the update, a co-worker signed up. Unlike everyone else I knew, she used Internet Explorer. She hit the summary page and everything stalled. The progress list was populated, the throbbers were throbbing, and… that's it. They just kept going. Oh, and a little indicator woke up in the status bar, saying that there was an error on the page: \" Object doesn't support this property or method \". The reported line numbers didn't match my source file, but via judicious application of alerts() s, I was able to isolate the problem to a callback that's executed on a successful card check to update a span that holds a row count: function refresh_table_count(table_selector) { count = $(table_selector + ' tbody tr').length; $(table_selector + ' thead #count').html(count); } That seemed pretty innocuous, and not dissimilar from code that I had elsewhere in the <script> block. Quick web searches revealed nothing, so I resorted to cutting and renaming bits until I could see what was going on. I was down to an HTML body with a single table definition, and the function above. The error persisted. Suspicious, I renamed the count variable to c , and the problem disappeared. At this point, I was convinced that IE's Javascript interpreter reserved the count keyword for itself. I made this claim to a friend, who was skeptical. Eager to show him, I whipped up a quick example, and… it worked. There were no problems with the word count . I was stymied again, but not for long: my sample HTML file didn't include an element with a \"count\" id. Once I added the count id, the sample broke. It turns out that IE is actually creating a global object that matches the item's ID! As Rick Strahl explains, the problem is a little worse than that, because the assignment on line 3 above should've overwritten the variable reference, but there's \"some whacky scoping going on\". Workarounds: do away with the temporary variable (possible in this case) rename the temporary variable (always possible, but lame) use more specific id attribute values (probably a good idea in any case) use the var statement to declare all variables – this is safest and probably the easiest to remember: function refresh_table_count(table_selector) { var count = $(table_selector + ' tbody tr').length; $(table_selector + ' thead #count').html(count); } Now everything is working on the new page, and I've every confidence that var will help keep it so.","tags":"Development","url":"https://blairconrad.com/2010/03/07/ie-binds-to-id-attributes-or-how-i-learned-to-love-var/","loc":"https://blairconrad.com/2010/03/07/ie-binds-to-id-attributes-or-how-i-learned-to-love-var/"},{"title":"Automated Testing using App Engine Service APIs (and a Memcaching Memoizer)","text":"I'm a fan of Test-driven development, and automated testing in general. As such, I've been trying ensure that the LibraryHippo code has an adequate set of automated tests before deploying new versions. Importing Google App Engine Modules Unfortunately, testing code that relies on the Google App Engine SDK is a little tricky, as I found when working with one of the LibraryHippo entities. There's an entity called a Card, which extends db.Model and represents a user's library card. The Card definition is not entirely unlike this: class Card(db.Model): family = db.ReferenceProperty(Family) number = db.StringProperty() name = db.StringProperty() pin = db.StringProperty() library = db.ReferenceProperty(Library) def pin_is_valid(self): return self.pin != '' Unfortunately, testing this class isn't as straightforward as one would hope. Suppose I have this test file: from card import Card def test_card_blank_pin_is_invalid(): c = Card() c.pin = '' assert not c.pin_is_valid() It fails miserably, spewing out a string of import errors. Here's the tidied-up stack: > from card import Card > from google.appengine.ext import db > from google.appengine.api import datastore > from google.appengine.datastore import datastore_index > from google.appengine.api import validation > import yaml E ImportError: No module named yaml Not so good. Fortunately, it's not that hard to find out what needs to be done in order to make the imports work: import sys import dev_appserver sys.path = dev_appserver.EXTRA_PATHS + sys.path from card import Card def test_card_blank_pin_is_invalid(): c = Card() c.pin = '' assert not c.pin_is_valid() Now Python can find all the imports it needs. For a while this was good enough, since I wasn't testing any code that hit the datastore or actually used any of the app Engine Service APIs. Running the App Engine Service APIs However, I recently found a need to use Memcache to store partially-calculated results and decided (like everyone else) to write a memoizing decorator to do the job. There's enough logic in my memoizer that I felt it needed an automated test. I tried this: import sys import dev_appserver sys.path = dev_appserver.EXTRA_PATHS + sys.path from google.appengine.api import memcache from gael.memcache import * def test_memoize_formats_string_key_using_kwargs(): values = [1, 2] @memoize('hippo %(animal)s zebra', 100) def pop_it(animal): return values.pop() result = pop_it(animal='rabbit') assert 2 == result cached_value = memcache.get('hippo rabbit zebra') assert 2 == cached_value ( gael is Google App Engine Library – my extension/utility package - as it grows and I gain experience, I may spin it out of LibraryHippo to be its own project.) Again, it failed miserably. Here's a cleaned-up version of the failure: > result = pop_it(animal='rabbit') > cached_result = google.appengine.api.memcache.get(key_value) > self._make_sync_call('memcache', 'Get', request, response) > return apiproxy.MakeSyncCall(service, call, request, response) > assert stub, 'No api proxy found for service \"%s\"' % service E AssertionError: No api proxy found for service \"memcache\"; This was puzzling. All the imports were in place, so why the failure? This time the answer was a little harder to find, but tenacious searching paid off, and I stumbled on a Google Group post called Unit tests / google apis without running the dev app server . The author had actually done the work to figure out what initialization code had to be run in order to get have the Service APIs work. The solution relied on hard-coded paths to the App Engine imports, but it was obvious how to combine it with the path manipulation I used earlier to produce this: import sys from dev_appserver import EXTRA_PATHS sys.path = EXTRA_PATHS + sys.path from google.appengine.tools import dev_appserver from google.appengine.tools.dev_appserver_main import ParseArguments args, option_dict = ParseArguments(sys.argv) # Otherwise the option_dict isn't populated. dev_appserver.SetupStubs('local', **option_dict) from google.appengine.api import memcache from gael.memcache import * def test_memoize_formats_string_key_using_kwargs(): values = [1, 2] @memoize('hippo %(animal)s zebra', 100) def pop_it(animal): return values.pop() result = pop_it(animal='rabbit') assert 2 == result cached_value = memcache.get('hippo rabbit zebra') assert 2 == cached_value There's an awful lot of boilerplate here, so I tried to clean up the module, moving the App Engine setup into a new module in gael: import sys def add_appsever_import_paths(): from dev_appserver import EXTRA_PATHS sys.path = EXTRA_PATHS + sys.path def initialize_service_apis(): from google.appengine.tools import dev_appserver from google.appengine.tools.dev_appserver_main import ParseArguments args, option_dict = ParseArguments(sys.argv) # Otherwise the option_dict isn't populated. dev_appserver.SetupStubs('local', **option_dict) Then the top of the test file becomes import gael.testing gael.testing.add_appsever_import_paths() gael.testing.initialize_service_apis() from google.appengine.api import memcache from gael.memcache import * def test_memoize_formats_string_key_using_kwargs(): ... The Decorator In case anyone's curious, here's the memoize decorator I was testing. I needed something flexible, so it takes a key argument that can either be a format string or a callable. I've never cared for positional format arguments – not in Python, C#, Java, nor C/C++ – so both the format string and the callable use the **kwargs to construct the key. I'd prefer to use str.format instead of the % operator, but not until App Engine moves to Python 2.6+ def memoize(key, seconds_to_keep=600): def decorator(func): def wrapper(*args, **kwargs): if callable(key): key_value = key(args, kwargs) else: key_value = key % kwargs cached_result = google.appengine.api.memcache.get(key_value) if cached_result is not None: logging.debug('found ' + key_value) return cached_result logging.info('calling func to get ' + key_value) result = func(*args, **kwargs) google.appengine.api.memcache.set(key_value, result, seconds_to_keep) return result return wrapper return decorator Faking out Memcache - Unit Testing the Decorator The astute among you are probably thinking that I could've saved myself a lot of trouble if I'd just faked out memcache and unit tested the decorator instead of trying to hook everything up for an integration test. That's true, but at first I couldn't figure out how to do that cleanly, and it was my first foray into memcache, so I didn't mind working with the service directly. Still, the unit testing approach would be better, so I looked at my decorator and rebuilt it to use a class rather than a function. It's my first time doing this, and it'll probably not be the last – I really like the separation between initialization and execution that the __init__ / __call__ methods give me; I think it makes things a lot easier to read. def memoize(key, seconds_to_keep=600): class memoize(): def __init__(self, func): self.key = key self.seconds_to_keep=600 self.func = func self.cache=google.appengine.api.memcache def __call__(self, *args, **kwargs): if callable(self.key): key_value = self.key(args, kwargs) else: key_value = self.key % kwargs cached_result = self.cache.get(key_value) if cached_result is not None: logging.debug('found ' + key_value) return cached_result logging.info('calling func to get ' + key_value) result = self.func(*args, **kwargs) self.cache.set(key_value, result, self.seconds_to_keep) return result return memoize Then the test can inject its own caching mechanism to override self.cache : class MyCache: def __init__(self): self.cache = {} def get(self, key): return self.cache.get(key, None) def set(self, key, value, *args): self.cache[key] = value def test_memoize_formats_string_key_using_kwargs(): values = [1, 2] @memoize('hippo %(animal)s zebra', 100) def pop_it(animal): return values.pop() cache = MyCache() pop_it.cache = cache result = pop_it(animal='rabbit') assert 2 == result cached_value = cache.get('hippo rabbit zebra') assert 2 == cached_value And that's it. Now I have a unit-tested implementation of my memoizer and two new helpers in my extension library.","tags":"Development","url":"https://blairconrad.com/2010/02/20/automated-testing-using-app-engine-service-apis-and-a-memcaching-memoizer/","loc":"https://blairconrad.com/2010/02/20/automated-testing-using-app-engine-service-apis-and-a-memcaching-memoizer/"},{"title":"A first look at Appstats - where's my time spent?","text":"After hearing about the release of Google's App Engine SDK 1.3.1, I rushed out to try the new Appstats Event Recorder to help profile LibraryHippo. I didn't expect great things, as I'm generally happy with the performance, with one notable exception, but I was curious about the tool. App Engine Fan has posted a great introduction of some of the features that make Appstats a useful and powerful tool - it's very easy to hook up, and seems to add very little overhead. In addition, it has very rich configuration options - one can omit classes of calls, fold calls together, select the amount of information retained about each call, and specify how many such records are retained (in what amounts to a circular buffer). I didn't use (or need) any particularly advanced configuration, so I just installed the Event Recorder and let it go. Here's what I saw: I don't have an in-depth analysis, but here are some impressions: it's pretty the information is presented very well - with only minimal reading, I can see that LibraryHippo made a handful of datastore queries, as well as a series of urlfetch.Fetch calls for each library card it checked I can get a quick view of what's taking what proportion of the time - for example, the fetches easily dominate total time (about 2.3 seconds) is easy to find, as well as the amount taken by the underlying API - 73 milliseconds there's something else that's going on - 1056 ms for cpu + api - nearly half the elapsed time. I'm not sure what that means exactly So far, no big surprises - I knew that most of the time was taken up by queries to the library web pages, but it's very cool to see it this way, and to see how much time is taken up going to the Datastore (not much). There's room for improvement, but 2.3 seconds is more than acceptable for this family - one of LibraryHippo's heaviest users. Two things did stand out, though. First, in the first group of urlfetch.Fetches, there are gaps between the fourth, fifth, and sixth calls (the ones that take 128 ms, 91ms, and 52ms) and the pattern repeats (with smaller gaps) in the second batches. This is where the retrieved records are processed and transformed into a normalized representation before rendering. The total time taken is a small, but I didn't expect to see anything . Second, there's a datastore_v3.Get call before each card is checked. This is not an explicit call that LibraryHippo makes, so I clicked on the line in the graph and got a detailed view of what was going on: It looks like the call is coming from the create method on line 8 of the all_libraries.py file. Curious, I click on that line and lo and behold, I get a view of the source . This is very cool. 1: #!/usr/bin/env python 2: 3: import sys 4: 5: modules = {} 6: 7: def create(card, fetcher): 8: id = card.library.type 9: if not modules.has_key(id): 10: modules[id] = __import__(id) 11: return modules[id].LibraryAccount(card, fetcher) 12: 13: def main(args=None): 14: if args == None: 15: args = sys.argv[1:] 16: return 0 Correlating the detail view and the source code, we see that create is handed a card parameter that has an as-yet-unresolved library instance. Accessing the library attribute on the card must complete what was a lazy load initiated when I loaded the Family entity - the cards come from the Family.card_set member. Ordinarily, I might start investigating the gaps and the implicit gets, but I know there's a much greater threat to LibraryHippo usability, which I confirm by checking out the record for another family's notification: Here's where the presentation really packs a wallop - there's clearly a qualitative difference here. And what a difference - instead of 2.5 seconds on the horizontal axis, it's 25 seconds, and most of the fetches are compressed to nigh-invisibility. There are two differences between this family's accounts and the first family's: they have an extra Kitchener Library card that the first family didn't, and they have a Region of Waterloo Library card. It's the RWL card that makes the difference: you can see it being checked in the last batch of urlfetch.Fetches. The 4 Waterloo and Kitchener library card checks are completely done after 3154ms, but the Region of Waterloo checking goes on for a further 21 seconds - for one library, and it's not an aberration - the library web site is slow . This post is already long enough, so I'll use an upcoming one to talk about what this slowness means for LibraryHippo and how I've tried to keep it from destroying the user experience.","tags":"Development","url":"https://blairconrad.com/2010/02/12/a-first-look-at-appstats-wheres-my-time-spent/","loc":"https://blairconrad.com/2010/02/12/a-first-look-at-appstats-wheres-my-time-spent/"},{"title":"Waiting pays off again - Google App Engine gets Datastore retries","text":"Sometimes when we wait a little, the universe provides for us. Last year, I was holding off taking LibraryHippo live because it's almost useless without daily notifications. At the time, Google App Engine had no scheduled tasks. I'd just resigned myself to soliciting users and setting up a cron job on one of my own computers to trigger the notifications, or use some other kind of hackery. Right after I made this decision, the Google App Engine team announced that Scheduled Tasks were available. Well, it's happened again. I really enjoy working on the Google App Engine framework, but one of the more frustrating aspects (lately) has been the timeouts I get when reading from the Datastore. I've recently taken some steps that will reduce the impact on users, but was on the verge of implementing an automatic Datastore retry mechanism. Fortunately, it looks like I may not have to, with the arrival of App Engine SDK 1.3.1 : \"App Engine now automatically retries all datastore calls (with the exception of transaction commits) when your applications encounters a datastore error caused by being unable to reach Bigtable. Datastore retries automatically builds in what many of you have been doing in your code already, and our tests have shown it drastically reduces the number of errors your application experiences (by up to 3-4x error reduction for puts, 10-30x for gets).\" There are other Datastore improvements, including Datastore Query Cursors, and unlimited result set size for queries (previously the maximum was 1000). Also instrumentation for the Python version of the SDK, and unit tests for Java. I'm keen to try out the Python instrumentation, but I'm pretty sure I already know where my bottleneck is...","tags":"Development","url":"https://blairconrad.com/2010/02/10/waiting-pays-off-again-google-app-engine-gets-datastore-retries/","loc":"https://blairconrad.com/2010/02/10/waiting-pays-off-again-google-app-engine-gets-datastore-retries/"},{"title":"Using Subversion to Evangelize PowerShell","text":"I've never been really comfortable with the Windows Command prompt - whenever I can, I grab Cygwin to give myself a more familiar (and powerful) command-line environment. I really appreciate the tools included with the Unix command shells, as well as the easy composability of the utilities that come with Unix. Unsurprisingly, I was immediately attracted to PowerShell - a powerful replacement shell for Windows, with .NET integration, a Unix-like pipeline that works on objects rather than strings, and has plenty of built-in cmdlets. I installed it and tried to work. There were familiar commands (many of the Unix and Windows command names are aliased to their PowerShell equivalents), and these both helped and hindered - it was easy to find a command, but the options were slightly off, so the commands my fingers knew produced errors or unexpected results. Eventually I fell off the wagon, reverting to cmd.exe. Over the next year or so, I would return to PowerShell, only to stop using it again. A few months ago, I was talking to the Guy in the Next Cubicle. He was also interested in PowerShell. We talked a little about the few scripts (neither of us had written new cmdlets) we'd made, and shared them. Having someone to talk to about PowerShell sparked something, and gave me the impetus to change my shortcuts to start PowerShell instead of cmd.exe. Unfortunately, sharing our scripts was awkward - usually via e-mail or instant messaging. In addition, as we improved our scripts, adding some to perform common tasks, we found the answers to some questions when people came for help was, \"Well, if you had this PowerShell script, you'd just...\", with no convenient way to get them the scripts. Finally we decided to do something about it, and initiated a plan to help share our setup among our coworkers. We wanted a system that: was easy to adopt, made it easy to get and share scripts, and was customizable - so people could have different prompts, for example Inspired by Joey Hess's keeping your life in svn , we tried putting the whole profile directory under Subversion control. It was an easy choice, since everyone at the Day Job is already using Subversion to wrangle our source code. Getting Started Getting started with the profile is almost as easy as running cd %USERPROFILE%\\My Documents svn checkout http://svn.dayjob.com/path/to/PowerShellProfile/trunk WindowsPowerShell After this, the user will have a directory inside their My Documents directory that looks something like this: It's not quite usable, though. By default PowerShell doesn't allow scripts to be run, so the new profile will be of no benefit to users. To ease their pain, the profile directory contains a setup_powershell.bat which runs powershell -Command \"Set-ExecutionPolicy RemoteSigned\" After running setup_powershell.bat, all a user has to do is start PowerShell and they will benefit from the new profile. Inside the WindowsPowerShell Directory The first item of note inside the WindowsPowerShellDirectory is the Includes directory, which is populated with .ps1 files. Each .ps1 file contains functions to be loaded into memory and made available for the user's session. At startup, each of these .ps1 files are each dot-sourced (using the dot-source from a function trick I talked about last time ). Next, the Scripts directory, which is added to the user's $env:PATH . Each of the .ps1 files in the directory contains a standalone script that a user might choose to execute as they work. We have a number of Day Job-specific scripts as well as some Subversion helpers and two meta-scripts, designed to make it easier to work with the PowerShell profile. Since new users won't be familiar with all the scripts in the profile, and because new scripts might be added at any time, we include a script to provide a quick synopsis of the available scripts: Get-ProfileHelp . It scans the Scripts directory, printing out, in an easy-to-read table, the Synopsis from the top of each script. #<# .Synopsis Get help for the PowerShellProfile scripts > Get-ScriptDirectory | Get-ChildItem -include \" .ps1\" -recurse | ForEach-Object { $name = $ .Name; $name = $name.Remove($name.Length-4) $synopsis = \"\" $content = (Get-Content $ .PSPath) for ($i = 0; $i -le ($content.length - 1); $i += 1) { if ( $content[$i] -like ' .Synopsis*' ) { $synopsis = $content[$i+1].Substring(1).Trim() break } } $o = New-Object Object $o | Add-Member NoteProperty Name $name $o | Add-Member NoteProperty Synopsis $synopsis $o } | Format-Table -AutoSize The Get-ScriptDirectory function just finds the location of the currently executing script. We'll see it later. Running the script gives output like this: Name Synopsis ---- -------- Copy-Branch Copy an SVN branch, and optionally switch to the new branch Get-ProfileHelp Get help for the PowerShellProfile scripts Get-SslCertificate Load and display an SSL Certificate from a host Import-ResharperSettings Import Resharper 4.5 settings from a file Merge-Branch Merge SVN commits back into the current working directory Switch-Branch Switch to a new SVN branch Update-Profile Get the latest version of the PowerShell profile from SVN Rather than forcing users to navigate to the profile directory and run an svn command, the Update-Profile.ps1 script will automatically update the profile source: #<# .SYNOPSIS Get the latest version of the PowerShell profile from SVN > svn update (Split-Path $profile) User Profile Directories In addition to the Includes and Scripts directories, each user of the PowerShell profile can have their own directory full of customizations. The name of the directory is taken from the $env:USERNAME variable. On startup, if the directory exists, any Include and Scripts directories are processed - being dot-sourced or added to the path, respectively. This allows users to have their own personal scripts and functions. In addition, the profile.ps1 from the directory is dot-sourced. If a user runs PowerShell and doesn't already have a profile directory, a welcome message is printed to the screen, explaining basic usage of the profile. Then a profile directory is created and populated with an empty profile.ps1 to get the user started and to keep them from being welcomed again. Some users choose to commit their personal profile directories to the repository, and some don't - there's no requirement either way. If someone chose, they could even use an svn:externals on the WindowsPowerShell directory and host their personal directory in another part of the repository or even a different repository. Putting it All Together Finally we see the Microsoft.PowerShell_profile.ps1 file that orchestrates all this: # Will turn on extra output to help debug profile-loading. Don't check in as \"true\" $verbose = $false A convenience function to get the directory the current script lives in - useful for importing from relative paths function Get-ScriptDirectory { $Invocation = (Get-Variable MyInvocation -Scope 1).Value Split-Path $Invocation.MyCommand.Path } function Include-ProfileDirectory([string] $directory) { # Load every file in the Includes subdirectory - # hopefully they can be loaded in any order. # The Includes directory should contain files that define functions and # filters to be executed later, but not scripts that need to do # something when the file is sourced. if ( Test-Path ($directory + '\\Includes') ) { Get-ChildItem -Path:($directory + '\\Includes') -Filter:*.ps1 | ForEach-Object { if ( $verbose ) { Write-Output (\"importing \" + $ .PSPath) } . $ .PSPath } } # The Scripts directory should contain PowerShell scripts that someone # might want to be executed, so we'll add it to our path. if ( Test-Path \"$directory\\Scripts\" ) { $env:PATH = \"$($env:PATH);$directory\\Scripts\" } } dot-source the Include-ProfileDirectory function. If we run it directly, all the included functions will be scoped inside the Include-ProfileDirectory command, and inaccessible to the user. . Include-ProfileDirectory(Get-ScriptDirectory) Look for user-specfic customizations. If they're there, load them. $userProfileDir = ((Get-ScriptDirectory) + '\\' + $env:USERNAME) if ( Test-Path $userProfileDir ) { if ( $verbose ) { Write-Output \"including $userProfileDir\" } . Include-ProfileDirectory($userProfileDir) $userProfile = ($userProfileDir + '\\profile.ps1') if ( Test-Path $userProfile ) { . $userProfile } } else { Write-Host -foregroundColor yellow -backgroundColor darkblue @\" Welcome to the DayJob PowerShell Profile. It looks like this is your first time here, so I'll create a new profile for you. This profile will be called $userProfile If you want to customize your PowerShell experience, you can edit this file. Eventually you may want to modify files in the containing directories, but keep in mind that those changes will affect other users. Have fun! \"@ New-Item -path $userProfile -itemType \"file\" -Force > Out-Null }","tags":"Development","url":"https://blairconrad.com/2010/02/07/using-subversion-to-evangelize-powershell/","loc":"https://blairconrad.com/2010/02/07/using-subversion-to-evangelize-powershell/"},{"title":"expand your scope - you can dot-source more than just files","text":"I'm working on a small project that will require me to dot-source some PowerShell files in order to load their functions, aliases, and variables and make them available in a session. Actually, I have to do a little more than dot-source each file, but I'll keep the example simple to illustrate the wrinkle I ran into. Suppose I have this file, file-to-load.ps1 : Function Get-MyName { Write-Output \"Blair Conrad\" } I dot-source it from the console, and everything's great: PS> . .\\file-to-load.ps1 PS> Get-MyName Blair Conrad Because I'll be doing this over and over, and I want to manipulate the .ps1 files a little more, I decide to wrap the dot-sourcing in a function, and call it. Function Load-File([string] $filename) { . $filename } PS> Load-File('.\\file-to-load.ps1') PS> Get-MyName The term 'Get-MyName' is not recognized as the name of a cmdlet, function, script file, or operable program. Check the spelling of the name, or if a path was included, verify that the path is correct and try again. At line:1 char:11 + Get-MyName <<<< + CategoryInfo : ObjectNotFound: (Get-MyName:String) [], CommandNotFoundException + FullyQualifiedErrorId : CommandNotFoundException Not good. The Get-MyName function is loaded inside the scope of the Load-File function. It's only available as long as I'm inside Load-File . I thought about modifying all the script files that were to be loaded, scoping each contained function, alias, and variable as global , but that would be a pain, and I'm not going to be the only one writing these files. Eventually, I came upon it: dot-source the Load-File function: PS> . Load-File('.\\file-to-load.ps1') PS> Get-MyName Blair Conrad I'll admit I don't quite understand why it works, but for now, I'm content to know that it does.","tags":"Development","url":"https://blairconrad.com/2010/01/29/expand-your-scope-you-can-dot-source-more-than-just-files/","loc":"https://blairconrad.com/2010/01/29/expand-your-scope-you-can-dot-source-more-than-just-files/"},{"title":"Brandon Sanderson - Good author and class act","text":"I won't talk about every book I read here - that's what Goodreads is for. However, every once in a while something exceptional's bound to come up, and I'll be compelled to mention it here. This is one such thing. Anyone who's talked to me this year will have heard how much I enjoyed Brandon Sanderson's Mistborn Trilogy . Individually, they're very good books, but as a whole, they're exceptional - the planning Mr. Sanderson put into the books ties them together to provide a tight, compelling read, the likes of which I rarely see. Anyhow, before Christmas, Mr. Sanderson offered the Gift of Mistborn for 10 days - buy 7 signed, hard cover, first edition Mistborn: The Final Empire for $70US, plus shipping. I thought this was a tremendous deal and convinced a coworker to go in on it with me. A few days after I ordered, I received an e-mail saying that Mr. Sanderson had accidentally signed and personalized the entire trilogy for me, so he'd be sending me those as well. And sure enough they arrived just after Christmas, looking absolutely beautiful. This was a very generous thing to do. He could've said, \"Ooops, I ruined those copies,\" and sold them at a discount or ground them up and burnt them to heat his house. Instead, he poured more of his own money into shipping the books to Canada, just to make me happy. Thanks!","tags":"Miscellany","url":"https://blairconrad.com/2010/01/24/brandon-sanderson-good-author-and-class-act/","loc":"https://blairconrad.com/2010/01/24/brandon-sanderson-good-author-and-class-act/"},{"title":"Acronyms of the Day: VOMIT and BARF","text":"I was listening to CBC's Ideas Podcast today, specifically to You Are \"Pre-Diseased\", Part 2 (which aired on 18 January and is available for download until mid-February, in case you want to listen) when I heard a new acronym that's relevant to my Day Job. V ictim O f M edical I maging T echnology refers to patients who are operated on after an abnormality is detected in an imaging procedure, but nothing is found during the operation. Closely related is B rainlessly A pplying R adiological F indings - treating the result of an imaging study, not the patient and her symptoms. I don't mean to make light of the plight of patients who undergo operations or treatments when they're not warranted, but I thought the acronyms themselves are good for a chuckle. Read more.","tags":"Miscellany","url":"https://blairconrad.com/2010/01/19/acronyms-of-the-day-vomit-and-barf/","loc":"https://blairconrad.com/2010/01/19/acronyms-of-the-day-vomit-and-barf/"},{"title":"Cookies, Redirects, and Transcripts - Supercharging urlfetch","text":"LibraryHippo 's main function is fetching current library account status for patrons. Since I have no special relationship with any of the libraries involved, LibraryHippo web scrapes the libraries' web interfaces. The library websites issue cookies and redirects, so I needed to do something to augment the URL Fetch Python API . I wrote a utility class that worked with the urllib2 interface, but that didn't allow me to set the `deadline` argument, and I wanted to increase its value to 10 seconds. I resigned myself to wiring up a version that used urlfetch, when I found Scott Hillman's URLOpener , which uses cookielib to follow redirects and handle any cookies met along the way. URLOpener looked like it would work for me, with a few tweaks - it didn't support relative URLs in redirects, it doesn't allow one to specify headers in requests, and it lacked one feature that I really wanted - a transcript . Why a transcript? The libraries don't provide a spec for their output, so I built the web scraper by trial and error, sometimes putting books on hold or taking them out just to get test data. Every once in a while something comes up that I haven't coded for and the application breaks. In these cases, I can't rely on the problem being reproducible, since the patron could've returned (or picked up) the item whose record was troublesome or some other library state might've changed. I need to know what the web site looked like when the problem occurred, and since the ultimate cause might be several pages back, I need a history. I started adding a transcript feature to the URLOpener - recording every request and response including headers. As I worked, I worried about two things: the fetch logic was becoming convoluted, and the approach was inflexible - what if later I didn't want to follow redirects, or to keep a transcript? Decorators to the rescue I decided to separate each bit of functionality - following redirects, tracking cookies, and keeping a transcript - into its own decorator , to be applied as needed. First I teased out the code that followed redirects, with my change to allow relative URLs: class RedirectFollower(): def __init__(self, fetcher): self.fetcher = fetcher def __call__(self, url, payload=None, method='GET', headers={}, allow_truncated=False, follow_redirects=False, deadline=None): while True: response = self.fetcher(url, payload, method, headers, allow_truncated, False, deadline) new_url = response.headers.get('location') if new_url: # Join the URLs in case the new location is relative url = urlparse.urljoin(url, new_url) # Next request should be a get, payload needed method = 'GET' payload = None else: break return response After that, the cookie-handling code was easy to put in its own class: class CookieHandler(): def __init__(self, fetcher): self.fetcher = fetcher self.cookie_jar = Cookie.SimpleCookie() def __call__(self, url, payload=None, method='GET', headers={}, allow_truncated=False, follow_redirects=True, deadline=None): headers['Cookie'] = self._make_cookie_header() response = self.fetcher(url, payload, method, headers, allow_truncated, follow_redirects, deadline) self.cookie_jar.load(response.headers.get('set-cookie', '')) return response def _make_cookie_header(self): cookieHeader = \"\" for value in self.cookie_jar.values(): cookieHeader += \"%s=%s; \" % (value.key, value.value) return cookieHeader Now I had the `URLOpener` functionality back, just by creating an object like so: fetch = RedirectFollower(CookieHandler(urlfetch.fetch)) Implementing transcripts I still needed one more decorator - the transcriber. class Transcriber(): def __init__(self, fetcher): self.fetcher = fetcher self. transactions = [] def __call__(self, url, payload=None, method='GET', headers={}, allow_truncated=False, follow_redirects=True, deadline=None): self.transactions.append(Transcriber._Request(vars())) response = self.fetcher(url, payload, method, headers, allow_truncated, follow_redirects, deadline) self.transactions.append(Transcriber._Response(response)) return response class _Request: def __init__(self, values): self.values = dict((key, values[key]) for key in ('url', 'method', 'payload', 'headers')) self.values['time'] = datetime.datetime.now() def __str__(self): return '''Request at %(time)s: url = %(url)s method = %(method)s payload = %(payload)s headers = %(headers)s''' % self.values class _Response: def __init__(self, values): self.values = dict(status_code=values.status_code, headers=values.headers, content=values.content, time=datetime.datetime.now()) def __str__(self): return '''Response at %(time)s: status_code = %(status_code)d headers = %(headers)s content = %(content)s''' % self.values To record all my transactions, all I have to do is wrap my fetcher one more time. When something goes wrong, I can examine the whole chain of calls and have a better shot at fixing the scraper. fetch = Transcriber(RedirectFollower(CookieHandler(urlfetch.fetch))) response = fetch(patron_account_url) try: process(response) except: logging.error('error checking account for ' + patron, exc_info=True) for action in fetch.transactions: logging.debug(action) Extra-fine logging without rewriting fetch The exercise of transforming URLOpener into a series of decorators may seem like just that, an exercise that doesn't provide real value, but provides a powerful debugging tool for your other decorators. By moving the Transcriber to the inside of the chain of decorators, you can see each fetch that's made due to a redirect, and which cookies are set when: fetch = RedirectFollower(CookieHandler(Transcriber(urlfetch.fetch))) The only trick is that the `Transcriber.transactions` attribute isn't available from the outermost decorator. This is easily solved by extracting a base class and having it delegate to the wrapped item. class _BaseWrapper: def __init__(self, fetcher): self.fetcher = fetcher def __getattr__(self, name): return getattr(self.fetcher, name) Then the other decorators extend `_BaseWrapper`, either losing their `__init__` or having them modified. For example, `CookieHandler` becomes: class CookieHandler(_BaseWrapper): def __init__(self, fetcher): _BaseWrapper.__init__(self, fetcher) self.cookie_jar = Cookie.SimpleCookie() ... And then the following code works, and helped me diagnose a small bug I'd originally had in my `RedirectFollower`. As a bonus, if I ever need to get at `CookieHandler.cookie_jar`, it's right there too. fetch = RedirectFollower(CookieHandler(Transcriber(urlfetch.fetch))) fetch(patron_account_url) for action in fetch.transactions: logging.debug(action)","tags":"Development","url":"https://blairconrad.com/2010/01/17/cookies-redirects-and-transcripts-supercharging-urlfetch/","loc":"https://blairconrad.com/2010/01/17/cookies-redirects-and-transcripts-supercharging-urlfetch/"},{"title":"New Year's Python Meme","text":"It's a little late, but I'm participating in Tarek Ziadé's Python Meme ( via Richard Jones ): What's the coolest Python application, framework or library you have discovered in 2009? Google App Engine . I'd known of it before, but hadn't tried it until early this year when I started to work on LibraryHippo . What new programming technique did you learn in 2009? I'm not sure if this counts as a technique, but I recently found (and found a use for) Jean-Paul S. Boodhoo's Static Gateway Pattern . At the Day Job, we have a lot of hard-coded dependencies and reliance on well-known static methods for authorization. The Static Gateway Pattern made it easy to provide an injectable implementation without rewriting the whole application. I expect it to continue to be useful, at least until we take the time to introduce a full Inversion of Control container. What's the name of the open source project you contributed the most in 2009? What did you do? I didn't, really. Unless you count LibraryHippo. I've an interest in working on Noda Time , but I haven't managed to yet. What was the Python blog or website you read the most in 2009? Word Aligned What are the three top things you want to learn in 2010? more PowerShell .NET Framework 4.0, and whatever I've missed since 1.1... jQuery Ajax","tags":"Miscellany","url":"https://blairconrad.com/2010/01/09/new-years-python-meme/","loc":"https://blairconrad.com/2010/01/09/new-years-python-meme/"},{"title":"Meet LibraryHippo","text":"I enjoy reading and using my local libraries. My wife and I have four library cards between us - one each for the Waterloo Public Library , one for the Kitchener Public Library , and one for the Region of Waterloo Library . Using our cards, we were able to find all kinds of books to read and DVDs to watch, but organizing our borrowing was a little annoying, since: we had to log into four different library accounts to get an overview of our current borrowings and holds, each account had a long, hard-to-remember ID, and the library would send e-mail when items were overdue, not in time to take them back. I'd been using Library Elf to manage our cards, but they'd recently moved to a for-pay model, so I combined a sense of frugality with the desire to build something using a new technology and created LibraryHippo , a Google App Engine -powered web application that takes care of my library cards. LibraryHippo: manages multiple cards per family shows a comprehensive overview of a family's current library status sends e-mail every morning if a family has items that are nearly due there are items ready to be picked up, or there's a problem checking an account Feel free to check out the project, hosted on Google Code. A fair number of my future posts will talk about the adventures I've had implementing and improving LibraryHippo.","tags":"Development","url":"https://blairconrad.com/2010/01/08/meet-libraryhippo/","loc":"https://blairconrad.com/2010/01/08/meet-libraryhippo/"}]};